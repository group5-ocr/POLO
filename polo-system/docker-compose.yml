version: "3.9"

services:
  easy-train:
    build: ./models/fine-tuning
    container_name: easy-train
    environment:
      - HF_TOKEN=${HUGGINGFACE_TOKEN}
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    working_dir: /app
    command:
      [
        "python3", "training/qlora.py",
        "--model_name_or_path", "meta-llama/Llama-3.2-3B-Instruct",
        "--train_file", "/app/training/train.jsonl",
        "--output_dir", "/app/outputs/llama32-3b-qlora",
        "--report_to_tensorboard",
        "--train_fraction", "0.3",
        "--num_train_epochs", "3",
        "--save_every_steps", "300",
        "--logging_steps", "10",
        "--bf16",
        "--bnb_4bit",
        "--bnb_4bit_quant_type", "nf4",
        "--per_device_train_batch_size", "1",
        "--gradient_accumulation_steps", "4",
        "--max_seq_length", "256",
        "--gradient_checkpointing",
        "--learning_rate", "2e-4",
        "--warmup_ratio", "0.03"
      ]
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    volumes:
      - ./models/fine-tuning:/app
      - ./models/fine-tuning/outputs:/app/outputs
      - ~/.cache/huggingface:/root/.cache/huggingface:rw
    shm_size: "8g"
    ipc: host

  server:
    build: ./server
    container_name: polo-server
    environment:
      - MODEL_BASE=meta-llama/Llama-3.2-3B-Instruct
      - ADAPTER_PATH=/models/fine-tuning/outputs/llama32-3b-qlora/checkpoint-600
      - MAX_PAGES=4
      - MAX_CHARS=5000
      - HF_TOKEN=${HUGGINGFACE_TOKEN}
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - ./server:/app/server
      - ./models/fine-tuning:/models/fine-tuning
    working_dir: /app
    command: ["uvicorn", "server.app:app", "--host", "0.0.0.0", "--port", "8000"]
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]