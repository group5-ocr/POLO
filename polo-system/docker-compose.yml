version: "3.9"

services:
  # 모델 파인튜닝만 도커로 구성
  easy-train:
    build: ./models/fine-tuning
    container_name: easy-train
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    command: >
      python /app/training/qlora.py
      --model_name_or_path meta-llama/Llama-3.2-3B-Instruct
      --train_file /app/training/train.jsonl
      --output_dir /app/outputs/llama32-3b-qlora
      --report_to_tensorboard
      --train_fraction 0.3
      --num_train_epochs 3
      --save_every_steps 500
      --logging_steps 10
      --bf16
      --bnb_4bit
      --bnb_4bit_quant_type nf4
      --per_device_train_batch_size 1
      --gradient_accumulation_steps 4
      --max_seq_length 256
      --gradient_checkpointing
      --learning_rate 2e-4
      --warmup_ratio 0.03
      --target_modules q_proj,k_proj,v_proj,o_proj
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          devices:
            - capabilities: [gpu]
    volumes:
      - ./models/fine-tuning:/app
      - ./models/fine-tuning/outputs:/app/outputs
    # 데이터베이스는 외부 서버 사용

  # 파인튜닝된 모델을 서빙하는 서비스 (선택사항)
  easy-llm:
    build: ./models/fine-tuning
    ports:
      - "5003:5003"
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - ./models/fine-tuning:/app
      - ./models/fine-tuning/outputs:/app/outputs
    command: python /app/app.py
    depends_on:
      easy-train:
        condition: service_started
    # 데이터베이스는 외부 서버 사용
