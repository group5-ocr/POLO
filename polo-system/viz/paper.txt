
[Auto-Encoding Variational Bayes (Variational Autoencoder, 2013) - 논문 전체 해설본, 길고 자세하지만 이해하기 쉽게]

────────────────────────────────────────────
1. 문제의식과 서론 (Introduction)
────────────────────────────────────────────
확률적 생성 모델(Probabilistic Generative Model)은 데이터 x를 어떤 숨겨진(latent) 변수 z로부터 생성한다고 가정합니다.
즉, 우리가 관찰하는 데이터는 잠재 공간(latent space)에 존재하는 z에서 나왔다는 것입니다.

예를 들어, 손글씨 숫자 이미지는 “숫자 ID”, “획의 굵기”, “회전 각도” 같은 숨겨진 변수들의 조합으로 만들어진다고 볼 수 있습니다.
문제는 우리가 실제로는 z를 보지 못한다는 점입니다. z를 직접 관측할 수 없고, 오직 데이터 x만 볼 수 있습니다.

→ 우리가 하고 싶은 일은 x만 보고 z의 분포를 추론하고, 그 과정을 통해 새로운 데이터를 만들 수 있도록 학습하는 것입니다.
하지만 pθ(z|x) (잠재 변수의 사후 확률 분포)는 대부분의 경우 계산 불가능(intractable)합니다.

Variational Autoencoder (VAE)의 기여:
- 사후 분포를 근사하는 방법을 제안 (Variational Inference 기반)
- Stochastic Gradient Variational Bayes (SGVB) 추정기를 통해 미분 가능한 학습 가능
- Reparameterization Trick이라는 아이디어로 역전파가 가능하도록 변환
- Recognition Model (Encoder)을 사용해 사후를 근사하고, Generative Model (Decoder)와 함께 학습

────────────────────────────────────────────
2. 방법론 (Method)
────────────────────────────────────────────

2.1 문제 시나리오 (Problem Scenario)
데이터셋 X = {x(1), …, x(N)} 이 있다고 하자.
각 데이터는 잠재 변수 z(i)에서 생성되었다고 가정합니다.

생성 과정:
1) z(i) ~ pθ(z) (사전 분포, 보통 N(0, I))
2) x(i) ~ pθ(x|z(i)) (디코더를 통한 데이터 생성)

우리가 풀어야 할 세 가지 문제:
1. θ (모델 파라미터)의 최대우도(MLE) 근사 추정
2. x가 주어졌을 때 z의 사후 분포 근사 추정 (인코더 역할)
3. 새로운 x 샘플 생성 (데이터 생성)

→ 즉, 학습이 끝나면 Encoder qφ(z|x)와 Decoder pθ(x|z)를 모두 얻게 됩니다.

2.2 변분 하한 (Variational Lower Bound, ELBO)
관심은 log pθ(x)입니다. 하지만 이는 적분이 복잡해서 직접 계산이 어렵습니다.
대신 변분 근사 qφ(z|x)를 도입합니다.

log pθ(x) ≥ L(θ, φ; x) = Eqφ(z|x)[log pθ(x|z)] - DKL(qφ(z|x)||pθ(z))

여기서 L은 ELBO(Evidence Lower Bound)라고 부르며, 우리가 최적화할 대상입니다.

- 첫 번째 항: 재구성 정확도 (x를 얼마나 잘 복원하는가)
- 두 번째 항: KL Divergence (사후 분포가 prior와 얼마나 가까운가)

2.3 SGVB 추정기와 AEVB 알고리즘
ELBO를 직접 최적화하려면 qφ(z|x) 샘플링 과정이 미분 불가능합니다.
여기서 SGVB(Stochastic Gradient Variational Bayes) 추정기를 제안합니다.
- 몬테카를로 샘플링으로 기대값을 근사
- 이를 미분 가능한 형태로 바꿈 (Reparameterization Trick)

AEVB(Auto-Encoding Variational Bayes) 알고리즘은:
- Recognition model qφ(z|x) (Encoder)
- Generative model pθ(x|z) (Decoder)
- 둘을 동시에 학습

2.4 Reparameterization Trick (재매개변수화 기법)
기존에는 z ~ qφ(z|x) 를 직접 샘플링해야 했습니다. 이는 역전파 불가.
대신 z = μ(x) + σ(x) ⊙ ε, ε ~ N(0, I) 로 변환합니다.

이제 z는 결정적 함수 gφ(ε, x)로 표현되므로, 역전파가 가능해집니다.
즉, 신경망으로 μ, σ를 출력하고, ε는 단순 노이즈에서 뽑아옵니다.

────────────────────────────────────────────
3. Variational Autoencoder (VAE) 구조
────────────────────────────────────────────

- Prior: p(z) = N(0, I)
- Encoder (Recognition model): qφ(z|x) = N(z; μ(x), σ²(x)I)
- Decoder (Generative model): pθ(x|z) = Bernoulli or Gaussian, 파라미터는 z에서 MLP로 계산
- ELBO Loss: Reconstruction term + KL divergence

즉, VAE는 확률적 오토인코더입니다.
- 인코더: x → (μ, σ)
- 샘플링: z = μ + σ ⊙ ε
- 디코더: z → x 복원
- 손실: 재구성 오차 + KL 정규화

────────────────────────────────────────────
4. 관련 연구 (Related Work)
────────────────────────────────────────────
- Wake-Sleep Algorithm (1995): Recognition model과 generative model을 번갈아 학습. 하지만 최적화가 비효율적.
- Stochastic Variational Inference (2013): 고분산 문제 해결 시도, 하지만 제한적.
- Autoencoder 연구: 단순 재구성 기준만으로는 의미 있는 표현을 학습하기 어려움. 그래서 노이즈 추가, 스파스, 컨트랙티브 등 다양한 변형 제안.
- VAE의 장점: KL term이 자연스러운 정규화 역할을 해 의미 있는 잠재공간 형성.

────────────────────────────────────────────
5. 실험 결과 (Experiments)
────────────────────────────────────────────
데이터셋: MNIST, Frey Face

- 모델 구조: 인코더/디코더 MLP, 1 hidden layer
- MNIST: latent dimension Nz = 3, 5, 10, 20, 200
- Frey Face: latent dimension Nz = 2, 5, 10, 20

결과:
- AEVB(VAE)가 Wake-Sleep보다 빠르게 수렴하고 더 좋은 해를 찾음
- Latent dimension이 커져도 과적합이 크게 일어나지 않음 (KL term이 규제 역할)
- 저차원 latent space에서는 데이터의 매니폴드를 시각화 가능 (2D latent → MNIST manifold)

추가 실험:
- Marginal Likelihood 비교: AEVB > Wake-Sleep > Monte Carlo EM
- Latent space 시각화: 2D latent 공간에 MNIST/Frey Face 투영 → 비슷한 숫자끼리 모여 있음

────────────────────────────────────────────
6. 결론 (Conclusion)
────────────────────────────────────────────
- SGVB 추정기를 제안해 변분 추론을 효율적으로 수행 가능
- AEVB(VAE)는 Recognition model과 Generative model을 함께 학습해 강력한 생성 모델을 얻음
- 오토인코더와 변분 추론을 연결하는 새로운 패러다임 제시
- VAE는 안정적 학습, 의미 있는 잠재공간, 다양한 응용 가능 (압축, 복원, 생성)

────────────────────────────────────────────
7. 이후 연구 방향 (Future Work)
────────────────────────────────────────────
- CNN/RNN을 Encoder/Decoder에 활용해 더 복잡한 데이터(이미지, 시퀀스)에 적용
- 시계열 데이터(다이나믹 베이지안 네트워크)로 확장
- 지도학습/준지도학습 결합
- 대규모 데이터셋에 적용 및 GPU/분산 학습 최적화

────────────────────────────────────────────
요약 (Takeaway)
────────────────────────────────────────────
VAE는 단순한 오토인코더를 확률적 모델로 확장하여, 
- 데이터 분포 근사 (Generative modeling),
- 의미 있는 잠재공간 학습 (Representation learning),
- 새로운 샘플 생성 (Generation) 
을 가능하게 했습니다.

이후 수많은 변형(CVAE, β-VAE, VQ-VAE 등)이 등장했으며, 오늘날 생성 AI의 핵심 구성 요소가 되었습니다.