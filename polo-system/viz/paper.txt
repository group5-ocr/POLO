
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)
— 논문 전체 해설본 (길고 자세하지만 이해하기 쉽게)]

──────────────────────────────────────────────────────────────────────────────
1. 문제의식과 배경 (Introduction)
──────────────────────────────────────────────────────────────────────────────
자연어 처리는 “문맥”을 잘 이해하는 것이 핵심입니다. 그런데 2018년 이전의 대표적인 사전학습(Pre-training) 전략은 대체로 단방향이었어요. 예를 들면, 왼쪽→오른쪽으로만 다음 단어를 예측하는 언어 모델(예: GPT-1) 같은 방식이죠. 이러면 “나는 [MASK]를 먹었다” 같은 빈칸 문제에서 오른쪽 단어 ‘먹었다’의 힌트를 활용할 수 없습니다. 하지만 사람은 문장을 왼쪽과 오른쪽을 동시에 보면서 이해합니다.

BERT는 딱 이 점을 바꿉니다.
- “깊은 양방향(Deep Bidirectional)” Transformer 인코더를 사전학습하고,
- 그 결과물을 거의 그대로 가져다가(아키텍처를 거의 바꾸지 않고) 다양한 다운스트림 과제에 미세조정(fine-tuning)만 붙이면,
- 당대 최고 성능(SoTA)을 연달아 갈아치웁니다.

핵심 기여 두 가지:
1) **Masked Language Model (MLM)**: 입력 문장의 일부 토큰을 가려놓고(약 15%), 양옆 문맥을 모두 활용해 가려진 단어를 맞히게 학습합니다.
2) **Next Sentence Prediction (NSP)**: 두 문장을 주었을 때 B가 A의 진짜 다음 문장인지(IsNext) 아니면 무관한 문장인지(NotNext) 구분하도록 훈련합니다. (문장 관계 이해가 필요한 과제에 도움)

결과적으로 BERT는 질의응답(SQuAD), 자연어 추론(MNLI), 문장 유사도(STS-B), 문장 분류(MRPC, SST-2) 등 광범위한 벤치마크에서 강력한 성능을 보였습니다.


──────────────────────────────────────────────────────────────────────────────
2. 모델 아키텍처 (Transformer Encoder 스택)
──────────────────────────────────────────────────────────────────────────────
BERT는 **Transformer의 인코더 블록**만 여러 층 쌓아 올린 구조입니다. (디코더는 사용하지 않음)

- **Self-Attention(자기어텐션)**: 모든 토큰이 모든 토큰을 바라보며(양방향) 관계를 학습합니다.
- **Multi-Head**: 서로 다른 “관점”의 어텐션을 병렬로 학습합니다.
- **Feed-Forward**: 각 위치별로 독립적으로 적용되는 두 층짜리 MLP(중간 차원은 보통 4×hidden).
- **Residual + LayerNorm + Dropout**: 안정적 학습을 위한 표준 구성.

모델 크기(논문 기본 두 가지 변형):
- **BERT_BASE**: L=12층, hidden H=768, heads A=12  (약 1.1억 파라미터)
- **BERT_LARGE**: L=24층, hidden H=1024, heads A=16 (약 3.4억 파라미터)

왜 인코더만?
- 문장 이해(분류/추론/스팬 추출 등)는 입력 전체를 “깊게” 해석하는 인코더가 핵심이라서예요. 디코더는 주로 생성(출력 “직렬화”)에 쓰이죠.


──────────────────────────────────────────────────────────────────────────────
3. 입력 표현: 토큰/세그먼트/포지션 임베딩
──────────────────────────────────────────────────────────────────────────────
BERT는 입력 토큰을 다음 3가지 임베딩의 합으로 표현합니다.

(1) **토큰 임베딩(Token Embedding)**  
- WordPiece로 분절된 토큰(예: “playing” → “play ##ing”)을 30k 규모의 서브워드 사전에 매핑합니다.
- 대소문자를 구분하는 cased 모델과, 모두 소문자로 바꿔 학습하는 uncased 모델이 있습니다.

(2) **세그먼트 임베딩(Segment/A-B Embedding)**  
- 문장 쌍 입력 시, 앞 문장을 Segment A, 뒷 문장을 Segment B로 태깅합니다. NSP나 NLI처럼 문장 관계를 배우는 데 필요합니다.

(3) **포지션 임베딩(Positional Embedding)**  
- 단어의 순서를 잊지 않도록, 각 위치(0..511)에 대응하는 임베딩을 더합니다. (사인/코사인 대신 학습형 임베딩을 사용)

**스페셜 토큰**
- **[CLS]**: 맨 앞에 붙는 “문장 전체 대표” 토큰. 문장 분류 등에서는 이 토큰의 최종 표현 벡터를 사용합니다.
- **[SEP]**: 문장 경계 표시. 문장 하나만 있으면 맨 끝에 한 번, 문장 쌍이면 중간+끝에 등장합니다.


──────────────────────────────────────────────────────────────────────────────
4. 사전학습 목표 ①: Masked Language Model (MLM)
──────────────────────────────────────────────────────────────────────────────
MLM은 문장 내부의 일부 토큰을 가립니다(약 15%). 그리고 모델이 양옆 문맥을 모두 보고 그 가려진 토큰을 맞히도록 학습합니다. 이게 “깊은 양방향성”의 원천입니다.

하지만 단점이 하나 있어요. **[MASK]**라는 특수 토큰은 사전학습에는 등장하지만, 실제 다운스트림 과제에서는 거의 등장하지 않죠. 이 불일치를 줄이기 위해 논문은 다음의 **80/10/10 규칙**을 사용합니다.
- 80%: [MASK]로 바꿈 → 진짜 마스킹
- 10%: 전혀 상관없는 “랜덤 토큰”으로 바꿈 → 잡음에 대한 내성
- 10%: **그대로 둠**(원래 토큰 유지) → [MASK]에 과적응 방지

학습 시에는 마스크 위치만 손실을 계산합니다(비마스크 토큰은 정답으로 삼지 않음).


──────────────────────────────────────────────────────────────────────────────
5. 사전학습 목표 ②: Next Sentence Prediction (NSP)
──────────────────────────────────────────────────────────────────────────────
NSP는 두 문장의 “연속성”을 맞히는 이진 분류 과제입니다.
- 입력: “[CLS] 문장A [SEP] 문장B [SEP]”
- 출력: 문장B가 문장A의 실제 다음 문장인지(IsNext) 또는 무관한 문장인지(NotNext)
- 데이터 구성: 50% IsNext / 50% NotNext

왜 필요한가?
- 문장 관계 추론(NLI), QA(문단-질문 연결)처럼 “두 텍스트 조각의 관계”를 다루는 과제에 도움이 됩니다. (후속 연구에서는 NSP의 유용성에 의견이 갈리지만, BERT 원 논문에서는 기여가 있음을 보였습니다.)


──────────────────────────────────────────────────────────────────────────────
6. 사전학습 데이터와 학습 전략
──────────────────────────────────────────────────────────────────────────────
**데이터**
- **BooksCorpus**(약 8억 단어) + **영문 Wikipedia**(약 25억 단어) → 총 약 33억 단어 규모.
- WordPiece 사전 크기 ~30k.

**시퀀스 길이 전략**
- 초기 단계는 **짧은 문장 길이(예: 128)** 로 학습하여 속도를 높이고,
- 후반부 일부 스텝만 **긴 길이(예: 512)** 로 전환해 문맥 길이를 확장합니다.
  → 긴 길이로 처음부터 끝까지 학습하는 것보다 훨씬 효율적입니다.

**최적화/하이퍼 파라미터(핵심만)**
- 옵티마이저: Adam (warmup 후 선형 감쇠가 흔한 세팅)
- 드롭아웃: 0.1
- 레이블 스무딩은 사용하지 않음(Transformers 원 논문과 다름)
- 배치/스텝/시간 등 자원 관련 수치는 구현/하드웨어에 따라 달라질 수 있어 논문에서도 대략만 제시됩니다. 핵심은 “충분히 큰 배치, 적절한 워밍업, 긴 학습 스텝”이 성능에 중요하다는 점입니다.


──────────────────────────────────────────────────────────────────────────────
7. 파인튜닝(미세조정) 공통 레시피
──────────────────────────────────────────────────────────────────────────────
사전학습이 끝난 BERT를 다운스트림 과제에 맞춰 **아주 얇은 출력층만** 얹고 함께 학습합니다. 구조는 거의 그대로 둡니다.

① **문장/문서 분류(SST-2 등)**  
- 입력: [CLS] 문장 [SEP]
- 출력: [CLS]의 벡터 → MLP → softmax

② **문장쌍 관계 추론(MNLI, QQP, MRPC, RTE 등)**  
- 입력: [CLS] 문장A [SEP] 문장B [SEP]
- 출력: [CLS]의 벡터 → MLP → softmax(Entailment/Contradiction/Neutral 등)

③ **질의응답(QA; SQuAD 등)**  
- 입력: [CLS] 질문 [SEP] 문단 [SEP]
- 출력: 각 토큰에 대해 “정답 시작/끝 위치”를 분류(두 개의 선형층)  
  → 가장 그럴듯한 시작-끝 조합을 답으로 선택

④ **개체명 인식(NER)**  
- 입력: [CLS] 문장 [SEP]
- 출력: 각 토큰 벡터 → 토큰별 softmax (BIO/BILOU 태그 등)

**학습 팁**
- 드롭아웃 0.1, 학습률은 5e-5 / 3e-5 / 2e-5 중 탐색하는 경우가 많습니다.
- 최대 길이(예: 128/256/512)와 배치를 리소스에 맞춰 설정하고, 길이가 긴 과제는 gradient accumulation로 메모리 한계를 넘기기도 합니다.
- 토크나이저(cased/uncased) 선택은 도메인/과제 특성에 맞춥니다.


──────────────────────────────────────────────────────────────────────────────
8. 실험 결과 (논문 보고된 대표 성적 설명)
──────────────────────────────────────────────────────────────────────────────
① **GLUE 벤치마크(문장 이해 종합 지표)**  
- BERT_BASE 평균 점수 ≈ 고성능, BERT_LARGE는 더 높음.  
- CoLA(문법 수용성), MRPC/QQP(문장 유사도/패러프레이즈), STS-B(문장 유사도), MNLI/QNLI/RTE/WNLI(자연어 추론·문장 관계) 등 대부분의 태스크에서 이전 모델을 크게 앞섰습니다.

② **SQuAD v1.1 (추출형 QA)**  
- 단일 모델 기준으로도 F1이 매우 높고, 앙상블 시 인슐라(인간) 수준에 근접/초과하는 구간이 보고되었습니다.
- 스팬 시작/끝 위치를 예측하는 단순 헤드만 붙였는데도 강력했습니다.

③ **SQuAD v2.0 (“답 없음” 포함)**  
- 답이 없는 경우를 “답 없음”으로 올바르게 판단해야 하므로 더 어렵습니다.  
- BERT_LARGE가 기존 최고를 큰 폭으로 경신했습니다.

④ **NLI/Paraphrase/STS 등**  
- 문장쌍 과제에서 NSP의 도움을 받았는지 분석 결과, 문장 관계 과제에서 특히 강점을 보입니다.

※ 주의: 세부 수치(소수점 단위)는 버전·환경·전처리에 따라 조금씩 변동합니다. 여기서는 “BERT가 이전 최고 대비 유의미한 격차로 1위를 차지했다”는 큰 그림에 초점을 맞춥니다.


──────────────────────────────────────────────────────────────────────────────
9. 분석 및 어블레이션(Ablation)
──────────────────────────────────────────────────────────────────────────────
- **NSP의 효과**: 원 논문에서는 NSP를 제거하면 일부 문장쌍 과제에서 성능이 하락했다고 보고합니다. (후속 연구에서는 NSP 없이도 더 잘 되는 경우가 있어 논쟁 지점입니다.)
- **모델 크기(scale-up)**: 큰 모델(BERT_LARGE)이 거의 모든 태스크에서 더 강력했습니다. “사전학습으로 만들어진 일반 능력”은 모델 크기에 잘 스케일됩니다.
- **사전학습 목표**: 단방향 LM(왼→오 또는 마스크 없이 다음 단어 예측 등)만으로는 BERT만큼의 성능을 내기 어려웠다는 비교가 나옵니다. 양방향 MLM이 핵심입니다.
- **문장 길이/배치**: 긴 시퀀스 학습은 시간/메모리 비용이 커서, 짧은 길이로 대부분의 스텝을 먼저 학습 후 일부분만 긴 길이로 학습하는 전략이 효율적입니다.


──────────────────────────────────────────────────────────────────────────────
10. 실전 적용 팁과 자주 하는 실수
──────────────────────────────────────────────────────────────────────────────
- **토큰화/정답 정렬**: QA/NER에서 정답 스팬을 WordPiece 경계에 맞추는 전처리가 중요합니다. 공백/구두점/서브워드 경계로 인해 인덱스가 어긋나는 실수가 잦습니다.
- **길이 자르기(Truncation)**: 문장쌍 과제는 두 입력의 길이 합이 최대 길이를 넘기 쉬움. 질문/문단의 어느 쪽을 자를지 정책을 정하세요(슬라이딩 윈도우 등).
- **학습률 스윕**: 5e-5, 3e-5, 2e-5만 바꿔도 성능 차이가 큽니다. 에폭 수(2~4)도 같이 스윕하세요.
- **cased vs uncased**: 대소문자 구분이 중요한 도메인(뉴스, 법률, 의학 등)에서는 cased가 더 적합한 경우가 많습니다.
- **도메인 적응**: 일반 도메인(위키/북스)으로 사전학습된 BERT를 도메인 코퍼스(법률, 의학 등)로 추가 pre-training하면 큰 향상을 볼 수 있습니다(“도메인 어댑테이션”).


──────────────────────────────────────────────────────────────────────────────
11. 한계와 이후 발전
──────────────────────────────────────────────────────────────────────────────
- **NSP 논쟁**: 후속 연구(RoBERTa 등)에서는 NSP를 제거하고 더 큰 배치/데이터/학습 시간으로 성능을 끌어올렸습니다. 즉, NSP의 공헌은 태스크/세팅에 의존적입니다.
- **긴 문서 처리**: 자기어텐션은 길이에 대해 O(L^2) 복잡도를 가져 긴 문서(수천 토큰) 처리에 비용이 큽니다. Longformer/BigBird 등 희소 어텐션이 대안으로 등장했습니다.
- **지식/추론**: BERT는 언어 패턴을 잘 학습하지만, 외부 지식을 활용하는 능력은 제한적입니다. RAG(검색 결합), 지식 임베딩, 파인튜닝 데이터 설계가 중요합니다.
- **경량화**: 실무 배포를 위해 DistilBERT(지식증류), ALBERT(파라미터 공유·임베딩 분해), 양자화/프루닝 기법이 활발히 쓰입니다.


──────────────────────────────────────────────────────────────────────────────
12. 요약 정리 (Takeaways)
──────────────────────────────────────────────────────────────────────────────
- BERT는 **깊은 양방향 Transformer 인코더**를 사전학습하여, 거의 같은 구조로 다양한 과제에 강력히 전이됩니다.
- **MLM + NSP**의 조합으로 문맥 이해(특히 문장쌍 관계)에서 큰 성능 향상을 보였습니다.
- 다운스트림에선 [CLS] 벡터(문장 수준) 또는 토큰별 벡터(단어 수준)를 간단한 헤드에 연결해 미세조정하면 됩니다.
- 이후 BERT는 RoBERTa, ALBERT, DeBERTa, ELECTRA 등 수많은 파생/개선 연구의 출발점이 되었고, 오늘날 LLM/GPT 계열의 토대가 되었습니다.