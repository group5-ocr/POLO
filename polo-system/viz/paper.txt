YOLO: You Only Look Once – Unified, Real-Time Object Detection (2016)
한국어 번역본 (초록 + 서론)
초록 (Abstract)

우리는 YOLO라는 새로운 객체 탐지 접근 방식을 제안한다.

기존 객체 탐지 시스템은 분류기를 재사용하여 탐지를 수행한다. 반면, 우리는 객체 탐지를 공간적으로 분리된 바운딩 박스와 그에 해당하는 클래스 확률을 동시에 예측하는 **단일 회귀 문제(regression problem)**로 재정의한다.

하나의 합성곱 신경망(CNN)이 전체 이미지를 한 번만 처리하여 바운딩 박스와 클래스 확률을 직접 예측한다. 탐지 파이프라인 전체가 단일 네트워크이므로, 성능에 대해 엔드-투-엔드(end-to-end)로 직접 최적화할 수 있다.

우리의 통합 아키텍처는 매우 빠르다. 기본 YOLO 모델은 초당 45 프레임으로 이미지를 실시간 처리한다. 더 작은 네트워크인 Fast YOLO는 초당 155 프레임이라는 놀라운 속도를 기록하며, 다른 실시간 탐지기보다 두 배 높은 평균 정밀도(mAP, mean Average Precision)를 달성한다.

최신 탐지 시스템과 비교하면 YOLO는 로컬라이제이션(localization) 오류는 더 많지만, 배경을 잘못 탐지하는 경우는 더 적다. 마지막으로 YOLO는 매우 일반적인 객체 표현을 학습한다. 자연 이미지에서 학습한 후 예술 작품 같은 다른 도메인에 일반화할 때, DPM이나 R-CNN 같은 기존 탐지 방법보다 더 좋은 성능을 보인다.

1. 서론 (Introduction)

사람은 이미지를 힐끗 보기만 해도 그 안에 어떤 객체가 있는지, 어디에 있는지, 어떻게 상호작용하는지 즉시 알 수 있다. 인간 시각 시스템은 빠르고 정확하며, 운전처럼 복잡한 작업도 거의 무의식적으로 수행할 수 있다. 만약 빠르고 정확한 객체 탐지 알고리즘이 있다면, 컴퓨터는 특수 센서 없이도 자동차를 운전할 수 있고, 보조 장치는 시각장애인에게 실시간 장면 설명을 제공하며, 범용적이고 반응성 높은 로봇 시스템이 가능해질 것이다.

현재 객체 탐지 시스템은 분류기를 재활용한다. 즉, 탐지를 위해 특정 객체를 분류하는 신경망을 가져와 이미지의 여러 위치와 크기에서 반복적으로 평가한다. DPM(Deformable Parts Models) 같은 시스템은 슬라이딩 윈도우(sliding window) 방식을 사용해 분류기를 전체 이미지 위에서 실행한다.

최근 접근 방식(R-CNN 계열)은 우선 **region proposals(후보 영역)**을 생성하고, 각 후보 영역마다 분류기를 실행한다. 이후에는 바운딩 박스를 정제하고, 중복된 탐지를 제거하며, 장면 내 다른 객체를 기반으로 박스 점수를 재조정하는 후처리 과정이 필요하다. 이런 복잡한 파이프라인은 느리고 최적화하기 어렵다. 또한 각 모듈을 따로 학습해야 한다는 단점이 있다.

우리는 객체 탐지를 단일 회귀 문제로 재정의한다. 즉, 이미지 픽셀에서 곧바로 바운딩 박스 좌표와 클래스 확률을 예측하는 것이다. 우리의 시스템에서는 이미지를 한 번만 보면(You Only Look Once) 객체가 무엇이고 어디 있는지 예측할 수 있다.

YOLO는 단순하다. 하나의 합성곱 신경망이 동시에 여러 바운딩 박스와 클래스 확률을 예측한다. YOLO는 전체 이미지를 학습하고, 탐지 성능에 대해 직접 최적화한다. 이 통합 모델은 기존 탐지 방법에 비해 여러 가지 장점이 있다.

속도: YOLO는 매우 빠르다. 복잡한 파이프라인이 필요 없고, 테스트 시 이미지당 신경망을 한 번만 실행하면 된다. Titan X GPU에서 기본 네트워크는 초당 45프레임, Fast YOLO는 155프레임 이상을 처리한다. 이는 지연 시간이 25ms 미만으로, 실시간 비디오 스트리밍에 충분하다. 또한 YOLO는 기존 실시간 시스템보다 두 배 높은 mAP를 달성한다.

글로벌 문맥 활용: YOLO는 이미지 전체를 고려한다. 슬라이딩 윈도우나 region proposal 기반 방법과 달리, YOLO는 학습과 테스트 과정에서 전체 이미지를 본다. 따라서 클래스의 맥락(contextual information)을 암묵적으로 학습할 수 있다. Fast R-CNN은 배경 패치를 객체로 잘못 탐지하는 오류가 많지만, YOLO는 이러한 배경 오류를 절반 이하로 줄인다.

일반화 성능: YOLO는 일반화 능력이 뛰어나다. 자연 이미지로 학습한 후 예술 작품 데이터셋에서 테스트했을 때, YOLO는 DPM이나 R-CNN보다 훨씬 좋은 성능을 보였다. 이는 YOLO가 새로운 도메인이나 예상치 못한 입력에도 잘 적응할 수 있음을 보여준다.

YOLO의 한계도 있다. 최첨단 탐지 시스템보다 작은 객체의 위치를 잡는 데 어려움이 있어, 로컬라이제이션 정확도가 떨어질 수 있다. 하지만 실험 결과에서 보듯이, 속도·단순성·일반화라는 장점은 이 단점을 상쇄할 수 있다.

3. 네트워크 디자인 (Network Design)

YOLO의 합성곱 신경망은 분류용 CNN과 유사하지만, 최종적으로 바운딩 박스 좌표와 신뢰도(confidence), 클래스 확률을 한 번에 출력하도록 설계되어 있다. 입력은 448×448 RGB 이미지로 정규화되며, 일련의 합성곱/풀링을 거쳐 **완전연결층(fully connected)**에서 최종 예측을 낸다. 네트워크는 24개의 합성곱 층 + 2개의 완전연결층으로 구성된다. 큰 커널(예: 7×7)과 작은 커널(3×3)을 혼합해 넓은 수용영역과 세밀한 특징을 동시에 확보하고, 1×1 합성곱으로 채널 수를 압축하며 비선형성을 늘린다. 활성화 함수는 **Leaky ReLU(α=0.1)**를 사용한다. 마지막 출력층은 선형(linear)로, 예측값 스케일 왜곡을 피한다.

최종 출력 텐서는 S×S×(B×5 + C) 크기를 갖는다. PASCAL VOC 설정에서 S=7, B=2, C=20이므로, 한 이미지에 대해 7×7×(2×5+20)=1470 차원의 벡터를 낸다. 각 격자 셀마다 B개의 박스(각각 
𝑥
,
𝑦
,
𝑤
,
ℎ
,
𝑐
𝑜
𝑛
𝑓
𝑖
𝑑
𝑒
𝑛
𝑐
𝑒
x,y,w,h,confidence)와 **C차원 클래스 조건부 확률 
𝑃
𝑟
(
class
𝑖
∣
object
)
Pr(class
i
	​

∣object)**을 제공한다.

Fast YOLO는 정확도 일부를 희생하고 연산량을 크게 줄인 변형으로, 합성곱 층 수와 채널 폭을 줄여 **초당 155프레임(155 fps)**의 실시간성을 달성한다. 기본 모델은 45 fps로 동작한다.

분류 성능을 기반으로 빠르게 수렴시키기 위해, (ImageNet 등) 대규모 분류 데이터로 먼저 사전학습(pretrain)한 후, 입력 해상도와 출력층을 탐지 목적에 맞게 바꾸어 미세조정(fine-tuning)한다. 사전학습 동안 마지막 분류 헤드(softmax)는 사용하지 않고, 탐지 단계에서 **출력 차원을 
7
×
7
×
(
2
×
5
+
20
)
7×7×(2×5+20)**으로 교체한다.

4. 학습 (Training)
4.1 손실 함수

YOLO는 합계 제곱오차(sum-squared error) 기반의 다항식 손실로, (1) 좌표, (2) 신뢰도, (3) 분류를 한꺼번에 최적화한다. 한 격자 셀에 객체가 있을 때, 해당 객체와 IOU가 가장 큰 박스 예측자 1개만 그 객체를 “책임”지도록 할당한다(책임 할당: responsibility by max-IOU). 손실은 다음과 같다.

(a) 좌표 손실 (Localization loss)
박스 중심과 크기를 정밀히 맞춘다. 너비·높이는 
𝑤
,
ℎ
w
	​

,
h
	​

로 예측해 큰 박스가 과도하게 지배하지 않게 한다.

𝜆
coord
∑
𝑖
=
1
𝑆
2
∑
𝑗
=
1
𝐵
1
𝑖
𝑗
obj
[
(
𝑥
𝑖
𝑗
−
𝑥
^
𝑖
𝑗
)
2
+
(
𝑦
𝑖
𝑗
−
𝑦
^
𝑖
𝑗
)
2
]
λ
coord
	​

i=1
∑
S
2
	​

j=1
∑
B
	​

1
ij
obj
	​

[(x
ij
	​

−
x
^
ij
	​

)
2
+(y
ij
	​

−
y
^
	​

ij
	​

)
2
]
+
 
𝜆
coord
∑
𝑖
=
1
𝑆
2
∑
𝑗
=
1
𝐵
1
𝑖
𝑗
obj
[
(
𝑤
𝑖
𝑗
−
𝑤
^
𝑖
𝑗
)
2
+
(
ℎ
𝑖
𝑗
−
ℎ
^
𝑖
𝑗
)
2
]
+ λ
coord
	​

i=1
∑
S
2
	​

j=1
∑
B
	​

1
ij
obj
	​

[(
w
ij
	​

	​

−
w
^
ij
	​

	​

)
2
+(
h
ij
	​

	​

−
h
^
ij
	​

	​

)
2
]

(b) 신뢰도 손실 (Confidence loss)
객체가 있을 때(
1
obj
=
1
1
obj
=1)와 없을 때(
1
noobj
=
1
1
noobj
=1)를 다른 가중치로 처리한다.

∑
𝑖
=
1
𝑆
2
∑
𝑗
=
1
𝐵
[
1
𝑖
𝑗
obj
(
𝐶
𝑖
𝑗
−
𝐶
^
𝑖
𝑗
)
2
 
+
 
𝜆
noobj
 
1
𝑖
𝑗
noobj
(
𝐶
𝑖
𝑗
−
𝐶
^
𝑖
𝑗
)
2
]
i=1
∑
S
2
	​

j=1
∑
B
	​

[1
ij
obj
	​

(C
ij
	​

−
C
^
ij
	​

)
2
 + λ
noobj
	​

1
ij
noobj
	​

(C
ij
	​

−
C
^
ij
	​

)
2
]

여기서 
𝐶
𝑖
𝑗
=
𝑃
𝑟
(
object
)
⋅
𝐼
𝑂
𝑈
(
pred
,
truth
)
C
ij
	​

=Pr(object)⋅IOU(pred,truth).

(c) 분류 손실 (Classification loss)
객체가 있는 셀에 한해 클래스 분포를 학습한다.

∑
𝑖
=
1
𝑆
2
1
𝑖
obj
∑
𝑐
=
1
𝐶
(
𝑝
𝑖
(
𝑐
)
−
𝑝
^
𝑖
(
𝑐
)
)
2
i=1
∑
S
2
	​

1
i
obj
	​

c=1
∑
C
	​

(p
i
	​

(c)−
p
^
	​

i
	​

(c))
2

가중치 설정: 
𝜆
coord
=
5
λ
coord
	​

=5, 
𝜆
noobj
=
0.5
λ
noobj
	​

=0.5. 작은 박스의 폭·높이 오차가 크게 반영되지 않도록 
𝑤
,
ℎ
w
	​

,
h
	​

를 사용한다. 이 구성은 배경이 훨씬 많은 탐지 문제에서 **배경(=no object)**의 오차 항이 전체 손실을 지배하지 않도록 균형을 맞춘다.

4.2 학습 세부

책임 할당: 각 정답 객체당 IOU가 가장 큰 예측 박스(한 개)만 책임을 진다(그 박스만 좌표/신뢰도 “객체” 손실을 받음). 나머지 박스는 같은 셀 안에 있더라도 해당 객체에 대해 noobj로 취급된다.

최적화: 확률적 경사하강법(SGD), 모멘텀 0.9, weight decay 0.0005.

드롭아웃: 첫 완전연결층에 dropout 0.5 적용(분류 사전학습보다 과적합 위험이 커서 필요).

데이터 증가(data augmentation): 무작위 스케일/평행이동, HSV 공간에서 채도/명도(jitter) 조정 등으로 강건성 향상.

입력 해상도 스케줄: 분류 사전학습(보통 224×224) 후 탐지 파인튜닝 시 최종 448×448로 올려 디테일을 회복.

추론 후처리: 낮은 신뢰도 박스 제거(스코어 임계값)와 NMS(Non-Maximum Suppression) 적용으로 중복 제거.

5. 실험 (Experiments)
5.1 프로토콜

데이터셋: PASCAL VOC 2007, 2012.

VOC2007: train/val로 학습, test로 평가.

VOC2012: 2007+2012 train/val로 학습, 2012 test로 평가(일반적 세팅).

지표: mAP (mean Average Precision), IOU ≥ 0.5 매칭 기준.

비교 대상: DPM, R-CNN, Fast R-CNN 등 당시 대표 기법.

하드웨어: Titan X급 GPU에서 YOLO 45 fps, Fast YOLO 155 fps 추론 성능.

5.2 VOC 2007 결과 (예시 수치)

YOLO: mAP ≈ 63.4% @ 45 fps

Fast YOLO: mAP ≈ 52.7% @ 155 fps

R-CNN: ≈ 66% (매우 느림)

Fast R-CNN: ≈ 70% (하지만 0.5 fps 수준)

DPM: ≈ 33.7%

→ 정확도는 Fast R-CNN > YOLO, 그러나 속도는 YOLO가 압도적. 실시간(>30 fps) 조건에선 YOLO가 유일하게 높은 mAP을 제공.

5.3 VOC 2012 결과 (예시 수치)

YOLO: mAP ≈ 57.9%

Fast YOLO: mAP ≈ 52%대

Fast R-CNN: ≈ 68%대

→ 대규모/복잡한 VOC2012에서도 유사한 경향: YOLO는 빠르고 단순, 제안영역 기반은 더 정확하나 느림.

5.4 속도–정확도 트레이드오프

YOLO: 엔드-투-엔드 단일 네트워크 → 한 번의 전방향 패스로 결과 산출.

제안영역 기반: 수백~수천 개 영역에 특징을 반복 추출하고 분류/회귀 → 구조적으로 느림.

사용 맥락: 실시간 추적/로보틱스/AR 등 지연 예산이 작은 응용엔 YOLO가 현실적 선택.

6. 오류 분석 (Error Analysis)

에러를 (a) 분류 오류, (b) 로컬라이제이션 오류, (c) 배경 오류로 분해해 비교한다.

YOLO: 로컬라이제이션 오류가 상대적으로 큼. 한 셀당 한 객체 책임 구조(7×7 격자, B=2)가 작은 객체 밀집 상황에서 정밀한 위치 보정에 한계. 또한 너비/높이의 
	​

 파라미터화는 전반적 안정성은 좋으나, 극단적으로 작은 대상에서 민감도가 낮을 수 있다.

Fast R-CNN: 배경 오류가 큼. 제안영역에 배경 패치가 많이 포함되면 “없어야 할 곳에 객체”로 오검출(false positive)이 발생.

YOLO의 장점: 배경 오검출이 적음. 이유는 학습/추론 모두 전체 이미지 문맥을 보며 “그 장면에서 그 물체가 나올 법한가”를 내재적으로 학습하기 때문.

의미:
실시간 시스템에서 배경 오검출 감소는 **경보 폭주(false alarm flood)**를 줄인다. 반면 정밀한 박스 경계가 중요한 응용(정량 계측, 크기 추정 등)에서는 추가적인 박스 정제(post-refinement)나 멀티스케일 전략이 필요할 수 있다.

7. 새로운 도메인으로의 일반화 (Generalization to New Domains)

자연 이미지(VOC)로 학습한 모델을 **예술 작품(회화) 데이터(예: People-Art)**로 평가하면, R-CNN, DPM보다 YOLO가 더 높은 mAP을 보인다(예시: YOLO ≈ 53%, R-CNN ≈ 45%, DPM ≈ 38%). 이미지 스타일·텍스처·질감이 크게 달라져도, 전역 문맥을 학습하는 YOLO는 배경 혼동에 덜 취약해 도메인 이동(domain shift)에 더 강건하다.

8. 결론 (Conclusion)

YOLO는 객체 탐지를 단일 회귀 문제로 정식화하여, 하나의 CNN이 바운딩 박스와 클래스 확률을 한 번에 예측하도록 했다. 이는

단순성: 제안영역/후처리 다단계 파이프라인을 제거한 통합 모델

속도: 45 fps(YOLO), 155 fps(Fast YOLO)로 진정한 실시간

일반화: 배경 오검출 감소, 새 도메인으로의 이전에서 강건

이라는 장점을 제공한다. 반면 작은 객체 로컬라이제이션은 약점으로 남는다. 이후 연구(YOLOv2/YOLO9000, YOLOv3, …)는 앵커 박스, 멀티스케일 특징, 더 안정적인 박스 파라미터화 등으로 이 한계를 크게 줄였다. 그럼에도 원조 YOLO는 “You Only Look Once”라는 철학으로, 실시간 객체 탐지의 새 지평을 열었다.
