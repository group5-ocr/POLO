1) “좋은 박스 크기(앵커)”를 데이터로부터 자동으로 찾기

물체를 둘러싸는 박스의 가로·세로 비율은 데이터마다 다르잖아. 그래서 YOLOv2는 사람이 감으로 고르지 않고, k-means 클러스터링으로 훈련 데이터에 있는 박스 크기들을 묶어서 “자주 나오는 모양”을 뽑아내.

이때 “얼마나 비슷한지”를 재는 기준으로 **IOU(겹치는 정도)**를 써서, 크기가 큰 박스가 괜히 불리해지지 않게 했어. (거리 = 1 − IOU)

k(몇 개로 묶을지)를 바꿔보며 실험해 보니 k=5가 “모델이 너무 복잡해지지 않으면서도 잘 맞추는” 적당한 선택이었어.

이렇게 뽑힌 “대표 박스(=프라이어, 앵커)”는 사람이 손으로 고른 것과 모양이 달랐고, 낮고 넓은 박스보다 키가 크고 날씬한 박스가 더 많았어. COCO 데이터셋이 VOC보다 크기 변화가 더 다양하다는 점도 보였어.

결과적으로 데이터 기반으로 고른 5개 대표 박스가, 사람이 고른 9개 앵커와 비슷하게 잘 맞았고(평균 IOU 비슷), 9개 대표 박스를 쓰면 더 잘 맞았어. 즉, 데이터로 앵커를 뽑는 게 학습을 쉽게 하고 출발선을 앞당긴다는 뜻이야.

2) 박스 “위치”를 더 안정적으로 예측하기

기존 방식(일부 RPN 계열)은 앵커에서 **얼마나 이동할지(오프셋)**를 자유롭게 예측했어. 무작위 초기화 상태에선 앵커가 화면 어디로든 튈 수 있어서, 초반 학습이 불안정했지.

YOLOv2는 YOLO 철학을 따라, **격자 칸(cell) 안에서의 상대 위치(01 사이)**를 직접 예측하게 바꿨어. (가로·세로 중심은 시그모이드로 01에 “잠금”)
또 폭/높이는 앞에서 구한 대표 박스 크기에 비율을 곱하는 형태로 예측해, 말도 안 되는 크기가 나오지 않게 했어.

이렇게 예측 범위를 합리적으로 묶어주니 학습이 훨씬 안정적이 되었고, 성능도 약 5% 정도 개선되었어.

3) 작은 물체를 더 잘 잡기 위한 “세밀한 특징” 활용

기본 검출은 13×13 해상도의 특징맵에서 이루어져서 큰 물체엔 충분하지만, 작은 물체는 놓칠 수 있어.

YOLOv2는 중간 단계의 더 촘촘한 26×26 특징을 패스스루(passthrough) 레이어로 끌어와서 13×13 특징과 채널 방향으로 합쳐줘.
쉽게 말해, 고해상도 정보를 형태만 바꿔 쌓아 올려 작은 물체의 위치 정보를 보완하는 거야. (ResNet의 아이덴티티 연결과 비슷한 아이디어)

한눈에 요약

앵커는 “감”이 아니라 데이터로 뽑자: IOU 기반 k-means로 대표 박스(프라이어)를 만들면 시작부터 유리함.

위치 예측은 “셀 안에서 0~1”로 잠그자: 학습이 안정되고 성능이 오른다.

작은 물체는 고해상도 특징을 끌어와 보강: 패스스루로 26×26 정보를 13×13과 합쳐서 더 촘촘히 본다.