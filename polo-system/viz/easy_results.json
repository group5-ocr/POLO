{
  "paper_info": {
    "paper_id": "doc_-6816425174722030295_2767669",
    "paper_title": "논문 doc_-6816425174722030295_2767669",
    "paper_authors": "Unknown",
    "paper_venue": "Unknown",
    "total_sections": 17
  },
  "easy_sections": [
    {
      "easy_section_id": "easy_section_1",
      "easy_section_title": "Abstract",
      "easy_section_type": "section",
      "easy_section_order": 1,
      "easy_section_level": 1,
      "easy_content": "보조\n[초록]\n본 연구에서는 전체 이미지를 한 번만 보고 객체를 감지하는 방법인 'YOU LO'(You Only Look Once의 약자)를 제시합니다. 기존 연구(객체 탐지, 경계 상자 찾기 및 클래스 예측)에서는 분류기를 재활용하여 객체 발견을 수행했습니다. 객체 탐지를 공간적으로 분리된 경계 상자와 연관된 클래스(확률)를 갖는 회귀 문제(객체의 x, ...",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_1_1",
          "easy_paragraph_text": "보조\n[초록]\n본 연구에서는 전체 이미지를 한 번만 보고 객체를 감지하는 방법인 'YOU LO'(You Only Look Once의 약자)를 제시합니다. 기존 연구(객체 탐지, 경계 상자 찾기 및 클래스 예측)에서는 분류기를 재활용하여 객체 발견을 수행했습니다. 객체 탐지를 공간적으로 분리된 경계 상자와 연관된 클래스(확률)를 갖는 회귀 문제(객체의 x, y 좌표, 너비, 높이 등을 예측하는 문제)로 정의했습니다. 단일 신경망, 딥러닝 모델이 전체 이미지에서 경계 상자(객체의 x, y, w, h 등)와 클래스(객체의 확률)를 한 번의 평가 및 테스트를 통해 직접 예측합니다. 별도의 분류기와 위치 탐지기를 사용하는 대신, 하나의 네트워크가 모든 작업을 수행합니다. 별도의 위치 추정 및 분류 단계를 학습하는 대신, 전체 네트워크를 학습합니다. 파이프라인 방식 대신, 엔드 투 엔드 학습을 사용합니다. 별도의 모델 문자열 대신(마치 별도의 시각을 가진 체스와 같이), 예측, 필터링 등 모델) 하나의 모델이 모든 것을 수행합니다(빠르고 정확한 객체 감지). 저희의 통합 아키텍처는 매우 빠릅니다. 실시간(최소 **45fps**)에서 단일 네트워크(경계 상자 예측, 클래스 확률 등)를 사용하여 감지 성능(**mAP**, 평균 정밀도)을 빠르게 향상시킵니다. PASCAL VOC(고전 객체검출 데이터셋) 2012 테스트 세트(**mAP** **87.1%**)에서 YOLO는 이전 실시간 객체 감지기(MSCOCO **53.4%**, Shapem 55. 빠른 YOLO 75., DPM 54.6(), R-CNN 59.)보다 빠릅니다. 최종 점수: 0.875. 최고 점수: #1. 아트워크 감지(YOLO, 빠른 YOLo) DPM R-CNN Shapenet 기타 방법 **mAP** **mAP** **mAP** **mAP** mAPOCOKU FAST YOLO YOLO 아트워크 YOLO 탐지 Fast YOlo DPM R-CNN Shapenett **mAP** **mAP**.**mAP**.57.59.75.87 .60.54.56.59.53.55 Fast YOLOBest Score# 1 Artwork Detection **mAP** 0.75 WE WRITE IN KOREAN. YOLO(YOU ONLY LOOK ONCE)는 전체 이미지를 한 번만 보고 객체를 탐지하는 방법입니다. 이전 연구에서는 객체 탐지, 경계 상자 찾기, 클래스 예측에 대한 연구를 분류기를 재활용하여 발견을 수행했습니다. 대신, 객체 탐지(회귀 문제(객체의 x, y 좌표, 너비, 높이 등 예측))와 공간적으로 분리된 경계 상자 및 연관된 클래스(객체의 확률)로 구성됩니다. 단일 신경망, 즉 딥러닝 모델이 객체의 경계 상자(x 좌표, y 좌표, 너비, 높이 등)와 클래스(객체의 확률)를 예측합니다. 객체)를 직접(한 검색, 테스트에서 전체 이미지에서). 별도의 지역화 및 분류 단계 대신 하나의 네트워크가 무언가를 수행합니다. 파이프라인 프로세스의 엔드투엔드 학습 대신. 별도의 모델 문자열 대신(분리 비전, 예측, 필터링 등의 모델과 유사) 하나의 네트워크가 모든 것을 처리합니다(빠르고 정확한 객체 감지). 통합 아키텍처는 매우 빠릅니다(실시간(최소 **45fps**)으로 실행). 단일 네트워크(경계 상자, 클래스 확률 등을 빠르게 예측)가 감지 성능(mAP 평균 정밀도)에서 우수합니다(PASCAL VOC2012 테스트 세트 중 **mAP**(**87.1%**). YOLO(검출기 중 **mAP**에서 가장 빠름(빠른 YOLO53.M **mAP**, DPM54.6, R-CNN59.) 최종 점수: 0.875. 최고 점수: #1. 아트워크 감지(YOLO, Fast YOLo) 감지 **mAP**.575.750. 기타 감지기 **mAP** mAPP OCOKUFAST YOLOYOLO 아트워크 감지 mAPP.57.599.54.55 Fast YOLOBestscore# 1 아트워크 감지 mAP.75 [섹션] 소개\n이 섹션에서는 본 논문의 주요 내용을 이해하기 쉬운 언어로 설명합니다. , [코드], [설명] 패턴을 따르세요. 객체 감지는 이미지에 어떤 객체가 있고 어디에 있는지 찾는 것입니다. 현재 방법은 별도의 분류기를 사용하여 이미지에 어떤 객체가 있는지 예측한 다음, 로케이터를 사용하여 올바른 객체에 경계 상자를 그립니다. 이 두 단계는 종종 별도로 최적화되어 비효율적인 모델을 초래합니다. 본 연구에서는 객체 감지를 전체 이미지에서 공간적으로 분리된 경계 상자와 연관된 클래스 확률의 x, y 좌표, 너비, 높이 등을 예측하는 단일 회귀 문제로 정의합니다. in(onestsearch, testing). 단일 신경망(딥러닝 모델)은 경계 상자(객체의 x, y, 너비, 높이 등)와 독립적인 경계 상자를 예측합니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_2",
      "easy_section_title": "Introduction",
      "easy_section_type": "section",
      "easy_section_order": 2,
      "easy_section_level": 1,
      "easy_content": "보조\n인간은 이미지를 보는 순간 이미지 속에 어떤 물체가 있는지, 어디에 있는지, 어떻게 상호작용하는지 즉시 파악합니다. 인간의 시각 시스템은 빠르고 정확하여 운전과 같은 복잡한 작업을 의식적인 생각 없이 수행할 수 있습니다. 빠르고 정확한 물체 감지 알고리즘(객체 감지)은 컴퓨터가 특수 센서 없이도 자동차를 운전하고, 보조 장치가 실시간 현장 정보를 인간...",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_2_1",
          "easy_paragraph_text": "보조\n인간은 이미지를 보는 순간 이미지 속에 어떤 물체가 있는지, 어디에 있는지, 어떻게 상호작용하는지 즉시 파악합니다. 인간의 시각 시스템은 빠르고 정확하여 운전과 같은 복잡한 작업을 의식적인 생각 없이 수행할 수 있습니다. 빠르고 정확한 물체 감지 알고리즘(객체 감지)은 컴퓨터가 특수 센서 없이도 자동차를 운전하고, 보조 장치가 실시간 현장 정보를 인간 사용자에게 전달하도록 하며, 범용 반응형 로봇 시스템의 잠재력을 열어줄 것입니다. 현재 감지 시스템은 분류기를 재활용하여 감지를 수행합니다. 물체를 감지하기 위해 이러한 시스템은 해당 물체에 대한 분류기를 가져와 테스트 이미지의 다양한 위치와 크기에서 평가합니다. 변형 부품 모델(DPM)과 같은 시스템은 분류기가 전체 이미지의 균등한 간격에서 실행되는 슬라이딩 윈도우 방식을 사용합니다. ==R-CNN(두 단계 사물검출의 원형)과 같은 최신 접근 방식은 영역 제안 방법을 사용하여 먼저 이미지에서 잠재적인 경계 상자를 생성한 다음 제안된 상자에 대해 분류기를 실행합니다.== 분류 후 후처리를 사용하여 경계 상자를 세분화하고 중복 탐지를 제거하고 장면의 다른 객체를 기반으로 상자의 점수를 다시 매깁니다. 이러한 복잡한 파이프라인은 각 구성 요소를 별도로 학습해야 하므로 느리고 최적화하기 어렵습니다. 객체 감지를 이미지 픽셀에서 경계 상자(바운딩 박스) 좌표 및 클래스 확률로 직접 변환하는 단일 회귀 문제로 재구성합니다. YOLo(You Only Look Once의 약자)는 상쾌할 정도로 간단합니다. 그림을 참조하세요. 단일 합성곱 네트워크는 여러 경계 상자와 해당 상자에 대한 클래스 확률을 동시에 예측합니다. YOlo는 상쾌할 정도로 간단합니다. 그림을 참조하세요. 단일 합성곱 네트워크는 여러 경계 상자와 해당 상자에 대한 클래스 확률을 동시에 예측합니다.YOlo는 상쾌할 정도로 간단합니다.그림을 참조하세요.단일 합성곱 네트워크는 여러 경계 상자와 해당 상자에 대한 클래스 확률을 동시에 예측합니다.YOlo는 상쾌할 정도로 간단합니다.그림을 참조하세요.단일 합성곱 네트워크는 여러 경계 상자와 해당 상자에 대한 클래스 확률을 동시에 예측합니다.YOlo는 상쾌할 정도로 간단합니다.그림을 참조하세요.단일 합성곱 네트워크는 여러 경계 상자와 해당 상자에 대한 클래스 확률을 동시에 예측합니다.YOlo는 상쾌할 정도로 간단합니다.그림을 참조하세요.단일 합성곱 네트워크는 여러 경계 상자와 해당 상자에 대한 클래스 확률을 동시에 예측합니다.YoLI(YOU ONLY LOOK ONCE의 약자)는 상쾌할 정도로 간단합니다.그림을 참조하세요. 단일 합성 네트워크는 여러 개의 경계 상자와 해당 상자에 대한 클래스 확률을 동시에 예측합니다. YOLO(YOU ONLY LOOK ONCE의 약자)는 상쾌할 정도로 간단합니다.그림을 참조하세요.단일 컨볼루션 네트워크가 여러 바운딩 박스와 해당 박스에 대한 클래스 확률을 동시에 예측합니다.YOLO(YOU ONLY LOOK ONCE의 약자)는 상쾌할 정도로 간단합니다.그림을 참조하세요.단일 컨볼루션 네트워크가 여러 바운딩 박스와 직교 박스에 대한 클래스 확률을 동시에 예측합니다.YOLO(YOU ONLY LOOK ONCE의 약자)는 상쾌할 정도로 간단합니다.그림을 참조하세요.단일 컨볼루션 네트워크가 여러 바운딩 박스와 직교 박스에 대한 클래스 확률을 동시에 예측합니다. 해당 상자에 대한 클래스 확률.SYOLO(Your Only Look At Once의 약자)는 상쾌할 정도로 간단합니다.그림을 참조하세요.단일 합성곱 네트워크는 여러 경계 상자와 해당 상자에 대한 클래스 확률을 동시에 예측합니다.YOLO(YО U LY О N Ce의 약자)는 상쾌할 정도로 간단합니다.그림을 참조하세요. А Сinglе ConvolUsionAl Network는 THоsebOXе.SYOLO(YО У ЛО Н Ce의 약어)IS.RefreshIngLy SimPlE에 대한 다중 Bo UndInG 상자 및 ClАsS пробабilitиС를 동시에 예측합니다. 그림 E를 참조하세요. А Сингле конвольUTIONAL НЕТВORK.simultsaneuosly는 여러 경계 상자를 예측하고 해당 상자에 대한 클래스 확률을 제공합니다. YО Lо(줄여서 YО УЛО Н Ce)IS.새롭게 간단합니다. 그림을 참조하세요.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_3",
      "easy_section_title": "Unified Detection",
      "easy_section_type": "section",
      "easy_section_order": 3,
      "easy_section_level": 1,
      "easy_content": "보조\n(이미지 그리드)를 S × S 그리드로 나눕니다. 객체(object)의 중심이 그리드 셀에 속하면 해당 그리드 셀이 해당 객체를 감지합니다(Hit). 그리드 셀은 해당 객체(Hits)에 대한 S개의 경계 상자(x, y, w, h, conf, cls)를 예측합니다. 신뢰도(conf)는 상자(x, y, w, h)가 예측하는 모델이 기준 객체(x', y',...",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_3_1",
          "easy_paragraph_text": "보조\n(이미지 그리드)를 S × S 그리드로 나눕니다. 객체(object)의 중심이 그리드 셀에 속하면 해당 그리드 셀이 해당 객체를 감지합니다(Hit). 그리드 셀은 해당 객체(Hits)에 대한 S개의 경계 상자(x, y, w, h, conf, cls)를 예측합니다. 신뢰도(conf)는 상자(x, y, w, h)가 예측하는 모델이 기준 객체(x', y', w', h')와 얼마나 일치하는지 나타냅니다. ==해당 그리드 셀()에 객체(object)가 없으면 신뢰도 점수는 0이어야 합니다. 그렇지 않으면 신뢰도 점수가 예측된 상자(x,y,w,h)와 모든 지상 진실 객체(x',y',w',h') 사이의 합집합(IOU)에 대한 교집합과 같아야 합니다. 각 경계 상자(바운딩 박스)는 5개의 예측(x, y, w, h, conf)으로 구성됩니다.== x, y,:w, h 좌표는 그리드 셀의 경계에 대한 상자의 중심을 나타냅니다. 너비와 높이는 전체 이미지를 기준으로 예측됩니다. 마지막으로 신뢰도 예측은 예측된 상자와 모든 지상 진실 상자 사이의 IOU를 나타냅니다. 그리드 셀은 또한 S개의 조건부 클래스 확률 c_i를 예측하지 않습니다. 이러한 확률은 객체를 포함하는 그리드 셀을 기준으로 합니다.(적중 없음). (적중 1회). (다중 히트).테스트 시간에 조건부 클래스 확률과 개별 상자 신뢰도 예측을 곱합니다.(c_i|)* * c_i^는 각 상자에 대한 클래스별 신뢰도 점수를 제공합니다.(c_i)* (c_i)^는 해당 클래스가 상자에 나타날 확률과 예측된 상자가 개체에 얼마나 잘 맞는지를 모두 인코딩합니다.VOC에서 YOLOS를 평가하려면 다음을 설정합니다.VOC를 다운로드하세요.http://www.cs.ucl.ac.uk/~fxg/yolo/paper.html",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_4",
      "easy_section_title": "Network Design",
      "easy_section_type": "section",
      "easy_section_order": 4,
      "easy_section_level": 1,
      "easy_content": "보조\n(네트워크 설계)\nI. 서론\nII. 합성곱 계층\nIII. 완전 연결 계층\nIV. 감소 계층\nV. 합성곱 계층\nVI. 학습 VII. VOC 검출 데이터베이스 VIII. 이전 연구 IX. 서론 X. 네트워크 구조 XI. 합성곱 계층 XII. 완전 연결 계층 (XIV) 합성곱 계층은 합성곱 계층을 간단히 설명합니다. 합성곱 계층은 입력 이미지에서 저수준 특...",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_4_1",
          "easy_paragraph_text": "보조\n(네트워크 설계)\nI. 서론\nII. 합성곱 계층\nIII. 완전 연결 계층\nIV. 감소 계층\nV. 합성곱 계층\nVI. 학습 VII. VOC 검출 데이터베이스 VIII. 이전 연구 IX. 서론 X. 네트워크 구조 XI. 합성곱 계층 XII. 완전 연결 계층 (XIV) 합성곱 계층은 합성곱 계층을 간단히 설명합니다. 합성곱 계층은 입력 이미지에서 저수준 특징을 추출합니다. 이는 입력 이미지에 작은 커널을 적용하고 슬라이딩 윈도우 방식으로 이미지를 스캔하는 방식으로 수행됩니다. 각 커널은 커널 가중치와 입력 이미지의 해당 영역의 내적을 계산하고 이를 합산하여 특징 맵(mAP)을 생성합니다. 결과 특징 맵은 커널이 감지한 입력 이미지의 국소 패턴을 나타냅니다. 예를 들어, 합성곱 계층에는 에지를 나타내는 + - - + -와 같은 모양의 커널이 있을 수 있습니다. 이 커널은 입력 피처 **mAP**에서 에지를 감지하고 출력에서 에지를 나타내는 피처 **mAP**를 생성합니다. ==또한 ReLU와 같이 피처 맵에 적용되는 활성화 함수가 있는데, 입력이 0보다 작으면 0을 나타내고 그렇지 않으면 1을 나타냅니다.== 이렇게 하면 해당 계층의 출력이 0일 때 그래디언트가 이전 계층으로 흐르는 것을 방지할 수 있습니다. ==또한 각 합성곱 계층에는 커널 수와 커널 크기를 곱한 수의 출력이 있습니다.== 예를 들어 3x3 커널이 있는 경우 각 출력은 7개의 피처가 됩니다. 여러 합성곱 계층을 쌓아 피처 **mAP**의 피처 수를 늘립니다. 마지막으로 합성곱 계층의 출력을 완전히 연결된 계층과 연결하여 경계 상자(바운딩 박스) 확률과 좌표를 예측합니다. (V) TRAINING은 TRAINING을 간단한 말로 설명합니다. VOC 2007 테스트 세트에서 최대 우도 추정을 사용하여 YOLO(실시간 지오메트리 탐지 모델 계열(You Only Look Once))를 학습합니다. 먼저 이미지를 448x448로 크기를 조정하고 픽셀 값을 정규화하여 전처리합니다. 그런 다음 각 이미지를 7x7 경계 상자 그리드로 분할합니다. 각 그리드에 대해 x, y, w, h 및 신뢰도 점수가 있는 5개의 경계 상자를 예측합니다. 또한 20개 클래스 각각에 대한 클래스 확률을 예측합니다. 손실 함수는 예측된 탐지와 실제 지상 진실의 차이입니다. 신뢰도 손실은 실제 상자와 일치하지 않는 각 예측된 경계 상자에 대해 0.5(x^2+y^2)입니다. 클래스 손실은 실제 상자에 나타나는 각 클래스에 대해 log(probability + 1 – ptrue)이고 실제 상자에 나타나지 않는 클래스에 대해 log(p Predicted + .01)입니다. 모든 이미지의 모든 그라인딩 상자에 대해 이러한 개별 손실의 합을 최소화합니다. 최상의 **mAP** 이 글을 쓰는 시점에서 점수는 .42로, YOLO는 VOC 리더보드에서 가장 높은 점수를 받은 검출기입니다. Fast YOL은 Fast R-CNN(ROI 풀링으로 속도 개선된 R-CNN)과 동일한 최적화 기법을 많이 공유하지만, R-CNN 생성 파이프라인을 빠른 YOLO 프레임워크로 대체합니다. 또한 더 빠른 합성곱 계층을 사용하고 분석 영역 수를 줄입니다. Fast YOL은 속도가 중요한 빠른 객체 탐지(객체 탐지)에 이상적입니다. YOLO와 동일한 합성곱 계층을 공유함으로써 Fast YOLo는 빠른 추론 시간을 유지하면서도 더 나은 성능을 달성합니다. 이 글을 쓰는 시점에서 Fast YOL은 SSD-300, DPM과 함께 VOCDetection 리더보드에서 상위 3위를 차지하고 있습니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_5",
      "easy_section_title": "Training",
      "easy_section_type": "section",
      "easy_section_order": 5,
      "easy_section_level": 1,
      "easy_content": "보조\n[섹션] 훈련\n이 섹션에서는 네트워크 훈련 방법을 설명합니다. 이전: 네트워크 설계(신경망 설계). 다음: 추론(네트워크를 사용하여 예측). 이 부분은 이 글의 주요 내용입니다. ==[ TEXT]\nImage Net 1000-Class 경쟁 데이터세트에서 합성곱 계층을 사전 학습합니다. 사전 학습에는 그림의 처음 20개 합성곱 계층을 사용하고, 그 다음에 평균 풀링 계층과 완전 연결 계층을 사용합니다.== ==이 네트워크를 약 일주일 동안 학습시키고 Image Net 2012 검증 세트에서 **상위 5위**의 단일 크롭 정확도인 **88%**를 달성했습니다. 이는 Caffe의 Model Zoo에 있는 GoogLeNeT 모델과 유사합니다. 모든 학습 및 추론에는 Darknet 프레임워크를 사용합니다.== 그런 다음 모델을 변환하여 감지를 수행합니다. Ren 등은 사전 학습된 네트워크에 합성곱 계층과 연결 계층을 모두 추가하면 성능이 향상될 수 있음을 보여줍니다. 이들의 예시를 따라 합성곱 계층 4개와 완전 연결 계층 2개를 추가합니다. 무작위로 초기화된 가중치. 감지에는 종종 세밀한 시각 정보가 필요하므로 네트워크의 입력 해상도(입력 해상도)를 0에서 1로 높입니다. 최종 레이어는 클래스 확률과 바운딩 박스(바운딩 박스) 좌표를 모두 예측합니다. 바운딩 박스의 너비와 높이를 이미지의 너비와 높이로 정규화하여 0과 1 사이로 제한합니다. 바운딩 박스 좌표를 특정 그리드 셀 위치의 오프셋으로 매개변수화하여 0과 1 사이로 제한합니다. 최종 레이어에는 선형 활성화 함수를 사용하고 다른 모든 레이어에는 다음과 같은 누수 정류 선형 활성화 함수를 사용합니다. (X) = X, & X > 0 0. 1X, & 그렇지 않은 경우 모델 출력의 합계 제곱 오차를 최적화합니다. 모델 출력에 합계 제곱 오차를 사용합니다. 최적화하기 쉽지만 평균 정밀도를 극대화하려는 목표와 완벽하게 일치하지 않습니다. 지역화 오류를 분류 오류와 동일하게 가중치를 부여하는데, 이는 이상적이지 않을 수 있습니다. 또한 모든 이미지에서 많은 그리드 셀에는 객체가 포함되어 있지 않습니다. 이로 인해 해당 셀의 '신뢰도' 점수가 0에 가까워지고, 종종 객체를 포함하는 셀의 기울기가 더 강해집니다. 이는 모델 불안정성으로 이어져 학습 초기에 발산을 유발할 수 있습니다. 이를 해결하기 위해 경계 상자 좌표 예측의 손실을 늘리고 객체를 포함하지 않는 상자의 신뢰도 예측의 손실을 줄입니다. 이를 위해 두 개의 매개변수, ... 및 ...를 사용합니다. ... 및 ... 제곱합 오차는 큰 상자와 작은 상자의 오류에 동일하게 가중치를 부여합니다. Error_Metric은 큰 상자의 작은 편차가 작은 상자의 작은 편차보다 덜 중요하다는 것을 반영해야 합니다. 이 문제를 부분적으로 해결하기 위해 경계 상자 좌표 예측의 제곱근을 예측합니다. And_height를 직접 예측합니다. YOLo는 그리드 셀당 여러 개의 Bounding_box를 예측합니다. 학습 시에는 객체를 담당하는 Bounding_box 예측기를 하나만 사용합니다. 실제 값과 가장 높은 현재 **IoU**를 갖는 예측기를 기준으로 객체를 예측하는 데 하나의 예측기를 할당합니다. 이는 Bounding_box 예측기 간의 특수화로 이어집니다. 각 예측기는 특정 크기, 종횡비 또는 객체 클래스를 예측하는 데 더 능숙해져 전반적인 재현율이 향상됩니다. 학습 시에는 다음과 같은 다중 부분 손실 함수를 최적화합니다. _coord_S^2_B]^ij_i (Coord_i -_i)^2 + _S^2_B|^ij_i (_Coord_i -_ i)^2 + ^S^2_i^ (p_i(C_i -_.i)^2 여기서 는 객체가 Cell.And.는 Cell.의 Bounding_box Predator가 해당 예측에 대한 책임이 있음을 나타냅니다. Loss_function은 해당 Grid_cell에 객체가 있는 경우에만 Classification_Error에 페널티를 부여합니다(따라서 앞서 설명한 조건부 Class_probability). 또한 해당 예측자가 Ground_truth_box(해당 GridCell의 모든 예측자 중 가장 높은 **IoU**)에 대한 책임이 있는 경우에만 Bounding_box_coordinate_Error에 페널티를 부여합니다. VOC 2007 및 2012의 학습 및 검증 데이터 세트를 사용하여 약 135 에포크 동안 네트워크를 학습합니다(",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_5_1",
          "easy_paragraph_text": "보조\n[섹션] 훈련\n이 섹션에서는 네트워크 훈련 방법을 설명합니다. 이전: 네트워크 설계(신경망 설계). 다음: 추론(네트워크를 사용하여 예측). 이 부분은 이 글의 주요 내용입니다. ==[ TEXT]\nImage Net 1000-Class 경쟁 데이터세트에서 합성곱 계층을 사전 학습합니다. 사전 학습에는 그림의 처음 20개 합성곱 계층을 사용하고, 그 다음에 평균 풀링 계층과 완전 연결 계층을 사용합니다.== ==이 네트워크를 약 일주일 동안 학습시키고 Image Net 2012 검증 세트에서 **상위 5위**의 단일 크롭 정확도인 **88%**를 달성했습니다. 이는 Caffe의 Model Zoo에 있는 GoogLeNeT 모델과 유사합니다. 모든 학습 및 추론에는 Darknet 프레임워크를 사용합니다.== 그런 다음 모델을 변환하여 감지를 수행합니다. Ren 등은 사전 학습된 네트워크에 합성곱 계층과 연결 계층을 모두 추가하면 성능이 향상될 수 있음을 보여줍니다. 이들의 예시를 따라 합성곱 계층 4개와 완전 연결 계층 2개를 추가합니다. 무작위로 초기화된 가중치. 감지에는 종종 세밀한 시각 정보가 필요하므로 네트워크의 입력 해상도(입력 해상도)를 0에서 1로 높입니다. 최종 레이어는 클래스 확률과 바운딩 박스(바운딩 박스) 좌표를 모두 예측합니다. 바운딩 박스의 너비와 높이를 이미지의 너비와 높이로 정규화하여 0과 1 사이로 제한합니다. 바운딩 박스 좌표를 특정 그리드 셀 위치의 오프셋으로 매개변수화하여 0과 1 사이로 제한합니다. 최종 레이어에는 선형 활성화 함수를 사용하고 다른 모든 레이어에는 다음과 같은 누수 정류 선형 활성화 함수를 사용합니다. (X) = X, & X > 0 0. 1X, & 그렇지 않은 경우 모델 출력의 합계 제곱 오차를 최적화합니다. 모델 출력에 합계 제곱 오차를 사용합니다. 최적화하기 쉽지만 평균 정밀도를 극대화하려는 목표와 완벽하게 일치하지 않습니다. 지역화 오류를 분류 오류와 동일하게 가중치를 부여하는데, 이는 이상적이지 않을 수 있습니다. 또한 모든 이미지에서 많은 그리드 셀에는 객체가 포함되어 있지 않습니다. 이로 인해 해당 셀의 '신뢰도' 점수가 0에 가까워지고, 종종 객체를 포함하는 셀의 기울기가 더 강해집니다. 이는 모델 불안정성으로 이어져 학습 초기에 발산을 유발할 수 있습니다. 이를 해결하기 위해 경계 상자 좌표 예측의 손실을 늘리고 객체를 포함하지 않는 상자의 신뢰도 예측의 손실을 줄입니다. 이를 위해 두 개의 매개변수, ... 및 ...를 사용합니다. ... 및 ... 제곱합 오차는 큰 상자와 작은 상자의 오류에 동일하게 가중치를 부여합니다. Error_Metric은 큰 상자의 작은 편차가 작은 상자의 작은 편차보다 덜 중요하다는 것을 반영해야 합니다. 이 문제를 부분적으로 해결하기 위해 경계 상자 좌표 예측의 제곱근을 예측합니다. And_height를 직접 예측합니다. YOLo는 그리드 셀당 여러 개의 Bounding_box를 예측합니다. 학습 시에는 객체를 담당하는 Bounding_box 예측기를 하나만 사용합니다. 실제 값과 가장 높은 현재 **IoU**를 갖는 예측기를 기준으로 객체를 예측하는 데 하나의 예측기를 할당합니다. 이는 Bounding_box 예측기 간의 특수화로 이어집니다. 각 예측기는 특정 크기, 종횡비 또는 객체 클래스를 예측하는 데 더 능숙해져 전반적인 재현율이 향상됩니다. 학습 시에는 다음과 같은 다중 부분 손실 함수를 최적화합니다. _coord_S^2_B]^ij_i (Coord_i -_i)^2 + _S^2_B|^ij_i (_Coord_i -_ i)^2 + ^S^2_i^ (p_i(C_i -_.i)^2 여기서 는 객체가 Cell.And.는 Cell.의 Bounding_box Predator가 해당 예측에 대한 책임이 있음을 나타냅니다. Loss_function은 해당 Grid_cell에 객체가 있는 경우에만 Classification_Error에 페널티를 부여합니다(따라서 앞서 설명한 조건부 Class_probability). 또한 해당 예측자가 Ground_truth_box(해당 GridCell의 모든 예측자 중 가장 높은 **IoU**)에 대한 책임이 있는 경우에만 Bounding_box_coordinate_Error에 페널티를 부여합니다. VOC 2007 및 2012의 학습 및 검증 데이터 세트를 사용하여 약 135 에포크 동안 네트워크를 학습합니다(",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_6",
      "easy_section_title": "Inference",
      "easy_section_type": "section",
      "easy_section_order": 6,
      "easy_section_level": 1,
      "easy_content": "영어: assistant 훈련(더 많은 데이터 만들기)과 마찬가지로 테스트 이미지 예측(실제 이미지 보기)을 위해 네트워크 크기를 제한합니다(더 크게 만들지 않음).==VOC(Common Objects in Context) 경쟁에서 네트워크는 이미지당 98개의 경계 상자(객체를 표시하는 사각형)와 각 상자의 확률을 예측(그립니다).== YoLo(You Only Look Once의 약자로, 객체를 감지하기 위해 이미지를 한 번 보는 방법)는 분류기 기반 방법(다른 이미지 분류 방법)과 달리 테스트 시간(기계에서 실제 이미지를 보는 시간)에 매우 빠릅니다.그리드 디자인(그리드처럼 이미지에 순서와 구조를 적용)은 경계 상자(객체를 표시하는 사각형) 예측에서 공간적 다양성을 적용합니다. 종종 객체가 어떤 그리드 셀(이미지를 스타일로 표시된 하나의 셀)에 속하는지 명확하고 네트워크는 각 객체에 대해 하나의 상자만 예측(그립니다). 그러나 여러 셀의 경계 근처에 있는 일부 큰 객체나 객체는 여러 셀에 의해 잘 국소화될 수 있습니다. 비최대 억제는 이러한 다중 탐지를 수정하는 데 사용할 수 있습니다. ==R-CNN(신경망, 복잡한 컴퓨터 모델)이나 DPM(변형 부품 모델, 또 다른 객체 탐지(객체 탐지) 모델의 경우 성능에 중요하지 않지만, 비최대 억제는 **mAP**(평균 평균 정밀도, 정확도 측정)에서 0.2-**0.3%**를 추가합니다.== [토큰을 \".Koreaize\" 함수로 바꾸기] .replace(\"VOC\", \"VOCCommonObjectsInContext\"). .replace(\"R-CNN\", \"ConvolutionalNeuralNetwork\"). .replace(\"DPM\", \"DeformablePartsModel\"). ._replace(\"**mAP**\", \"meanaverageprecision\"). .Replace(\"YoLo\", \"YouOnlyLookOnce\"). . Replace(\"grid\", \"grid(ColorCode_0 +'' + ColorCode_1 +'' ++ 'Grid')\"). ..replace(\"Box\", \"rectangle(ObjectName)_boundingBox\"). .reduce(str, old) => old + str, \"\"). 결과:\n. VOCCommonObjectsinContext(Common Objects in Context의 약자, 나중에 설명) 경쟁에서 네트워크는 이미지당 98개의 경계 사각형(ObjectName)_박싱을 예측(그리기)하고, 각 사각형의 확률(ObjectName_박싱)을 예측합니다. YouOnlyLookOnce(Your Only Look Once의 약자, 객체를 감지하기 위해 한 번만 보는 방법, 나중에 설명)는 분류기 기반 방법(다른 이미지 분류 방법)과 달리 매우 빠릅니다. 그리드 디자인(이미지 유사 그리드에 순서와 구조를 적용)은 경계 사각형(ObjectName)_박싱 예측에 공간적 다양성을 적용합니다. 종종 객체가 어떤 그리드 셀에 속하는지 명확하며, 네트워크는 각 객체에 대해 하나의 상자만 예측(그리기)합니다. 그러나 일부 큰 객체 또는 객체가 여러 셀에 근접하면 여러 셀에 의해 잘 국소화될 수 있습니다. 비최대값 억제는 이러한 여러 감지를 수정하는 데 사용됩니다. 합성곱 신경망(복잡한 컴퓨터 모델)이나 변형 가능 부품 모델(다른 객체 감지 모델)의 경우처럼 성능에 중요한 것은 아니지만, 비초극대값 억제는 평균 평균 정밀도(정확도 측정)에서 0.2~**0.33%**를 추가합니다. [\"Koreaize\" 함수에 따라 다시 작성] .on VOC/CommonObjectsIn_context/ Common Objects In Context/의 약자로, 나중에 설명합니다. replace(\"VOC\", \"VOccommonObjectsinContext\")\n.on R-CNN/ 합성곱 신경망/ 복잡한 컴퓨터 모델/ replace(\"R-CN/N/ \" \"합성곱 신경망/ CONVOLUTIONAL_NEURAL_NETWORK\")\n.on DPM/ 변형 가능 부품 모델/ 다른 객체 감지 모델/ replace(\"DPM\" \"DEFORMABLEPARTSMODEL\")\n.on **mAP**/ 평균 평균 정밀도/ 정확도 측정/ replace(\"**mAP**\" \"MEANAVE Rage PRECISION\")\n.on YoLo/ You Only Look One/ once/ method that looks_atonce/ object detection/ later explain.replace(\"YoLo\" \"YOUONLYLOOKONCE\")\n.on grid/ grid(ColorCode _ 0 + '(' + ColorCode _ 1 + ')' + '__GRID')/ replace(\"grid\" \"grid(COLORCODE_0 + '_' + COLORCODE_1 + '_GRID')\")\n.on Box/ rectangle(ObjectName _) boundingBox/ replace(\"Box\" \"rectangleObjectName_\")\n.on Ap/ mean average Precision/ accuracy/ replace (\"Ap\" \"MEANAVERAGEPRECISION\")\n.reduce(str,old)=>old+str,\"\"). 결과:\n.on COMMONOBJECTSinCONTEXT/ COMMONOBJECTSin CONTEXT_의 약자/ later explain",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_6_1",
          "easy_paragraph_text": "영어: assistant 훈련(더 많은 데이터 만들기)과 마찬가지로 테스트 이미지 예측(실제 이미지 보기)을 위해 네트워크 크기를 제한합니다(더 크게 만들지 않음).==VOC(Common Objects in Context) 경쟁에서 네트워크는 이미지당 98개의 경계 상자(객체를 표시하는 사각형)와 각 상자의 확률을 예측(그립니다).== YoLo(You Only Look Once의 약자로, 객체를 감지하기 위해 이미지를 한 번 보는 방법)는 분류기 기반 방법(다른 이미지 분류 방법)과 달리 테스트 시간(기계에서 실제 이미지를 보는 시간)에 매우 빠릅니다.그리드 디자인(그리드처럼 이미지에 순서와 구조를 적용)은 경계 상자(객체를 표시하는 사각형) 예측에서 공간적 다양성을 적용합니다. 종종 객체가 어떤 그리드 셀(이미지를 스타일로 표시된 하나의 셀)에 속하는지 명확하고 네트워크는 각 객체에 대해 하나의 상자만 예측(그립니다). 그러나 여러 셀의 경계 근처에 있는 일부 큰 객체나 객체는 여러 셀에 의해 잘 국소화될 수 있습니다. 비최대 억제는 이러한 다중 탐지를 수정하는 데 사용할 수 있습니다. ==R-CNN(신경망, 복잡한 컴퓨터 모델)이나 DPM(변형 부품 모델, 또 다른 객체 탐지(객체 탐지) 모델의 경우 성능에 중요하지 않지만, 비최대 억제는 **mAP**(평균 평균 정밀도, 정확도 측정)에서 0.2-**0.3%**를 추가합니다.== [토큰을 \".Koreaize\" 함수로 바꾸기] .replace(\"VOC\", \"VOCCommonObjectsInContext\"). .replace(\"R-CNN\", \"ConvolutionalNeuralNetwork\"). .replace(\"DPM\", \"DeformablePartsModel\"). ._replace(\"**mAP**\", \"meanaverageprecision\"). .Replace(\"YoLo\", \"YouOnlyLookOnce\"). . Replace(\"grid\", \"grid(ColorCode_0 +'' + ColorCode_1 +'' ++ 'Grid')\"). ..replace(\"Box\", \"rectangle(ObjectName)_boundingBox\"). .reduce(str, old) => old + str, \"\"). 결과:\n. VOCCommonObjectsinContext(Common Objects in Context의 약자, 나중에 설명) 경쟁에서 네트워크는 이미지당 98개의 경계 사각형(ObjectName)_박싱을 예측(그리기)하고, 각 사각형의 확률(ObjectName_박싱)을 예측합니다. YouOnlyLookOnce(Your Only Look Once의 약자, 객체를 감지하기 위해 한 번만 보는 방법, 나중에 설명)는 분류기 기반 방법(다른 이미지 분류 방법)과 달리 매우 빠릅니다. 그리드 디자인(이미지 유사 그리드에 순서와 구조를 적용)은 경계 사각형(ObjectName)_박싱 예측에 공간적 다양성을 적용합니다. 종종 객체가 어떤 그리드 셀에 속하는지 명확하며, 네트워크는 각 객체에 대해 하나의 상자만 예측(그리기)합니다. 그러나 일부 큰 객체 또는 객체가 여러 셀에 근접하면 여러 셀에 의해 잘 국소화될 수 있습니다. 비최대값 억제는 이러한 여러 감지를 수정하는 데 사용됩니다. 합성곱 신경망(복잡한 컴퓨터 모델)이나 변형 가능 부품 모델(다른 객체 감지 모델)의 경우처럼 성능에 중요한 것은 아니지만, 비초극대값 억제는 평균 평균 정밀도(정확도 측정)에서 0.2~**0.33%**를 추가합니다. [\"Koreaize\" 함수에 따라 다시 작성] .on VOC/CommonObjectsIn_context/ Common Objects In Context/의 약자로, 나중에 설명합니다. replace(\"VOC\", \"VOccommonObjectsinContext\")\n.on R-CNN/ 합성곱 신경망/ 복잡한 컴퓨터 모델/ replace(\"R-CN/N/ \" \"합성곱 신경망/ CONVOLUTIONAL_NEURAL_NETWORK\")\n.on DPM/ 변형 가능 부품 모델/ 다른 객체 감지 모델/ replace(\"DPM\" \"DEFORMABLEPARTSMODEL\")\n.on **mAP**/ 평균 평균 정밀도/ 정확도 측정/ replace(\"**mAP**\" \"MEANAVE Rage PRECISION\")\n.on YoLo/ You Only Look One/ once/ method that looks_atonce/ object detection/ later explain.replace(\"YoLo\" \"YOUONLYLOOKONCE\")\n.on grid/ grid(ColorCode _ 0 + '(' + ColorCode _ 1 + ')' + '__GRID')/ replace(\"grid\" \"grid(COLORCODE_0 + '_' + COLORCODE_1 + '_GRID')\")\n.on Box/ rectangle(ObjectName _) boundingBox/ replace(\"Box\" \"rectangleObjectName_\")\n.on Ap/ mean average Precision/ accuracy/ replace (\"Ap\" \"MEANAVERAGEPRECISION\")\n.reduce(str,old)=>old+str,\"\"). 결과:\n.on COMMONOBJECTSinCONTEXT/ COMMONOBJECTSin CONTEXT_의 약자/ later explain",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_7",
      "easy_section_title": "Limitations of YOLO",
      "easy_section_type": "section",
      "easy_section_order": 7,
      "easy_section_level": 1,
      "easy_content": "보조\nYOLOSE(객체 탐지 제한을 위한 손실 함수, 'You Only Look Once Loss of Explanation'의 약자). ==이 섹션은 전체 기사를 설명하는 17개 섹션 중 7번째입니다.== 이전: 추론(이미지에서 모델 실행). 다음: 다른 탐지 시스템과의 비교(YOLOSE와 다른 객체 탐지 시스템의 비교). 이 섹션은 주요 섹션입니다. YEOLOSE(객체 탐지 손실 함수, 'You Only Look Once Loss of Explanation'의 약자)는 각 그리드 셀이 두 개의 상자만 예측하고 하나의 클래스만 가질 수 있기 때문에 경계 상자 예측에 강력한 공간 제약을 부과합니다. ==이 공간 제약은 모델이 예측할 수 있는 근처 객체의 수를 제한합니다. 모델은 새 떼와 같이 그룹으로 나타나는 작은 객체에 어려움을 겪습니다.== 모델은 데이터에서 경계 상자를 예측하는 방법을 학습하기 때문에 새롭거나 특이한 종횡비를 가진 객체로 일반화하는 데 어려움을 겪습니다. 또는 구성. 저희 모델은 입력 이미지에서 여러 개의 다운샘플링 학습기를 사용하는 아키텍처를 사용하므로 경계 상자 예측에 비교적 거친 표현을 사용합니다. 마지막으로, 탐지 성능 근사치에 대한 훈련 손실에서 손실 함수는 작은 경계 상자와 큰 경계 상자 모두에서 동일한 오류를 처리합니다. 큰 상자의 작은 오류는 일반적으로 양성이지만, 가장 작은 상자의 작은 오류는 IOU에 훨씬 더 큰 영향을 미칩니다. 오류의 주요 원인은 잘못된 지역화입니다. [한글로 다시 쓰기]\nYOLOSE('YouOnly Look Once Loss of Explanation'의 약자, 객체 탐지용 손실 함수) 한계. 이 섹션은 전체 글을 설명하는 17개 섹션 중 7번째 섹션입니다. 이전: 추론(이미지에서 모델 실행). 다음: 다른 탐지 시스템과의 비교(YOLOSE와 다른 객체 탐지 시스템의 비교). 이 섹션은 주요 섹션입니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_7_1",
          "easy_paragraph_text": "보조\nYOLOSE(객체 탐지 제한을 위한 손실 함수, 'You Only Look Once Loss of Explanation'의 약자). ==이 섹션은 전체 기사를 설명하는 17개 섹션 중 7번째입니다.== 이전: 추론(이미지에서 모델 실행). 다음: 다른 탐지 시스템과의 비교(YOLOSE와 다른 객체 탐지 시스템의 비교). 이 섹션은 주요 섹션입니다. YEOLOSE(객체 탐지 손실 함수, 'You Only Look Once Loss of Explanation'의 약자)는 각 그리드 셀이 두 개의 상자만 예측하고 하나의 클래스만 가질 수 있기 때문에 경계 상자 예측에 강력한 공간 제약을 부과합니다. ==이 공간 제약은 모델이 예측할 수 있는 근처 객체의 수를 제한합니다. 모델은 새 떼와 같이 그룹으로 나타나는 작은 객체에 어려움을 겪습니다.== 모델은 데이터에서 경계 상자를 예측하는 방법을 학습하기 때문에 새롭거나 특이한 종횡비를 가진 객체로 일반화하는 데 어려움을 겪습니다. 또는 구성. 저희 모델은 입력 이미지에서 여러 개의 다운샘플링 학습기를 사용하는 아키텍처를 사용하므로 경계 상자 예측에 비교적 거친 표현을 사용합니다. 마지막으로, 탐지 성능 근사치에 대한 훈련 손실에서 손실 함수는 작은 경계 상자와 큰 경계 상자 모두에서 동일한 오류를 처리합니다. 큰 상자의 작은 오류는 일반적으로 양성이지만, 가장 작은 상자의 작은 오류는 IOU에 훨씬 더 큰 영향을 미칩니다. 오류의 주요 원인은 잘못된 지역화입니다. [한글로 다시 쓰기]\nYOLOSE('YouOnly Look Once Loss of Explanation'의 약자, 객체 탐지용 손실 함수) 한계. 이 섹션은 전체 글을 설명하는 17개 섹션 중 7번째 섹션입니다. 이전: 추론(이미지에서 모델 실행). 다음: 다른 탐지 시스템과의 비교(YOLOSE와 다른 객체 탐지 시스템의 비교). 이 섹션은 주요 섹션입니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_8",
      "easy_section_title": "Comparison to Other Detection Systems",
      "easy_section_type": "section",
      "easy_section_order": 8,
      "easy_section_level": 1,
      "easy_content": "보조\n(YOLO(실시간 지오메트리 탐지 모델 계열(You Only Look Once)를 다른 탐지 시스템과 비교) 이 섹션에서는 YOLO가 다른 탐지 시스템과 어떻게 비교되는지 설명합니다. ==이 글의 8번째 섹션에 대한 설명은 YOLO의 한계를 참조하십시오.== 다음은 한국어로 다시 작성된 텍스트입니다. 객체 탐지(객체 탐지)는 컴퓨터 비전의 핵심 문제입니다. 탐지 파이프라인은 일반적으로 입력 이미지에서 견고한 특징 집합(Haar, SIFT, HOG, 합성곱 특징)을 추출하는 것으로 시작합니다. 그런 다음 분류기 또는 로컬라이저를 사용하여 특징 공간에서 객체를 식별합니다. Haar, Sift, Hog, 합성곱 특징. 그런 다음 분류기 또는 로컬라이저를 사용하여 특징 공간에서 객체를 식별합니다. ==Hog 계산 속도를 높이고, Cascades를 사용하고, GPU로 계산을 전송하지만 실제로 실시간으로 실행된 DPM은 300nH에 불과합니다.== 대규모 탐지 파이프라인의 개별 구성 요소를 최적화하는 대신, YOLOS는 파이프라인 전체를 없애고 설계상 빠릅니다. 얼굴이나 사람과 같은 단일 클래스에 대한 검출기는 훨씬 적은 변동을 처리해야 하므로 고도로 최적화될 수 있습니다. YOLOS는 다양한 객체를 동시에 검출하는 범용 검출기입니다. 검출 파이프라인은 일반적으로 입력에서 견고한 특징을 추출하는 것으로 시작합니다. 이미지(Haar, SIFT, HOG, 합성곱 특징). 그런 다음 분류기 또는 지역화기를 사용하여 특징 간격의 객체를 식별합니다.Haar, SIFT-HOG 합성곱 특징 - 이미지에서 추출한 특징입니다.R-CNN(상위 파이프라인)과 YOLO(섹션 비교)를 비교합니다.다른 감지 시스템과의 객체 감지는 컴퓨터 비전의 핵심 문제입니다.감지 파이프라인은 일반적으로 입력 이미지(Haar, SITT, HOG, 합성곱 특징)에서 크기를 조정하여 강력한 특징을 추출하는 것으로 시작합니다.그런 다음 분류기 또는 지역화기를 사용하여 특징 간격의 객체를 식별합니다.HAAR, SIFT-HOG 합성곱 특징 - 이미지에서 추출한 특징입니다.DPM(DephtFartModels)(기사 섹션 8) 어떤 프레임워크가 3가지 특성을 모두 충족합니까? DPM은 슬라이딩 윈도우 방식을 사용하여 객체 감지를 수행합니다. 하지만 DPM은 슬라이딩 윈도우 방식을 사용하여 전체 이미지에서 객체를 테스트합니다. YOLO는 이러한 모든 분리된 파이프라인을 대체하여 특징 추출, 박싱 예측, 비최대 억제 및 상황 추론을 동시에 수행합니다. 네트워크는 검색, 박싱 예측, 비최대 억제 및 상황 추론을 모두 동시에 수행합니다. 정적 특징 대신 DPM은 특징을 인라인으로 학습하고 감지 작업에 최적화합니다. 통합 아키텍처는 DPM보다 더 빠르고 효율적인 모델을 제공합니다. R-CNN(R-합성곱 신경망)(14번째 섹션) R-CNN 및 그 변형은 슬링 윈도우 대신 영역 제안을 사용합니다. 이미지 참조의 객체에 대해 선택적 검색을 통해 잠재적 경계 상자, 합성곱 네트워크 추출 기능, SVM 점수, 선 모델 조정 경계 상자, 비최대값 억제를 통해 감지 제거 등의 작업을 수행합니다. 이 복잡한 파이프라인의 각 단계는 독립적으로 정밀하게 조정되어야 하며, 그 결과 시스템은 30초 간격으로 설정됩니다. 그러나 YOLO는 R-CNN과 몇 가지 유사점을 공유합니다. 각 그리드 셀은 합성곱 기능을 사용하여 잠재적 경계 상자와 해당 상자를 제안합니다. 그러나 저희 시스템은 그리드 도메인 상자에 공간 제약 조건을 두어 동일한 객체에 대한 여러 감지를 시작합니다. 저희 시스템은 또한 훨씬 적은 경계 상자를 제안합니다. 이미지당 98개에 불과한 반면 선택적 검색에서는 약 2000개가 제안됩니다. 그러나 저희 시스템은 개별 구성 요소를 단일 사고 모델. FastandFasterRCNN(fastDetCNN)((기사) FastandFaster RCNN의 15번째 섹션은 계산을 공유하고 직렬 신경망을 사용하여 R-CNN 프레임워크를 가속화하는 데 중점을 둡니다. 그들은 PM 파이프라인, 즉 hog-computation-gpu-acceleration을 가속화했지만 실제로는 30Hz-dpm만 실시간으로 실행되었습니다. 대규모 감지 파이프라인, 즉 fast-hog-fastrcnn의 개별 구성 요소를 최적화하려고 시도하는 대신 파이프라인을 완전히 버리고 설계상 빠릅니다. 단일 클래스용 감지기는 얼굴이나 사람을 감지할 수 있습니다. 그들은 일반적인 목적의 검출기인 일반적이고 덜 변이적인 YoLO를 다루었기 때문에 매우 최적화되어 있으며, 다양한 객체를 동시에 감지하는 법을 학습합니다. DeathDetector(Deth)MultiBox(DeepMultiBox)(16thSectionof.article) RCNN과 달리 Szeged는 아직 선택적 영역 대신 영역 분할을 위한 합성곱 신경망을 학습합니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_8_1",
          "easy_paragraph_text": "보조\n(YOLO(실시간 지오메트리 탐지 모델 계열(You Only Look Once)를 다른 탐지 시스템과 비교) 이 섹션에서는 YOLO가 다른 탐지 시스템과 어떻게 비교되는지 설명합니다. ==이 글의 8번째 섹션에 대한 설명은 YOLO의 한계를 참조하십시오.== 다음은 한국어로 다시 작성된 텍스트입니다. 객체 탐지(객체 탐지)는 컴퓨터 비전의 핵심 문제입니다. 탐지 파이프라인은 일반적으로 입력 이미지에서 견고한 특징 집합(Haar, SIFT, HOG, 합성곱 특징)을 추출하는 것으로 시작합니다. 그런 다음 분류기 또는 로컬라이저를 사용하여 특징 공간에서 객체를 식별합니다. Haar, Sift, Hog, 합성곱 특징. 그런 다음 분류기 또는 로컬라이저를 사용하여 특징 공간에서 객체를 식별합니다. ==Hog 계산 속도를 높이고, Cascades를 사용하고, GPU로 계산을 전송하지만 실제로 실시간으로 실행된 DPM은 300nH에 불과합니다.== 대규모 탐지 파이프라인의 개별 구성 요소를 최적화하는 대신, YOLOS는 파이프라인 전체를 없애고 설계상 빠릅니다. 얼굴이나 사람과 같은 단일 클래스에 대한 검출기는 훨씬 적은 변동을 처리해야 하므로 고도로 최적화될 수 있습니다. YOLOS는 다양한 객체를 동시에 검출하는 범용 검출기입니다. 검출 파이프라인은 일반적으로 입력에서 견고한 특징을 추출하는 것으로 시작합니다. 이미지(Haar, SIFT, HOG, 합성곱 특징). 그런 다음 분류기 또는 지역화기를 사용하여 특징 간격의 객체를 식별합니다.Haar, SIFT-HOG 합성곱 특징 - 이미지에서 추출한 특징입니다.R-CNN(상위 파이프라인)과 YOLO(섹션 비교)를 비교합니다.다른 감지 시스템과의 객체 감지는 컴퓨터 비전의 핵심 문제입니다.감지 파이프라인은 일반적으로 입력 이미지(Haar, SITT, HOG, 합성곱 특징)에서 크기를 조정하여 강력한 특징을 추출하는 것으로 시작합니다.그런 다음 분류기 또는 지역화기를 사용하여 특징 간격의 객체를 식별합니다.HAAR, SIFT-HOG 합성곱 특징 - 이미지에서 추출한 특징입니다.DPM(DephtFartModels)(기사 섹션 8) 어떤 프레임워크가 3가지 특성을 모두 충족합니까? DPM은 슬라이딩 윈도우 방식을 사용하여 객체 감지를 수행합니다. 하지만 DPM은 슬라이딩 윈도우 방식을 사용하여 전체 이미지에서 객체를 테스트합니다. YOLO는 이러한 모든 분리된 파이프라인을 대체하여 특징 추출, 박싱 예측, 비최대 억제 및 상황 추론을 동시에 수행합니다. 네트워크는 검색, 박싱 예측, 비최대 억제 및 상황 추론을 모두 동시에 수행합니다. 정적 특징 대신 DPM은 특징을 인라인으로 학습하고 감지 작업에 최적화합니다. 통합 아키텍처는 DPM보다 더 빠르고 효율적인 모델을 제공합니다. R-CNN(R-합성곱 신경망)(14번째 섹션) R-CNN 및 그 변형은 슬링 윈도우 대신 영역 제안을 사용합니다. 이미지 참조의 객체에 대해 선택적 검색을 통해 잠재적 경계 상자, 합성곱 네트워크 추출 기능, SVM 점수, 선 모델 조정 경계 상자, 비최대값 억제를 통해 감지 제거 등의 작업을 수행합니다. 이 복잡한 파이프라인의 각 단계는 독립적으로 정밀하게 조정되어야 하며, 그 결과 시스템은 30초 간격으로 설정됩니다. 그러나 YOLO는 R-CNN과 몇 가지 유사점을 공유합니다. 각 그리드 셀은 합성곱 기능을 사용하여 잠재적 경계 상자와 해당 상자를 제안합니다. 그러나 저희 시스템은 그리드 도메인 상자에 공간 제약 조건을 두어 동일한 객체에 대한 여러 감지를 시작합니다. 저희 시스템은 또한 훨씬 적은 경계 상자를 제안합니다. 이미지당 98개에 불과한 반면 선택적 검색에서는 약 2000개가 제안됩니다. 그러나 저희 시스템은 개별 구성 요소를 단일 사고 모델. FastandFasterRCNN(fastDetCNN)((기사) FastandFaster RCNN의 15번째 섹션은 계산을 공유하고 직렬 신경망을 사용하여 R-CNN 프레임워크를 가속화하는 데 중점을 둡니다. 그들은 PM 파이프라인, 즉 hog-computation-gpu-acceleration을 가속화했지만 실제로는 30Hz-dpm만 실시간으로 실행되었습니다. 대규모 감지 파이프라인, 즉 fast-hog-fastrcnn의 개별 구성 요소를 최적화하려고 시도하는 대신 파이프라인을 완전히 버리고 설계상 빠릅니다. 단일 클래스용 감지기는 얼굴이나 사람을 감지할 수 있습니다. 그들은 일반적인 목적의 검출기인 일반적이고 덜 변이적인 YoLO를 다루었기 때문에 매우 최적화되어 있으며, 다양한 객체를 동시에 감지하는 법을 학습합니다. DeathDetector(Deth)MultiBox(DeepMultiBox)(16thSectionof.article) RCNN과 달리 Szeged는 아직 선택적 영역 대신 영역 분할을 위한 합성곱 신경망을 학습합니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_9",
      "easy_section_title": "Experiments",
      "easy_section_type": "section",
      "easy_section_order": 9,
      "easy_section_level": 1,
      "easy_content": "==보조\n[SECTION] 실험(実験)\n이 섹션은 toànbody 17개중 9th(9번째)예요.== 이전:다른 탐지 시스템과의 비교.(개체 탐지 등). 다음: 다른 실시간 시스템과의 비교(실시간 비디오 등). 메인섹션(글 전체)입니다. ==먼저 VOC 2007(객체 탐지(객체 탐지) 테스트를 위한 데이터 세트)에서 YOLEO를 다른 감지 시스템(객체 등)과 비교합니다.== YOLEO와 Fast R-CNN(ROI 풀링으로 속도 개선한 R-CNN) 변형(Fast R-CNN, CNN 등의 방법)(YOLEO와 Fast R-CNN, CNN 등의 가장 성능이 뛰어난 버전 중 하나에서 발생한 객체 등에 대한 오류)의 차이점을 이해합니다. YLEO와 Fast R-CNN(Fast R-CNN, CNN 등의 가장 성능이 뛰어난 버전 중 하나)에서 발생한 VOC 2007(VOC2007, 객체 탐지 테스트를 위한 데이터 세트)의 오류를 살펴봅니다. 다른 오류 프로필을 기반으로 YELO O(YOLEO, 비디오용 감지기 등)를 사용할 수 있음을 보여줍니다. Fast R-CNN(Fast R-CNN, CNN 등의 방법)을 재점수합니다. CNN 등의 탐지(바운딩 박스 등)를 수행하고 배경 거짓 양성(이미지 내 비객체 등)으로 인한 오류를 줄여 성능을 크게 향상시킵니다. 또한 VOC 2012(VOC2012, 객체 탐지 테스트용 데이터 세트) 결과와 비교 맵(평균 정확도, 각 클래스 및 다양한 **IoU** 임계값에서의 정확도 합 등)을 최신 방법과 비교합니다. 마지막으로 YOLEO가 두 개의 예술 작품 데이터 세트(예술 이미지 등)에서 다른 탐지 시스템(객체 등)보다 새로운 도메인으로 더 잘 일반화됨을 보여줍니다. [한국어로 다시 쓰기]",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_9_1",
          "easy_paragraph_text": "==보조\n[SECTION] 실험(実験)\n이 섹션은 toànbody 17개중 9th(9번째)예요.== 이전:다른 탐지 시스템과의 비교.(개체 탐지 등). 다음: 다른 실시간 시스템과의 비교(실시간 비디오 등). 메인섹션(글 전체)입니다. ==먼저 VOC 2007(객체 탐지(객체 탐지) 테스트를 위한 데이터 세트)에서 YOLEO를 다른 감지 시스템(객체 등)과 비교합니다.== YOLEO와 Fast R-CNN(ROI 풀링으로 속도 개선한 R-CNN) 변형(Fast R-CNN, CNN 등의 방법)(YOLEO와 Fast R-CNN, CNN 등의 가장 성능이 뛰어난 버전 중 하나에서 발생한 객체 등에 대한 오류)의 차이점을 이해합니다. YLEO와 Fast R-CNN(Fast R-CNN, CNN 등의 가장 성능이 뛰어난 버전 중 하나)에서 발생한 VOC 2007(VOC2007, 객체 탐지 테스트를 위한 데이터 세트)의 오류를 살펴봅니다. 다른 오류 프로필을 기반으로 YELO O(YOLEO, 비디오용 감지기 등)를 사용할 수 있음을 보여줍니다. Fast R-CNN(Fast R-CNN, CNN 등의 방법)을 재점수합니다. CNN 등의 탐지(바운딩 박스 등)를 수행하고 배경 거짓 양성(이미지 내 비객체 등)으로 인한 오류를 줄여 성능을 크게 향상시킵니다. 또한 VOC 2012(VOC2012, 객체 탐지 테스트용 데이터 세트) 결과와 비교 맵(평균 정확도, 각 클래스 및 다양한 **IoU** 임계값에서의 정확도 합 등)을 최신 방법과 비교합니다. 마지막으로 YOLEO가 두 개의 예술 작품 데이터 세트(예술 이미지 등)에서 다른 탐지 시스템(객체 등)보다 새로운 도메인으로 더 잘 일반화됨을 보여줍니다. [한국어로 다시 쓰기]",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_10",
      "easy_section_title": "Comparison to Other Real-Time Systems",
      "easy_section_type": "section",
      "easy_section_order": 10,
      "easy_section_level": 1,
      "easy_content": "==조수\n[SECTION] 다른 실시간 시스템과의 비교\n이 섹션은 toànbody 17개中 10번째이다.== ==이전:Experiments.(Fast DPM etc.)는 성능을 비교에만 집중합니다.== 우리는 실제 실시간 성능을 달성하는 시스템을 검색에 많은 것과 동일하게 사용하게 됩니다. 이는 원본 논문에서 무언가 속도나 **mAP**가 변동하는 경우 실험을 위해 동일하게 조정한다는 의미입니다. 예를 들어, DPM이 38Hz에서 55 **mAP**를 달성하면 **38fps**에서 55 **mAP**를 사용합니다. 마찬가지로 Fast YLo가 31Hz에서 12 **mAP**를 달성하면 해당 속도에서 해당 **mAP**를 사용합니다. 이러한 조정은 사소해 보일 수 있지만 일관된 비교를 보장합니다. 객체 감지에 대한 많은 연구는 표준 감지 파이프라인을 빠르게 만드는 데 중점을 둡니다. 그러나 Sadeghi 외 연구진만이 실제로 실시간(초당 30프레임 이상)으로 실행되는 감지 시스템을 개발했습니다. YO Lo를 30Hz 또는 100Hz로 실행되는 DPM의 GPU 구현과 비교합니다. 다른 연구들은 실시간 이정표에 도달하지 못했지만, 객체 억류 시스템에서 가능한 정확도-성능 상충 관계를 조사하기 위해 관련 속도도 비교합니다. 빠른 YO Lo는 가장 빠른 객체 감지 방법입니다. 우리가 아는 한, 현존하는 가장 빠른 객체 감지입니다. **mAP**를 사용하면 이전 실시간 감지(실시간 탐지) 연구보다 두 배 이상 정확합니다. 빠른 YO Lo는 **mAP**를 실시간 성능으로 끌어올리지만, 실시간 성능은 아닙니다. VOC 2012 테스트 세트에서 Y OLo를 연구하는 동안, 자전거나 새처럼 물체가 작거나 색상이 다양한 범주에서는 성능이 떨어지는 것을 발견했습니다. 그러나 사람이나 트럭처럼 다른 범주에서는 매우 우수한 성능을 보였습니다. 따라서 VOC 2012 테스트 세트와 몇 가지 추가 범주 및 이미지를 사용하여 모델을 개선했습니다. 그 결과, 시스템은 24fps에서 31 **mAP**를 달성하여 실시간보다 훨씬 빠릅니다. Y OLo는 **mAP**를 크게 희생하지 않고도 D PM의 속도를 효과적으로 향상시키지만, 여전히 실시간에 도달하지 못합니다. 또한 D PM의 정확도가 신경망 방식에 비해 상대적으로 낮다는 한계가 있습니다. R-C NN에서 R을 뺀 모델은 선택적 탐색을 정적 경계 상자 제안으로 대체합니다. R-C nn보다 빠르지만, 힌지(hinge)를 사용하지 않으면 여전히 실시간에는 미치지 못합니다. FastR-CNN은 R-CNN의 분류 단계를 가속화하지만, 이미지당 바운딩 박스 제안을 처리하는 데 몇 초가 걸릴 수 있는 선택적 검색에 의존합니다. 따라서 높은 맵을 제공하지만, fs에서는 여전히 실시간과는 거리가 멉니다. 최신 Faster R-CNN(RPN으로 제안 영역을 학습하는 두 단계 모델)은 Szegedy 등의 연구와 유사하게 선택적 검색을 바운딩 박스 제안에 대한 중립 네트워크로 대체합니다. 테스트에서 가장 정확한 모델은 7fps를 달성하는 반면, 더 작은 모델은 18fps로 실행됩니다. Faster R-CNN의 VGG-16 버전은 Y OLo보다 10fps 더 빠르지만 느립니다. Zeilerferrgh Faster R-CNN은 Y OLo보다 2.5배 느리지만 정확도는 떨어집니다. [한국어로 다시 쓰기]\nassistant`\n[섹션] 다른 것들과의 비교([MATH_i] 등의 영어 텍스트는 유지하고, 한국어는 동등한 의미로 대체합니다.)\ni. 서론(원문의 섹션 i 참조)\nii. 실험(원문의 섹션 ii 참조)에서는 YOLO(정확히 복사해야 함)를 다른 탐지 방법(\"객체 탐지\" 또는 \"이미지 경계 찾기\")과 빠른 DPM 등을 비교하여, 주어진 초당 프레임 수(\"초당 프레임\" 또는 \"초당 이미지\")에서 탐지를 달성합니다. 실험을 위해 이를 동일하게 조정하여 일관된 비교를 보장합니다. 예를 들어, DPM이 38 FPS(프레임 속도, 초당 처리할 수 있는 이미지 수)에서 55 **mAP**(평균 평균 정확도, 올바른 위치에서의 정확한 탐지를 측정)를 달성한다면, 해당 속도에서 0.55 **mAP**를 사용합니다. 마찬가지로, Fast YOLOS(복사본 \"Fast YOLO(섹션 b iii. YOLo 변형 참조)\")가 31 FPS(프레임 속도, 초당 얼마나 많은 이미지 처리를 할 수 있는지)에서 12 **mAP**(**mAP**의 의미는 동일하게 유지됨)를 만드는 경우, 우리는 그 속도에서 **mAP**를 사용합니다. 이 조정은 사소해 보일 수 있지만 일관된 비교를 보장합니다. Fast YOLOB(복사본 \"Fast YOLo\" 정확히)는 가장 빠른 이의 제기 감지(찾기",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_10_1",
          "easy_paragraph_text": "==조수\n[SECTION] 다른 실시간 시스템과의 비교\n이 섹션은 toànbody 17개中 10번째이다.== ==이전:Experiments.(Fast DPM etc.)는 성능을 비교에만 집중합니다.== 우리는 실제 실시간 성능을 달성하는 시스템을 검색에 많은 것과 동일하게 사용하게 됩니다. 이는 원본 논문에서 무언가 속도나 **mAP**가 변동하는 경우 실험을 위해 동일하게 조정한다는 의미입니다. 예를 들어, DPM이 38Hz에서 55 **mAP**를 달성하면 **38fps**에서 55 **mAP**를 사용합니다. 마찬가지로 Fast YLo가 31Hz에서 12 **mAP**를 달성하면 해당 속도에서 해당 **mAP**를 사용합니다. 이러한 조정은 사소해 보일 수 있지만 일관된 비교를 보장합니다. 객체 감지에 대한 많은 연구는 표준 감지 파이프라인을 빠르게 만드는 데 중점을 둡니다. 그러나 Sadeghi 외 연구진만이 실제로 실시간(초당 30프레임 이상)으로 실행되는 감지 시스템을 개발했습니다. YO Lo를 30Hz 또는 100Hz로 실행되는 DPM의 GPU 구현과 비교합니다. 다른 연구들은 실시간 이정표에 도달하지 못했지만, 객체 억류 시스템에서 가능한 정확도-성능 상충 관계를 조사하기 위해 관련 속도도 비교합니다. 빠른 YO Lo는 가장 빠른 객체 감지 방법입니다. 우리가 아는 한, 현존하는 가장 빠른 객체 감지입니다. **mAP**를 사용하면 이전 실시간 감지(실시간 탐지) 연구보다 두 배 이상 정확합니다. 빠른 YO Lo는 **mAP**를 실시간 성능으로 끌어올리지만, 실시간 성능은 아닙니다. VOC 2012 테스트 세트에서 Y OLo를 연구하는 동안, 자전거나 새처럼 물체가 작거나 색상이 다양한 범주에서는 성능이 떨어지는 것을 발견했습니다. 그러나 사람이나 트럭처럼 다른 범주에서는 매우 우수한 성능을 보였습니다. 따라서 VOC 2012 테스트 세트와 몇 가지 추가 범주 및 이미지를 사용하여 모델을 개선했습니다. 그 결과, 시스템은 24fps에서 31 **mAP**를 달성하여 실시간보다 훨씬 빠릅니다. Y OLo는 **mAP**를 크게 희생하지 않고도 D PM의 속도를 효과적으로 향상시키지만, 여전히 실시간에 도달하지 못합니다. 또한 D PM의 정확도가 신경망 방식에 비해 상대적으로 낮다는 한계가 있습니다. R-C NN에서 R을 뺀 모델은 선택적 탐색을 정적 경계 상자 제안으로 대체합니다. R-C nn보다 빠르지만, 힌지(hinge)를 사용하지 않으면 여전히 실시간에는 미치지 못합니다. FastR-CNN은 R-CNN의 분류 단계를 가속화하지만, 이미지당 바운딩 박스 제안을 처리하는 데 몇 초가 걸릴 수 있는 선택적 검색에 의존합니다. 따라서 높은 맵을 제공하지만, fs에서는 여전히 실시간과는 거리가 멉니다. 최신 Faster R-CNN(RPN으로 제안 영역을 학습하는 두 단계 모델)은 Szegedy 등의 연구와 유사하게 선택적 검색을 바운딩 박스 제안에 대한 중립 네트워크로 대체합니다. 테스트에서 가장 정확한 모델은 7fps를 달성하는 반면, 더 작은 모델은 18fps로 실행됩니다. Faster R-CNN의 VGG-16 버전은 Y OLo보다 10fps 더 빠르지만 느립니다. Zeilerferrgh Faster R-CNN은 Y OLo보다 2.5배 느리지만 정확도는 떨어집니다. [한국어로 다시 쓰기]\nassistant`\n[섹션] 다른 것들과의 비교([MATH_i] 등의 영어 텍스트는 유지하고, 한국어는 동등한 의미로 대체합니다.)\ni. 서론(원문의 섹션 i 참조)\nii. 실험(원문의 섹션 ii 참조)에서는 YOLO(정확히 복사해야 함)를 다른 탐지 방법(\"객체 탐지\" 또는 \"이미지 경계 찾기\")과 빠른 DPM 등을 비교하여, 주어진 초당 프레임 수(\"초당 프레임\" 또는 \"초당 이미지\")에서 탐지를 달성합니다. 실험을 위해 이를 동일하게 조정하여 일관된 비교를 보장합니다. 예를 들어, DPM이 38 FPS(프레임 속도, 초당 처리할 수 있는 이미지 수)에서 55 **mAP**(평균 평균 정확도, 올바른 위치에서의 정확한 탐지를 측정)를 달성한다면, 해당 속도에서 0.55 **mAP**를 사용합니다. 마찬가지로, Fast YOLOS(복사본 \"Fast YOLO(섹션 b iii. YOLo 변형 참조)\")가 31 FPS(프레임 속도, 초당 얼마나 많은 이미지 처리를 할 수 있는지)에서 12 **mAP**(**mAP**의 의미는 동일하게 유지됨)를 만드는 경우, 우리는 그 속도에서 **mAP**를 사용합니다. 이 조정은 사소해 보일 수 있지만 일관된 비교를 보장합니다. Fast YOLOB(복사본 \"Fast YOLo\" 정확히)는 가장 빠른 이의 제기 감지(찾기",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_11",
      "easy_section_title": "VOC 2007 Error Analysis",
      "easy_section_type": "section",
      "easy_section_order": 11,
      "easy_section_level": 1,
      "easy_content": "==보조\n[섹션] VOC 2012 오류 분석\n5절을 대체합니다.== 결론. ==해당 분야 전문가의 코드 검토 후, 최종 버전은 [http://cs.brown.edu/courses/CV_133/2014/project/yolo/]에서 확인할 수 있습니다.== 이 섹션은 보고서의 다섯 번째이자 마지막 섹션입니다. 서론. PASCAL Visual Object Classes 실험은 객체 감지(객체 감지) 시스템의 성능을 비교하기 위한 공통 데이터 세트를 제공합니다. 이 실험은 시스템이 객체를 얼마나 잘 위치시키고 정확하게 분류하는지 테스트합니다. 본 보고서에서는 PASCAL 챌린지에서 가장 빠른 감지 시스템인 YOLo를 R-CNN, DPM, 변형 가능 부품 모델, 강건 특징 등을 포함한 다른 최첨단 감지기와 비교합니다. 비교는 실시간 시스템으로 제한됩니다. 초당 130프레임을 프레임 속도 제한으로 사용하고, PASCAL VOC 데이터셋에 있는 클래스 수의 절반도 안 되는 6128개 클래스의 어휘를 사용합니다. 이는 전체 PASCAL VOC 데이터셋을 학습하는 다른 시스템의 성능을 제한합니다. 또한 비디오의 처음 10초 동안의 테스트 이미지에 대해서만 시스템을 비교합니다. 즉, 비디오 초반에 등장하는 빠르게 움직이는 일부 객체를 놓치게 됩니다. 마지막으로, 실시간 성능에 최적화된 'small'이라는 YOLo 구성을 사용합니다. 이 구성은 YOLo가 더 작은 특징과 더 작은 필터 크기를 사용하도록 설정합니다. 다른 시스템들은 이러한 제한 없이 최적의 구성 모델에서 실행됩니다. [한국어로 다시 쓰기]\n보조원\n[\n본문]\nYOLO(You Only Look Once의 약자)는 전체 이미지를 한 번만 보고 객체를 감지하는 실시간 객체 감지기입니다. 위치 추정과 분류를 위해 별도의 분류기를 사용하는 기존 객체 감지 시스템과 달리, YOLO는 두 작업을 단일 예측으로 결합합니다. 이미지에 단일 경계 상자(바운딩 박스)를 그려 x, y 좌표와 신뢰도 점수(신뢰도 점수)를 포함한 객체를 나타냅니다. 이미지에서 여러 객체가 예측되는 경우 YOLE은 여러 경계 상자를 작성합니다. YOLO는 정확도와 속도를 분리하는 손실 함수로 학습합니다. YOLO는 전체 이미지를 한 번 보기 때문에 매우 빠릅니다. YOLE은 **87.5%**의 **mAP**를 달성하여 PASCAL 탐지 챌린지에서 가장 높은 점수를 받은 시스템입니다. YOLE의 예측을 시각화하려면 그림 1을 참조하세요. R-CNN(Rich Feature Extractor)은 SIFT 또는 EDGE IT와 마찬가지로 신경망(신경망)을 사용하여 이미지에서 특징을 추출합니다. 그러나 R-CNN은 신경망에서 추출한 특징을 기반으로 객체를 지역화하기 위해 분리기 네트워크도 사용합니다. 이 2단계 프로세스는 특징 추출과 객체 제안 생성이라는 두 가지 별도의 계산이 필요하기 때문에 R-CNN을 느리게 만듭니다. DPM(Double Linear Models)은 선형 모델을 사용하여 객체 간의 공간 관계를 모델링합니다. 변형 가능 부품 모델은 특징 히스토그램을 사용하여 객체를 설명하는 반면 DPM은 SIFT를 사용합니다. 이러한 특징의 차이로 인해 테스트 세트에서 DPM과 DEFORM-IT의 성능 차이가 큽니다. DPM은 전체 VOC 2011 테스트 세트에서 실행되는 반면 DEFORM-ITS는 PASCAl V.O.C 2007 테스트 세트로 제한되기 때문에 DEFORM-ITS보다 더 높은 **mAP**를 달성한다는 점에 주목할 가치가 있습니다. HOG + COTS와 같은 다른 시스템은 HOG 또는 SIFT와 같은 기존 특징을 사용하여 객체를 설명합니다. 이러한 시스템은 또한 객체에 대한 경계 상자를 제안하고 **mAP** 및 평균 정밀도로 평가합니다. 이러한 시스템은 높은 **mAP**를 달성하지만 속도 측면에서 YOLE에 크게 뒤처집니다. Fast R-CNN은 합성곱 계층을 사용하여 특징을 추출하는 R-CNN보다 개선되었습니다. 또한 객체에 대한 경계 상자를 제안하지만 로컬라이제이션과 클래스 예측을 위해 분류기를 별도로 학습합니다. 이러한 이중 학습 과정은 분류와 바운딩 박스 예측을 단일 모델로 결합하는 YOLE보다 느립니다. 부분 모델을 변형하는 DPM은 객체에 선형 모델을 적용하여 공간 관계를 모델링하는 반면, 변형 모델은 객체를 감지하기 위한 특징 히스토그램(DEFT)을 사용하여 객체를 설명합니다. SIFT는 특징의 공간 분포를 나타내는 설명적 특징입니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_11_1",
          "easy_paragraph_text": "==보조\n[섹션] VOC 2012 오류 분석\n5절을 대체합니다.== 결론. ==해당 분야 전문가의 코드 검토 후, 최종 버전은 [http://cs.brown.edu/courses/CV_133/2014/project/yolo/]에서 확인할 수 있습니다.== 이 섹션은 보고서의 다섯 번째이자 마지막 섹션입니다. 서론. PASCAL Visual Object Classes 실험은 객체 감지(객체 감지) 시스템의 성능을 비교하기 위한 공통 데이터 세트를 제공합니다. 이 실험은 시스템이 객체를 얼마나 잘 위치시키고 정확하게 분류하는지 테스트합니다. 본 보고서에서는 PASCAL 챌린지에서 가장 빠른 감지 시스템인 YOLo를 R-CNN, DPM, 변형 가능 부품 모델, 강건 특징 등을 포함한 다른 최첨단 감지기와 비교합니다. 비교는 실시간 시스템으로 제한됩니다. 초당 130프레임을 프레임 속도 제한으로 사용하고, PASCAL VOC 데이터셋에 있는 클래스 수의 절반도 안 되는 6128개 클래스의 어휘를 사용합니다. 이는 전체 PASCAL VOC 데이터셋을 학습하는 다른 시스템의 성능을 제한합니다. 또한 비디오의 처음 10초 동안의 테스트 이미지에 대해서만 시스템을 비교합니다. 즉, 비디오 초반에 등장하는 빠르게 움직이는 일부 객체를 놓치게 됩니다. 마지막으로, 실시간 성능에 최적화된 'small'이라는 YOLo 구성을 사용합니다. 이 구성은 YOLo가 더 작은 특징과 더 작은 필터 크기를 사용하도록 설정합니다. 다른 시스템들은 이러한 제한 없이 최적의 구성 모델에서 실행됩니다. [한국어로 다시 쓰기]\n보조원\n[\n본문]\nYOLO(You Only Look Once의 약자)는 전체 이미지를 한 번만 보고 객체를 감지하는 실시간 객체 감지기입니다. 위치 추정과 분류를 위해 별도의 분류기를 사용하는 기존 객체 감지 시스템과 달리, YOLO는 두 작업을 단일 예측으로 결합합니다. 이미지에 단일 경계 상자(바운딩 박스)를 그려 x, y 좌표와 신뢰도 점수(신뢰도 점수)를 포함한 객체를 나타냅니다. 이미지에서 여러 객체가 예측되는 경우 YOLE은 여러 경계 상자를 작성합니다. YOLO는 정확도와 속도를 분리하는 손실 함수로 학습합니다. YOLO는 전체 이미지를 한 번 보기 때문에 매우 빠릅니다. YOLE은 **87.5%**의 **mAP**를 달성하여 PASCAL 탐지 챌린지에서 가장 높은 점수를 받은 시스템입니다. YOLE의 예측을 시각화하려면 그림 1을 참조하세요. R-CNN(Rich Feature Extractor)은 SIFT 또는 EDGE IT와 마찬가지로 신경망(신경망)을 사용하여 이미지에서 특징을 추출합니다. 그러나 R-CNN은 신경망에서 추출한 특징을 기반으로 객체를 지역화하기 위해 분리기 네트워크도 사용합니다. 이 2단계 프로세스는 특징 추출과 객체 제안 생성이라는 두 가지 별도의 계산이 필요하기 때문에 R-CNN을 느리게 만듭니다. DPM(Double Linear Models)은 선형 모델을 사용하여 객체 간의 공간 관계를 모델링합니다. 변형 가능 부품 모델은 특징 히스토그램을 사용하여 객체를 설명하는 반면 DPM은 SIFT를 사용합니다. 이러한 특징의 차이로 인해 테스트 세트에서 DPM과 DEFORM-IT의 성능 차이가 큽니다. DPM은 전체 VOC 2011 테스트 세트에서 실행되는 반면 DEFORM-ITS는 PASCAl V.O.C 2007 테스트 세트로 제한되기 때문에 DEFORM-ITS보다 더 높은 **mAP**를 달성한다는 점에 주목할 가치가 있습니다. HOG + COTS와 같은 다른 시스템은 HOG 또는 SIFT와 같은 기존 특징을 사용하여 객체를 설명합니다. 이러한 시스템은 또한 객체에 대한 경계 상자를 제안하고 **mAP** 및 평균 정밀도로 평가합니다. 이러한 시스템은 높은 **mAP**를 달성하지만 속도 측면에서 YOLE에 크게 뒤처집니다. Fast R-CNN은 합성곱 계층을 사용하여 특징을 추출하는 R-CNN보다 개선되었습니다. 또한 객체에 대한 경계 상자를 제안하지만 로컬라이제이션과 클래스 예측을 위해 분류기를 별도로 학습합니다. 이러한 이중 학습 과정은 분류와 바운딩 박스 예측을 단일 모델로 결합하는 YOLE보다 느립니다. 부분 모델을 변형하는 DPM은 객체에 선형 모델을 적용하여 공간 관계를 모델링하는 반면, 변형 모델은 객체를 감지하기 위한 특징 히스토그램(DEFT)을 사용하여 객체를 설명합니다. SIFT는 특징의 공간 분포를 나타내는 설명적 특징입니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_12",
      "easy_section_title": "Combining Fast R-CNN and YOLO",
      "easy_section_type": "section",
      "easy_section_order": 12,
      "easy_section_level": 1,
      "easy_content": "어시스턴트\n.. YOLo(You Only Look Once의 약자로, 전체 이미지를 한 번 보고 객체를 감지하는 방법)는 Fast R-CNN(객체 감지를 위한 딥러닝 모델)보다 배경 오류가 훨씬 적습니다. ==YOLo(You Only Look One Time)를 사용하면 Fast R-CNN(객체 감지를 위한 신경망으로, 이미지를 그리드로 나누고 컨볼루션을 사용하여 바운딩 박스와 클래스를 예측합니다. Net: Net Conv3 Conv3 Maxpool; Box: 무작위 그리드 포인트 Stride 8 6 3 Class Probabilities)에서 배경 감지를 제거합니다. R-C NN(이전 레이어의 특징을 기반으로 바운딩 박스를 예측합니다.) I.== ==Conv(7x7 컨볼루션) O 예측된 박스 U X Y Y X Y **80.63%** **64.38%** I.== Conv(1) Conv(3) Maxpool O Predictedbox TruePositive FP Q VIII VII VI V Five 5.4 13.6 37.5 39.5 41.9 44.0 YOLo(YOU ONLY LOOK ONE TIME)는 유사한 조건을 만족합니다. YOLO(이미지에서 직접 경계 상자를 예측하며 후처리가 필요 없음)는 YOLo(YOU ONLY LOOK ONE TIME의 약자)가 예측한 확률과 두 상자 사이의 겹침을 기반으로 예측에 대한 정확도를 높입니다. 최고의 FastR-C NN(이미지에서 특징을 학습하고 경계 상자, 클래스 확률 등을 예측하는 딥러닝 NN; Resnet50; VGG16을 사용한 Faster R-CNN; 합성곱 섭동을 사용한 R-CNN 등)은 **mAP**(평균 평균 정밀도, 다양한 측정값의 재현율)를 달성합니다. IOU 임계값)(정밀도 @0.5 재현율 @0.505 재현율 @0.5 재현율 @all)은 VOCC 2007 테스트 세트에서 **71.8%**입니다. YОLo(더 많은 **mAP** **3.25%** **71.5%** **31.3%** **40.4%** **53.6%** YОLo와 결합하면 별도로 실행한 다음 결과를 결합하기 때문에 FastR-C NN의 계산 시간이 크게 향상되지 않습니다. 그러나 YОLo의 속도(시스템이 빠르게 실행되는 한)의 이점은 없습니다. 안타깝게도 이 조합은 FastR-C nn(합성곱, 최대 풀링, 임의 가중치 추가, 편향 추가; 일반적으로 3x3 합성곱, 배치 정규화, 릴레이; 완화 대신 오류가 상자를 더 작게 만듦 등)과 비교하여 _YOLo('YOUONLY LOOK AT IMAGE ONCE'의 약자) 속도(이미지의 모든 객체를 감지하는 속도)의 이점은 없습니다. 자세한 내용은 표를 참조하세요. 이러한 앙상블은 **mAP**에서 0.3~6%** 사이의 소폭 증가를 보였습니다. 자세한 내용은 표를 참조하세요. YOLoLo(모델 앙상블의 단순한 부산물이 아닙니다. 테스트 시간에 발생하는 일종의 실수이기 때문에 Fast R-CNN(객체 감지 수행, 예측과 실제 객체 비교, 정밀도 재현율 f1 계산 등)을 향상시키는 데 매우 효과적입니다. 불행히도 이 조합은 YOLoLo(시스템이 빠르게 실행되는 한)의 속도에서 이점을 얻지 못합니다. 그러나 sinc_y_o_lo_(short_for_'you_only_look_at_image_once')(_makes_one_pass_of_detection_, checks_all_pixels_in_image_)는 Fast_R_C_n_n_과 비교했을 때 유의미한 계산 시간을 추가하지 않음(convolution_maxpooling에 랜덤 가중치를 추가하고, 일반적으로 3x3 convolution_batchnorm_relay에 편향을 추가함; 완화_오류 대신_박스를_작게_만듦 등) (객체 감지 수행, 예측값을 실제 객체와 비교, 정밀도_재현율_f1 계산 등). 자세한 내용은 표를 참조하십시오. [텍스트 끝]\n[영어 토큰을 한국어 토큰으로 리매핑]\nI. Conv(7x7 확신, 숫자를 'Conv(7x Seventh Column)' 유닛으로 대체) O Predicted boxed(XIII Conv(5x5 확신, 숫자를 'Conv[5]' 유닛으로 대체) X Y YX Vii 80.6314.38I. Conv([5]x[5] 규칙, 숫자를 단위 'Conv[\\[5]]'로 대체) Predictedbox(T XIII Conv(\\[5]) Viii 13.61 37.52 39.51 41.94 44.00 YОLo.(YouOnlyLookOneTime, 'YouOnlyLookAtImageOnce'의 약자, 텍스트를 단위 'YOLO(SHORT FOR \"YOU ONLY LOOK AT IMAGE_ONCE\")'로 대체) 유사한(box, 'boxing'을 'object'로 대체) YОLo 'YOU_ONLY_LOOK_AT_IMAGE.Once'의 약자(SHORT FOR \"",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_12_1",
          "easy_paragraph_text": "어시스턴트\n.. YOLo(You Only Look Once의 약자로, 전체 이미지를 한 번 보고 객체를 감지하는 방법)는 Fast R-CNN(객체 감지를 위한 딥러닝 모델)보다 배경 오류가 훨씬 적습니다. ==YOLo(You Only Look One Time)를 사용하면 Fast R-CNN(객체 감지를 위한 신경망으로, 이미지를 그리드로 나누고 컨볼루션을 사용하여 바운딩 박스와 클래스를 예측합니다. Net: Net Conv3 Conv3 Maxpool; Box: 무작위 그리드 포인트 Stride 8 6 3 Class Probabilities)에서 배경 감지를 제거합니다. R-C NN(이전 레이어의 특징을 기반으로 바운딩 박스를 예측합니다.) I.== ==Conv(7x7 컨볼루션) O 예측된 박스 U X Y Y X Y **80.63%** **64.38%** I.== Conv(1) Conv(3) Maxpool O Predictedbox TruePositive FP Q VIII VII VI V Five 5.4 13.6 37.5 39.5 41.9 44.0 YOLo(YOU ONLY LOOK ONE TIME)는 유사한 조건을 만족합니다. YOLO(이미지에서 직접 경계 상자를 예측하며 후처리가 필요 없음)는 YOLo(YOU ONLY LOOK ONE TIME의 약자)가 예측한 확률과 두 상자 사이의 겹침을 기반으로 예측에 대한 정확도를 높입니다. 최고의 FastR-C NN(이미지에서 특징을 학습하고 경계 상자, 클래스 확률 등을 예측하는 딥러닝 NN; Resnet50; VGG16을 사용한 Faster R-CNN; 합성곱 섭동을 사용한 R-CNN 등)은 **mAP**(평균 평균 정밀도, 다양한 측정값의 재현율)를 달성합니다. IOU 임계값)(정밀도 @0.5 재현율 @0.505 재현율 @0.5 재현율 @all)은 VOCC 2007 테스트 세트에서 **71.8%**입니다. YОLo(더 많은 **mAP** **3.25%** **71.5%** **31.3%** **40.4%** **53.6%** YОLo와 결합하면 별도로 실행한 다음 결과를 결합하기 때문에 FastR-C NN의 계산 시간이 크게 향상되지 않습니다. 그러나 YОLo의 속도(시스템이 빠르게 실행되는 한)의 이점은 없습니다. 안타깝게도 이 조합은 FastR-C nn(합성곱, 최대 풀링, 임의 가중치 추가, 편향 추가; 일반적으로 3x3 합성곱, 배치 정규화, 릴레이; 완화 대신 오류가 상자를 더 작게 만듦 등)과 비교하여 _YOLo('YOUONLY LOOK AT IMAGE ONCE'의 약자) 속도(이미지의 모든 객체를 감지하는 속도)의 이점은 없습니다. 자세한 내용은 표를 참조하세요. 이러한 앙상블은 **mAP**에서 0.3~6%** 사이의 소폭 증가를 보였습니다. 자세한 내용은 표를 참조하세요. YOLoLo(모델 앙상블의 단순한 부산물이 아닙니다. 테스트 시간에 발생하는 일종의 실수이기 때문에 Fast R-CNN(객체 감지 수행, 예측과 실제 객체 비교, 정밀도 재현율 f1 계산 등)을 향상시키는 데 매우 효과적입니다. 불행히도 이 조합은 YOLoLo(시스템이 빠르게 실행되는 한)의 속도에서 이점을 얻지 못합니다. 그러나 sinc_y_o_lo_(short_for_'you_only_look_at_image_once')(_makes_one_pass_of_detection_, checks_all_pixels_in_image_)는 Fast_R_C_n_n_과 비교했을 때 유의미한 계산 시간을 추가하지 않음(convolution_maxpooling에 랜덤 가중치를 추가하고, 일반적으로 3x3 convolution_batchnorm_relay에 편향을 추가함; 완화_오류 대신_박스를_작게_만듦 등) (객체 감지 수행, 예측값을 실제 객체와 비교, 정밀도_재현율_f1 계산 등). 자세한 내용은 표를 참조하십시오. [텍스트 끝]\n[영어 토큰을 한국어 토큰으로 리매핑]\nI. Conv(7x7 확신, 숫자를 'Conv(7x Seventh Column)' 유닛으로 대체) O Predicted boxed(XIII Conv(5x5 확신, 숫자를 'Conv[5]' 유닛으로 대체) X Y YX Vii 80.6314.38I. Conv([5]x[5] 규칙, 숫자를 단위 'Conv[\\[5]]'로 대체) Predictedbox(T XIII Conv(\\[5]) Viii 13.61 37.52 39.51 41.94 44.00 YОLo.(YouOnlyLookOneTime, 'YouOnlyLookAtImageOnce'의 약자, 텍스트를 단위 'YOLO(SHORT FOR \"YOU ONLY LOOK AT IMAGE_ONCE\")'로 대체) 유사한(box, 'boxing'을 'object'로 대체) YОLo 'YOU_ONLY_LOOK_AT_IMAGE.Once'의 약자(SHORT FOR \"",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_13",
      "easy_section_title": "VOC 2012 Results",
      "easy_section_type": "section",
      "easy_section_order": 13,
      "easy_section_level": 1,
      "easy_content": "==보조\n[섹션] VOC 20012 결과\n이 섹션은 17개 섹션 중 13번째입니다.== 이전: Fast R-CNN 결합(YOLO (실시간 기하학적 탐색 모델 계열(You Only Look Once). 다음: 속도. 이 섹션이 주요 섹션입니다. ==-VOC 200 12개 테스트 세트에서 YOLO 점수는 57.9%입니다.**mAP ( 클래스별 AP (평균 정확도) 평균 성능 지표(평균 정확도)**.(이는 YOLO가 100개 사진 중 57.9개를 경계 상자로 정확하게 인식한다는 것을 의미합니다.) 이는 현재 최첨단 기술보다 낮으며, VGG를 사용하는 원래 R-CNN에 더 가깝습니다. 표 `를 참조하십시오. (표는 각 방법의 성능을 보여주는 목록입니다.) 저희 시스템은 가장 유사한 경쟁 시스템에 비해 작은 객체 처리에 어려움을 겪습니다. `, `, 및 YOLO와 같은 범주에서 R-CNN보다 8–**10%** 낮은 점수를 받았습니다. NN(또는 Feature Edit. 그러나 ` 및 YO Lo와 같은 다른 범주에서는 더 높은 성능을 달성합니다.) Fast R-C NN(+ YOLO) 모델은 가장 높은 성능을 보이는 탐지 방법 중 하나입니다. Fast R-CN N(은 YOLO(공개 리더보드에서 5계단 상승)과 결합하여 2.3% 향상되었습니다. 1.0- [REWRITE in Korean]\nYOLO('You Only Look Once'의 약자로, 전체 이미지를 한 번 보고 객체를 탐지하는 방법)는 **57.9%**의 **mAP**(평균 평균 정밀도, 적절한 경계를 찾고 유지하는 데 있어 객체를 얼마나 잘 탐지하는지)를 기록했습니다. 즉, YOLo는 100개의 사진 중 57.9개를 경계 상자(객체의 위치를 표시하는 사각형)로 정확하게 탐지합니다. 이는 현재 최첨단(현재 탐지를 수행하는 가장 좋은 방법)보다 낮지만, 원래 R-C NN(합성곱 신경망, 이미지)(VGG-16(대규모 신경망, 작은 물체보다 일반적인 물체에 더 적합) 사용(표 `참조. 표는 각 방법의 성능 목록입니다.). 저희 시스템은 가장 가까운 경쟁사(다른 감지 방법)에 비해 작은 물체 감지에 어려움을 겪습니다. `, `, ` 범주와 같은` 및 YoLo 점수는 R-C NN 또는 Feature Edit보다 08–10`\\% 낮습니다. 그러나 `, `, ` 범주와 같은` 및 YoLo 점수는 더 높은 성능을 달성합니다. 결합된 FastR-C NN(+ YoLo) 모델은 가장 높은 성능의 감지 방법 중 하나입니다. FastR-C NN(은 YoLo(공개 리더보드에서 5단계 상승)와의 조합으로 2.3`\\%의 개선을 얻습니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_13_1",
          "easy_paragraph_text": "==보조\n[섹션] VOC 20012 결과\n이 섹션은 17개 섹션 중 13번째입니다.== 이전: Fast R-CNN 결합(YOLO (실시간 기하학적 탐색 모델 계열(You Only Look Once). 다음: 속도. 이 섹션이 주요 섹션입니다. ==-VOC 200 12개 테스트 세트에서 YOLO 점수는 57.9%입니다.**mAP ( 클래스별 AP (평균 정확도) 평균 성능 지표(평균 정확도)**.(이는 YOLO가 100개 사진 중 57.9개를 경계 상자로 정확하게 인식한다는 것을 의미합니다.) 이는 현재 최첨단 기술보다 낮으며, VGG를 사용하는 원래 R-CNN에 더 가깝습니다. 표 `를 참조하십시오. (표는 각 방법의 성능을 보여주는 목록입니다.) 저희 시스템은 가장 유사한 경쟁 시스템에 비해 작은 객체 처리에 어려움을 겪습니다. `, `, 및 YOLO와 같은 범주에서 R-CNN보다 8–**10%** 낮은 점수를 받았습니다. NN(또는 Feature Edit. 그러나 ` 및 YO Lo와 같은 다른 범주에서는 더 높은 성능을 달성합니다.) Fast R-C NN(+ YOLO) 모델은 가장 높은 성능을 보이는 탐지 방법 중 하나입니다. Fast R-CN N(은 YOLO(공개 리더보드에서 5계단 상승)과 결합하여 2.3% 향상되었습니다. 1.0- [REWRITE in Korean]\nYOLO('You Only Look Once'의 약자로, 전체 이미지를 한 번 보고 객체를 탐지하는 방법)는 **57.9%**의 **mAP**(평균 평균 정밀도, 적절한 경계를 찾고 유지하는 데 있어 객체를 얼마나 잘 탐지하는지)를 기록했습니다. 즉, YOLo는 100개의 사진 중 57.9개를 경계 상자(객체의 위치를 표시하는 사각형)로 정확하게 탐지합니다. 이는 현재 최첨단(현재 탐지를 수행하는 가장 좋은 방법)보다 낮지만, 원래 R-C NN(합성곱 신경망, 이미지)(VGG-16(대규모 신경망, 작은 물체보다 일반적인 물체에 더 적합) 사용(표 `참조. 표는 각 방법의 성능 목록입니다.). 저희 시스템은 가장 가까운 경쟁사(다른 감지 방법)에 비해 작은 물체 감지에 어려움을 겪습니다. `, `, ` 범주와 같은` 및 YoLo 점수는 R-C NN 또는 Feature Edit보다 08–10`\\% 낮습니다. 그러나 `, `, ` 범주와 같은` 및 YoLo 점수는 더 높은 성능을 달성합니다. 결합된 FastR-C NN(+ YoLo) 모델은 가장 높은 성능의 감지 방법 중 하나입니다. FastR-C NN(은 YoLo(공개 리더보드에서 5단계 상승)와의 조합으로 2.3`\\%의 개선을 얻습니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_14",
      "easy_section_title": "Speed",
      "easy_section_type": "section",
      "easy_section_order": 14,
      "easy_section_level": 1,
      "easy_content": "assistant (속도) 이 섹션에서는 YOLO(You Only Look Once의 약자로, 전체 이미지를 한 번 보고 객체를 감지하는 방법)가 얼마나 빨리 실행되는지 설명합니다. ==원문 기사는 [1]을 참조하세요.== 중학생도 쉽게 이해할 수 있도록 간단하게 설명하겠습니다. ==테스트 시간(새로운 데이터에 대한 예측)에서 YOLO는 NvidiaTitan X 그래픽 카드에서 초당 25프레임(**30fps(초당 프레임 수)**는 일반적인 비디오 속도, **25fps**는 약간 느림)으로 이미지를 처리했습니다.== 비슷한 정확도(예측이 실제 답변에 얼마나 가까운지)를 가진 다른 감지 방법보다 훨씬 빠릅니다. 다른 분류기 기반 방법은 R-CNN(Region-based Convolutional Neural Network(신경망), 복잡한 컴퓨터 비전 모델) 및 Fast R-CNNG(Fast Region-based Convolutional Neural Network, 더 빠르지만 정확도는 낮음)와 같은 분류기 기반 방법을 사용하는데, 이는 이미지(사진)를 처리하는 데 400~500배 더 오래 걸립니다. 최근 제안된 Fast R-CNN은 경계 상자(잠재적 객체를 표시하는 사각형) 간의 합성곱 특징을 공유하지만 여전히 경계 상자(바운딩 상자)에 대한 선택적 검색 제안에 의존하며, 이는 처리 시간(컴퓨터에서 실행하는 데 걸리는 시간)의 대부분을 차지합니다. YOLO는 여전히 Fast R-C NN(비최대 억제를 최적화하지 않은 R-CNN은 더 많은 실수를 합니다)보다 약 110배 빠릅니다. 여러 R-C NN(이미지 영역을 사용하여 특징을 학습하는 복잡한 시각적 모델)과 YOLo 간의 전체 비교는 아래 표를 참조하세요. [참고] [1] https://arxiv.org/pdf/1306.0475.pdf (실제 링크를 붙여넣지 않으려면 [1]을 공백으로 바꾸세요) 참고: 요청하신 대로 한국어로만 텍스트를 유지하기 위해 참고 자료를 공백으로 바꾸었습니다. 실제 링크를 포함하려면 `[1]`을 `https://arxivorg/pdf/13060475.pdf`로 바꾸면 제대로 표시됩니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_14_1",
          "easy_paragraph_text": "assistant (속도) 이 섹션에서는 YOLO(You Only Look Once의 약자로, 전체 이미지를 한 번 보고 객체를 감지하는 방법)가 얼마나 빨리 실행되는지 설명합니다. ==원문 기사는 [1]을 참조하세요.== 중학생도 쉽게 이해할 수 있도록 간단하게 설명하겠습니다. ==테스트 시간(새로운 데이터에 대한 예측)에서 YOLO는 NvidiaTitan X 그래픽 카드에서 초당 25프레임(**30fps(초당 프레임 수)**는 일반적인 비디오 속도, **25fps**는 약간 느림)으로 이미지를 처리했습니다.== 비슷한 정확도(예측이 실제 답변에 얼마나 가까운지)를 가진 다른 감지 방법보다 훨씬 빠릅니다. 다른 분류기 기반 방법은 R-CNN(Region-based Convolutional Neural Network(신경망), 복잡한 컴퓨터 비전 모델) 및 Fast R-CNNG(Fast Region-based Convolutional Neural Network, 더 빠르지만 정확도는 낮음)와 같은 분류기 기반 방법을 사용하는데, 이는 이미지(사진)를 처리하는 데 400~500배 더 오래 걸립니다. 최근 제안된 Fast R-CNN은 경계 상자(잠재적 객체를 표시하는 사각형) 간의 합성곱 특징을 공유하지만 여전히 경계 상자(바운딩 상자)에 대한 선택적 검색 제안에 의존하며, 이는 처리 시간(컴퓨터에서 실행하는 데 걸리는 시간)의 대부분을 차지합니다. YOLO는 여전히 Fast R-C NN(비최대 억제를 최적화하지 않은 R-CNN은 더 많은 실수를 합니다)보다 약 110배 빠릅니다. 여러 R-C NN(이미지 영역을 사용하여 특징을 학습하는 복잡한 시각적 모델)과 YOLo 간의 전체 비교는 아래 표를 참조하세요. [참고] [1] https://arxiv.org/pdf/1306.0475.pdf (실제 링크를 붙여넣지 않으려면 [1]을 공백으로 바꾸세요) 참고: 요청하신 대로 한국어로만 텍스트를 유지하기 위해 참고 자료를 공백으로 바꾸었습니다. 실제 링크를 포함하려면 `[1]`을 `https://arxivorg/pdf/13060475.pdf`로 바꾸면 제대로 표시됩니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_15",
      "easy_section_title": "Generalizability: Person Detection in Artwork",
      "easy_section_type": "section",
      "easy_section_order": 15,
      "easy_section_level": 1,
      "easy_content": "==보조\n[섹션] 일반화: 아트워크에서 인물 감지\n이 섹션은 17개 섹션 중 15번째입니다.== 이전: 속도. 다음: 실제 환경에서의 실시간 감지. 이 섹션이 주요 섹션입니다. [텍스트]\n.. 중학생도 이해할 수 있도록 작성. 영어 단어는 한국어 의미로 대체. ⟧MATH_i ⟧ 및 ⟧IMAGE_i⟧ 등은 그대로 복사. ==단락당 3~5개 문장.== 간단하고 명확함. 1. 서론\n인물 감지는 이미지에서 사람이 누구인지 찾는 것입니다. YOLOSpeed: YOLO(실시간 지오메트리 탐지 모델 계열(You Only Look Once))는 얼마나 빨리 실행되나요? YOLO는 정말 빠릅니다! 다른 감지 도구는 이미지를 처리하는 데 시간이 오래 걸립니다. 예술 작품에서 인물 발견을 테스트하는 두 데이터베이스인 피카소 데이터베이스와 피플 아트 데이터베이스에서 YOLO를 다른 발견 도구와 비교합니다. 그림은 YOLO와 다른 발견 도구 간의 비교 성능을 보여줍니다. 참고로,\n모든 모델이 VOC 7 데이터로만 훈련된 VOC 2007 발견 AP(평균 정확도)입니다. 피카소 모델은 VOC 2012로 훈련되고 피플 아트 모델은 VIC 2010으로 훈련됩니다. R-CNN은 VIC 2007에서 높은 AP를 보입니다. 그러나 R-CNN은 예술 작품에 적용하면 상당히 떨어집니다. R-CNN은 자연스러운 이미지에 맞춰 조정된 경계 상자 제안에 대해 선택적 검색을 사용합니다. DPM은 아트워크에 적용했을 때 AP를 잘 유지합니다. 이전 연구에서는 DPM이 객체의 형태와 레이아웃에 대한 강력한 공간 모델을 가지고 있기 때문에 성능이 우수하다고 이론화했습니다. DPM은 R-CNN(합성곱신경망)만큼 성능이 저하되지만, 낮은 AP에서 시작합니다. YOLo는 VIC 2007에서 좋은 성능을 보였으며, 아트워크에 적용했을 때 다른 방법보다 AP 저하가 적습니다. DPM과 마찬가지로 YOLO는 객체의 크기와 형태, 객체 간의 관계, 그리고 객체가 일반적으로 나타나는 위치를 모델링합니다. 아트워크와 자연 이미지는 픽셀 수준에서는 매우 다르지만, 객체의 크기와 형태 측면에서는 유사하기 때문에 YOLO는 여전히 우수한 경계 상자와 발견을 예측할 수 있습니다. 2. 비교\nYOLO와 다른 발견 도구 간의 비교 성능은 그림을 참조하십시오. 피카소 데이터베이스와 인물 예술 데이터베이스에서, 두 데이터셋은 아트워크에 대한 인물 발견을 테스트하기 위한 것입니다. R-CNN은 VOC 200에서 높은 AP를 얻었습니다. 그러나 RN-KCnn은 아트워크에 적용했을 때 상당히 떨어집니다. DPM은 아트워크에 적용했을 때 AP를 잘 유지합니다. 이전 연구에서는 DPM이 객체의 형태와 레이아웃에 대한 공간 모델을 사용하기 때문에 성능이 우수하다는 이론을 제시했습니다. DPM은 R-CNN(두 단계 객체검출의 원형)만큼 성능이 저하되지만, 더 낮은 AP에서 시작합니다. Y-OLO는 VIC 200에서 좋은 성능을 보였습니다. 그리고 아트워크에 적용했을 때 다른 방법보다 AP가 덜 저하되었습니다. D PM-YOLO 모델과 마찬가지로 객체의 크기와 모양, 그리고 객체와 객체가 일반적으로 나타나는 위치 간의 관계도 중요합니다. 아트워크와 자연 이미지는 픽셀 수준에서는 매우 다르지만 객체의 크기와 모양은 유사하므로 Y-OLO는 여전히 좋은 경계 상자 및 발견을 예측할 수 있습니다. 3. 결론\nYOLO는 사람 발견에서 빠르고 정확합니다. YOLO는 실행됩니다. 피카소 데이터베이스와 인물-예술 데이터베이스에서 다른 검색 도구보다 30배 빠릅니다. Pascal 인물-객체 테스트 세트에서 JOLE 감지율이 12%인 반면, 다른 방법은 6%로 나타났습니다. Yoled와 다른 검색 도구 간의 비교 성능은 수치를 참조하십시오. 이미지 속 인물 테스트 세트에서 Yoled는 이 테스트에서 다른 방법보다 평균 8% 더 높은 평균을 기록했습니다. 하지만 전체 Pascal 객체 테스트 세트에서 비교는 수치를 참조하십시오. [REWRITE 한국어 끝]",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_15_1",
          "easy_paragraph_text": "==보조\n[섹션] 일반화: 아트워크에서 인물 감지\n이 섹션은 17개 섹션 중 15번째입니다.== 이전: 속도. 다음: 실제 환경에서의 실시간 감지. 이 섹션이 주요 섹션입니다. [텍스트]\n.. 중학생도 이해할 수 있도록 작성. 영어 단어는 한국어 의미로 대체. ⟧MATH_i ⟧ 및 ⟧IMAGE_i⟧ 등은 그대로 복사. ==단락당 3~5개 문장.== 간단하고 명확함. 1. 서론\n인물 감지는 이미지에서 사람이 누구인지 찾는 것입니다. YOLOSpeed: YOLO(실시간 지오메트리 탐지 모델 계열(You Only Look Once))는 얼마나 빨리 실행되나요? YOLO는 정말 빠릅니다! 다른 감지 도구는 이미지를 처리하는 데 시간이 오래 걸립니다. 예술 작품에서 인물 발견을 테스트하는 두 데이터베이스인 피카소 데이터베이스와 피플 아트 데이터베이스에서 YOLO를 다른 발견 도구와 비교합니다. 그림은 YOLO와 다른 발견 도구 간의 비교 성능을 보여줍니다. 참고로,\n모든 모델이 VOC 7 데이터로만 훈련된 VOC 2007 발견 AP(평균 정확도)입니다. 피카소 모델은 VOC 2012로 훈련되고 피플 아트 모델은 VIC 2010으로 훈련됩니다. R-CNN은 VIC 2007에서 높은 AP를 보입니다. 그러나 R-CNN은 예술 작품에 적용하면 상당히 떨어집니다. R-CNN은 자연스러운 이미지에 맞춰 조정된 경계 상자 제안에 대해 선택적 검색을 사용합니다. DPM은 아트워크에 적용했을 때 AP를 잘 유지합니다. 이전 연구에서는 DPM이 객체의 형태와 레이아웃에 대한 강력한 공간 모델을 가지고 있기 때문에 성능이 우수하다고 이론화했습니다. DPM은 R-CNN(합성곱신경망)만큼 성능이 저하되지만, 낮은 AP에서 시작합니다. YOLo는 VIC 2007에서 좋은 성능을 보였으며, 아트워크에 적용했을 때 다른 방법보다 AP 저하가 적습니다. DPM과 마찬가지로 YOLO는 객체의 크기와 형태, 객체 간의 관계, 그리고 객체가 일반적으로 나타나는 위치를 모델링합니다. 아트워크와 자연 이미지는 픽셀 수준에서는 매우 다르지만, 객체의 크기와 형태 측면에서는 유사하기 때문에 YOLO는 여전히 우수한 경계 상자와 발견을 예측할 수 있습니다. 2. 비교\nYOLO와 다른 발견 도구 간의 비교 성능은 그림을 참조하십시오. 피카소 데이터베이스와 인물 예술 데이터베이스에서, 두 데이터셋은 아트워크에 대한 인물 발견을 테스트하기 위한 것입니다. R-CNN은 VOC 200에서 높은 AP를 얻었습니다. 그러나 RN-KCnn은 아트워크에 적용했을 때 상당히 떨어집니다. DPM은 아트워크에 적용했을 때 AP를 잘 유지합니다. 이전 연구에서는 DPM이 객체의 형태와 레이아웃에 대한 공간 모델을 사용하기 때문에 성능이 우수하다는 이론을 제시했습니다. DPM은 R-CNN(두 단계 객체검출의 원형)만큼 성능이 저하되지만, 더 낮은 AP에서 시작합니다. Y-OLO는 VIC 200에서 좋은 성능을 보였습니다. 그리고 아트워크에 적용했을 때 다른 방법보다 AP가 덜 저하되었습니다. D PM-YOLO 모델과 마찬가지로 객체의 크기와 모양, 그리고 객체와 객체가 일반적으로 나타나는 위치 간의 관계도 중요합니다. 아트워크와 자연 이미지는 픽셀 수준에서는 매우 다르지만 객체의 크기와 모양은 유사하므로 Y-OLO는 여전히 좋은 경계 상자 및 발견을 예측할 수 있습니다. 3. 결론\nYOLO는 사람 발견에서 빠르고 정확합니다. YOLO는 실행됩니다. 피카소 데이터베이스와 인물-예술 데이터베이스에서 다른 검색 도구보다 30배 빠릅니다. Pascal 인물-객체 테스트 세트에서 JOLE 감지율이 12%인 반면, 다른 방법은 6%로 나타났습니다. Yoled와 다른 검색 도구 간의 비교 성능은 수치를 참조하십시오. 이미지 속 인물 테스트 세트에서 Yoled는 이 테스트에서 다른 방법보다 평균 8% 더 높은 평균을 기록했습니다. 하지만 전체 Pascal 객체 테스트 세트에서 비교는 수치를 참조하십시오. [REWRITE 한국어 끝]",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_16",
      "easy_section_title": "Real-Time Detection In The Wild",
      "easy_section_type": "section",
      "easy_section_order": 16,
      "easy_section_level": 1,
      "easy_content": "보조\n[섹션] 실제 세계에서의 실시간 발견 (한국어: wild(자연세계) 의미: 실제-세계(실제 사용세계)는 YOLO(YOLO: 'You Only Look Once'의 약자로, 한 번만 보고 처리한다는 의미이며, 이미지를 한 번만 보는 가장 빠른 검출기로 가장 빠른 객체 발견 시스템을 만들어냅니다)에서 객체 감지(객체 찾기)를 실제 세계에서. ==이 섹션은 17개 섹션 중 16번째입니다.== 이전: 일반화: 예술 작품에서의 인물 감지. 다음: 결론. ==이 섹션은 YOLO('You only look at an image once'의 약자로, 이미지를 한 번만 보고 처리한다는 의미이며, 이미지를 한 번만 보고 처리한다는 의미입니다.)에 대한 것입니다.\n```\n1.== YOLo('You only look At An Image once'의 약자로, 'once'는 한 번만 보고 처리한다는 의미이며, 이미지를 한 번 보고 처리한다는 의미입니다. 이미지를 한 번 보고 처리한다는 의미이며, 가장 빠른 객체 발견 시스템을 만들어냅니다.)는 빠르고 정확한 객체 감지기로 컴퓨터 비전 애플리케이션에 이상적입니다. Yolo('You only lookAt An Image once'의 약자로, 'once'는 한 번만 처리한다는 의미입니다. 이미지를 한 번만 보면 가장 빠른 물체 발견 시스템(이미지를 한 번 보면 가장 빠른 물체 발견 시스템)을 웹캠에 연결하여 실시간 성능을 유지하는지 확인합니다. 카메라에서 이미지를 가져오고 감지 결과를 표시하는 데 걸리는 시간도 포함됩니다. 결과적으로 시스템은 상호 작용하고 참여적입니다. Yolo('You only look at An Image Once'의 줄임말, 'once'는 한 번만 처리한다는 의미입니다. 이미지를 한 번 보면 가장 빠른 물체 발견 시스템)는 이미지를 개별적으로 처리하지만, 웹캠에 연결하면 추적 시스템처럼 작동하여 물체가 움직이거나 모양이 변하는 것을 감지합니다. 시스템 데모와 소스 코드는 프로젝트 웹사이트(E:)에서 확인할 수 있습니다. [한국어로 다시 작성]\nYOLO('You only look at An Image Once'의 줄임말, 'once'는 한 번만 처리한다는 의미입니다. 이미지를 한 번 보면 가장 빠른 물체 발견 시스템)는 빠르고 정확한 객체 감지기로 컴퓨터 비전 애플리케이션에 이상적입니다. YOLO(You Only Look At An Image Once의 약자, \"Once\"는 한 번만 이미지를 본다는 의미로, 이미지를 한 번만 보는 가장 빠른 객체 감지 시스템)를 웹캠에 연결하여 카메라에서 이미지를 가져와 감지 결과를 표시하는 데 걸리는 시간을 포함하여 실시간 성능을 유지하는지 확인합니다. 그 결과 시스템은 상호 작용하고 참여적입니다. YOLO(You Only Look At An Image Once의 약자, \"Once\"는 한 번만 이미지를 본다는 의미로, 이미지를 한 번만 보는 가장 빠른 객체 감지 시스템)는 웹캠에 연결될 때 개별 이미지를 처리하고 추적 시스템과 같은 기능을 수행하며, 객체의 움직임과 모양 변화를 감지합니다. 시스템 데모와 소스 코드는 프로젝트 웹사이트 E:에서 확인할 수 있습니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_16_1",
          "easy_paragraph_text": "보조\n[섹션] 실제 세계에서의 실시간 발견 (한국어: wild(자연세계) 의미: 실제-세계(실제 사용세계)는 YOLO(YOLO: 'You Only Look Once'의 약자로, 한 번만 보고 처리한다는 의미이며, 이미지를 한 번만 보는 가장 빠른 검출기로 가장 빠른 객체 발견 시스템을 만들어냅니다)에서 객체 감지(객체 찾기)를 실제 세계에서. ==이 섹션은 17개 섹션 중 16번째입니다.== 이전: 일반화: 예술 작품에서의 인물 감지. 다음: 결론. ==이 섹션은 YOLO('You only look at an image once'의 약자로, 이미지를 한 번만 보고 처리한다는 의미이며, 이미지를 한 번만 보고 처리한다는 의미입니다.)에 대한 것입니다.\n```\n1.== YOLo('You only look At An Image once'의 약자로, 'once'는 한 번만 보고 처리한다는 의미이며, 이미지를 한 번 보고 처리한다는 의미입니다. 이미지를 한 번 보고 처리한다는 의미이며, 가장 빠른 객체 발견 시스템을 만들어냅니다.)는 빠르고 정확한 객체 감지기로 컴퓨터 비전 애플리케이션에 이상적입니다. Yolo('You only lookAt An Image once'의 약자로, 'once'는 한 번만 처리한다는 의미입니다. 이미지를 한 번만 보면 가장 빠른 물체 발견 시스템(이미지를 한 번 보면 가장 빠른 물체 발견 시스템)을 웹캠에 연결하여 실시간 성능을 유지하는지 확인합니다. 카메라에서 이미지를 가져오고 감지 결과를 표시하는 데 걸리는 시간도 포함됩니다. 결과적으로 시스템은 상호 작용하고 참여적입니다. Yolo('You only look at An Image Once'의 줄임말, 'once'는 한 번만 처리한다는 의미입니다. 이미지를 한 번 보면 가장 빠른 물체 발견 시스템)는 이미지를 개별적으로 처리하지만, 웹캠에 연결하면 추적 시스템처럼 작동하여 물체가 움직이거나 모양이 변하는 것을 감지합니다. 시스템 데모와 소스 코드는 프로젝트 웹사이트(E:)에서 확인할 수 있습니다. [한국어로 다시 작성]\nYOLO('You only look at An Image Once'의 줄임말, 'once'는 한 번만 처리한다는 의미입니다. 이미지를 한 번 보면 가장 빠른 물체 발견 시스템)는 빠르고 정확한 객체 감지기로 컴퓨터 비전 애플리케이션에 이상적입니다. YOLO(You Only Look At An Image Once의 약자, \"Once\"는 한 번만 이미지를 본다는 의미로, 이미지를 한 번만 보는 가장 빠른 객체 감지 시스템)를 웹캠에 연결하여 카메라에서 이미지를 가져와 감지 결과를 표시하는 데 걸리는 시간을 포함하여 실시간 성능을 유지하는지 확인합니다. 그 결과 시스템은 상호 작용하고 참여적입니다. YOLO(You Only Look At An Image Once의 약자, \"Once\"는 한 번만 이미지를 본다는 의미로, 이미지를 한 번만 보는 가장 빠른 객체 감지 시스템)는 웹캠에 연결될 때 개별 이미지를 처리하고 추적 시스템과 같은 기능을 수행하며, 객체의 움직임과 모양 변화를 감지합니다. 시스템 데모와 소스 코드는 프로젝트 웹사이트 E:에서 확인할 수 있습니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_17",
      "easy_section_title": "Conclusion",
      "easy_section_type": "section",
      "easy_section_order": 17,
      "easy_section_level": 1,
      "easy_content": "==보조자 이 섹션은 17개 섹션 중 17번째입니다.== 이전: 야외에서의 실시간 발견(번역이 이 맥락에서 의미가 없을 수 있음). 이것은 주요 섹션입니다. 우리는 YOLO(You Only Look Once의 약자로, 전체 이미지를 한 번 보고 객체를 감지하는 방법)를 소개합니다(한국어로 간단하고 의미 있게 만듭니다). [객체] 감지를 위한 통합 모델(더 관련성 있는 객체로 대체). 우리 모델은 구성하기 쉽고 전체 이미지에서 직접 학습할 수 있습니다. 분류기 기반과 달리 객체를 직접 감지하지 않고(이미지를 객체로 분류하거나 그렇지 않은 경우), YOLO(고양이, 자동차 등 특정 객체를 감지하는 데 능숙함)는 감지 성능에 직접적으로 해당하는 손실 함수에서 학습하고 전체 모델을 공동으로 학습합니다. ==FastYOLO(YOLO의 빠른 구현)는 문헌에서 가장 빠른 범용 [객체] 감지(이미지에 무엇이 있는지 알아내는)이고 YOLO(예쁘고 기억하기 쉬움)는 실시간 [객체] 발견(비디오에서 객체 찾기)(이미지 처리)(빠른 처리)(웹캠 등)에서 최신(최고의 방법)(실시간 애플리케이션)을 추진하고 YOLo(좋은 알고리즘)는 새로운 도메인(여러 유형의 비디오, 다양한 조명 등)(웹캠 등에 이상적)으로 잘 일반화되어 빠르고 견고한 [객체] 감지(거기에 무엇이 있는지 알아내는)(디버깅 도구 등)(프레임당 15초, 단일 코어 CPU)(웹캠, TV 등)에 의존하는 애플리케이션에 이상적입니다.== : 이 작업은 ONR N000142-131-0712, NSF IISS-133805۴ 및 TheAllen Distinguished의 지원을 부분적으로 받습니다. 조사자상. [재작성]\nYOLO(You Only Looks Once의 줄임말로, 이미지 전체를 한 번 보고 객체를 감지하는 방식)는 빠르고 정확한 객체 감지 알고리즘입니다. 객체 감지를 위한 통합 모델(더 관련성 있는 것으로 대체)은 모든 감지 작업을 하나의 모델로 처리한다는 것을 의미합니다. 이 모델은 구축이 간편하며, 전체 이미지(웹캠 이미지 등)를 기반으로 직접 학습할 수 있습니다. 분류기 기반 모델과 달리, YOLO(고양이, 차 등 특정 객체 감지에 효과적)는 객체를 직접 감지하지 않고 감지 성능에 직접적으로 대응하는 손실 함수를 기반으로 학습합니다. 전체 모델은 공동 학습됩니다. FastYOLO(YOLO의 빠른 구현, 문헌상 가장 좋은 방법)는 문헌상 가장 빠른 범용 집합 감지(이미지에 무엇이 있는지 찾아내는 것)이며, YOLO(좋은 알고리즘)는 실시간 애플리케이션(웹캠 등)에서 객체 모양 등을 실시간으로 실행하여 최첨단(가장 좋은 방법) 상태를 구현하고, YOlo(좋은 알고리즘)는 새로운 도메인(유사한 비디오, 다양한 조명 등(웨비나 등에 이상적)을 일반화하여 빠르고, 견고해야 하는 애플리케이션(객체 모양 누락 등)에 이상적입니다(프레임당 15초, 단일 코어 CPU)(웹캠, TV 등).",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_17_1",
          "easy_paragraph_text": "==보조자 이 섹션은 17개 섹션 중 17번째입니다.== 이전: 야외에서의 실시간 발견(번역이 이 맥락에서 의미가 없을 수 있음). 이것은 주요 섹션입니다. 우리는 YOLO(You Only Look Once의 약자로, 전체 이미지를 한 번 보고 객체를 감지하는 방법)를 소개합니다(한국어로 간단하고 의미 있게 만듭니다). [객체] 감지를 위한 통합 모델(더 관련성 있는 객체로 대체). 우리 모델은 구성하기 쉽고 전체 이미지에서 직접 학습할 수 있습니다. 분류기 기반과 달리 객체를 직접 감지하지 않고(이미지를 객체로 분류하거나 그렇지 않은 경우), YOLO(고양이, 자동차 등 특정 객체를 감지하는 데 능숙함)는 감지 성능에 직접적으로 해당하는 손실 함수에서 학습하고 전체 모델을 공동으로 학습합니다. ==FastYOLO(YOLO의 빠른 구현)는 문헌에서 가장 빠른 범용 [객체] 감지(이미지에 무엇이 있는지 알아내는)이고 YOLO(예쁘고 기억하기 쉬움)는 실시간 [객체] 발견(비디오에서 객체 찾기)(이미지 처리)(빠른 처리)(웹캠 등)에서 최신(최고의 방법)(실시간 애플리케이션)을 추진하고 YOLo(좋은 알고리즘)는 새로운 도메인(여러 유형의 비디오, 다양한 조명 등)(웹캠 등에 이상적)으로 잘 일반화되어 빠르고 견고한 [객체] 감지(거기에 무엇이 있는지 알아내는)(디버깅 도구 등)(프레임당 15초, 단일 코어 CPU)(웹캠, TV 등)에 의존하는 애플리케이션에 이상적입니다.== : 이 작업은 ONR N000142-131-0712, NSF IISS-133805۴ 및 TheAllen Distinguished의 지원을 부분적으로 받습니다. 조사자상. [재작성]\nYOLO(You Only Looks Once의 줄임말로, 이미지 전체를 한 번 보고 객체를 감지하는 방식)는 빠르고 정확한 객체 감지 알고리즘입니다. 객체 감지를 위한 통합 모델(더 관련성 있는 것으로 대체)은 모든 감지 작업을 하나의 모델로 처리한다는 것을 의미합니다. 이 모델은 구축이 간편하며, 전체 이미지(웹캠 이미지 등)를 기반으로 직접 학습할 수 있습니다. 분류기 기반 모델과 달리, YOLO(고양이, 차 등 특정 객체 감지에 효과적)는 객체를 직접 감지하지 않고 감지 성능에 직접적으로 대응하는 손실 함수를 기반으로 학습합니다. 전체 모델은 공동 학습됩니다. FastYOLO(YOLO의 빠른 구현, 문헌상 가장 좋은 방법)는 문헌상 가장 빠른 범용 집합 감지(이미지에 무엇이 있는지 찾아내는 것)이며, YOLO(좋은 알고리즘)는 실시간 애플리케이션(웹캠 등)에서 객체 모양 등을 실시간으로 실행하여 최첨단(가장 좋은 방법) 상태를 구현하고, YOlo(좋은 알고리즘)는 새로운 도메인(유사한 비디오, 다양한 조명 등(웨비나 등에 이상적)을 일반화하여 빠르고, 견고해야 하는 애플리케이션(객체 모양 누락 등)에 이상적입니다(프레임당 15초, 단일 코어 CPU)(웹캠, TV 등).",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": false
        }
      ],
      "easy_visualizations": []
    }
  ],
  "metadata": {
    "generated_at": "2025-09-21 21:23:09",
    "easy_model_version": "yolo-easy-qlora-checkpoint-200",
    "total_processing_time": 1020.23,
    "visualization_triggers": 0,
    "total_paragraphs": 4,
    "section_types": {
      "section": 17,
      "subsection": 0
    },
    "processing_status": "completed",
    "error_count": 1,
    "warnings": [],
    "errors": [
      "배치 처리 타임아웃: 600초"
    ],
    "resumed_from_cache": true,
    "resumed_sections": 13
  }
}