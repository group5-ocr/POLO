{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.3333333333333335,
  "eval_steps": 500,
  "global_step": 3500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0013333333333333333,
      "grad_norm": 1.4581272602081299,
      "learning_rate": 2.9411764705882355e-06,
      "loss": 3.447,
      "step": 1
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 1.2903960943222046,
      "learning_rate": 2.9411764705882354e-05,
      "loss": 3.4215,
      "step": 10
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.9390909075737,
      "learning_rate": 5.882352941176471e-05,
      "loss": 2.9852,
      "step": 20
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.001633644104004,
      "learning_rate": 8.823529411764706e-05,
      "loss": 2.1274,
      "step": 30
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.6207376718521118,
      "learning_rate": 0.00011764705882352942,
      "loss": 1.3983,
      "step": 40
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.8940634727478027,
      "learning_rate": 0.00014705882352941178,
      "loss": 1.0029,
      "step": 50
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.6952006816864014,
      "learning_rate": 0.00017647058823529413,
      "loss": 0.9672,
      "step": 60
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.5495834350585938,
      "learning_rate": 0.00019999958540892524,
      "loss": 0.8563,
      "step": 70
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.4292026162147522,
      "learning_rate": 0.00019998507508226524,
      "loss": 0.9096,
      "step": 80
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.4823049008846283,
      "learning_rate": 0.00019994983863945388,
      "loss": 0.9216,
      "step": 90
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.3913838565349579,
      "learning_rate": 0.0001998938833847273,
      "loss": 0.8184,
      "step": 100
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.34775510430336,
      "learning_rate": 0.00019981722091716783,
      "loss": 0.9481,
      "step": 110
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.35305824875831604,
      "learning_rate": 0.00019971986712829932,
      "loss": 0.8465,
      "step": 120
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.3779923915863037,
      "learning_rate": 0.00019960184219879303,
      "loss": 0.8561,
      "step": 130
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.4695846140384674,
      "learning_rate": 0.00019946317059428448,
      "loss": 0.8583,
      "step": 140
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.29916366934776306,
      "learning_rate": 0.00019930388106030166,
      "loss": 0.869,
      "step": 150
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.3900104761123657,
      "learning_rate": 0.00019912400661630658,
      "loss": 0.8448,
      "step": 160
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.4634402394294739,
      "learning_rate": 0.00019892358454885042,
      "loss": 0.8528,
      "step": 170
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.42351406812667847,
      "learning_rate": 0.00019870265640384435,
      "loss": 0.8367,
      "step": 180
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.7773142457008362,
      "learning_rate": 0.00019846126797794743,
      "loss": 0.8572,
      "step": 190
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.5767457485198975,
      "learning_rate": 0.00019819946930907332,
      "loss": 0.8062,
      "step": 200
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.6388047933578491,
      "learning_rate": 0.00019791731466601773,
      "loss": 0.8165,
      "step": 210
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.5803216099739075,
      "learning_rate": 0.00019761486253720915,
      "loss": 0.8411,
      "step": 220
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.39704135060310364,
      "learning_rate": 0.00019729217561858433,
      "loss": 0.8235,
      "step": 230
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.4458550214767456,
      "learning_rate": 0.00019694932080059217,
      "loss": 0.8828,
      "step": 240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.3398571014404297,
      "learning_rate": 0.00019658636915432788,
      "loss": 0.7745,
      "step": 250
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.4589298665523529,
      "learning_rate": 0.00019620339591680023,
      "loss": 0.7295,
      "step": 260
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.36223509907722473,
      "learning_rate": 0.00019580048047533578,
      "loss": 0.8538,
      "step": 270
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.3559981882572174,
      "learning_rate": 0.0001953777063511223,
      "loss": 0.7,
      "step": 280
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.4147840738296509,
      "learning_rate": 0.00019493516118189582,
      "loss": 0.8268,
      "step": 290
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.42176634073257446,
      "learning_rate": 0.0001944729367037736,
      "loss": 0.7408,
      "step": 300
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.3397749066352844,
      "learning_rate": 0.00019399112873223824,
      "loss": 0.8468,
      "step": 310
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.3048190474510193,
      "learning_rate": 0.00019348983714227583,
      "loss": 0.7907,
      "step": 320
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.3326151669025421,
      "learning_rate": 0.00019296916584767262,
      "loss": 0.8224,
      "step": 330
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.4839555025100708,
      "learning_rate": 0.00019242922277947448,
      "loss": 0.8653,
      "step": 340
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.26312723755836487,
      "learning_rate": 0.00019187011986361374,
      "loss": 0.8082,
      "step": 350
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.32993653416633606,
      "learning_rate": 0.0001912919729977078,
      "loss": 0.795,
      "step": 360
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.3257547914981842,
      "learning_rate": 0.00019069490202703438,
      "loss": 0.7878,
      "step": 370
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.26110321283340454,
      "learning_rate": 0.00019007903071968868,
      "loss": 0.8412,
      "step": 380
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.5051210522651672,
      "learning_rate": 0.00018944448674092714,
      "loss": 0.7581,
      "step": 390
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.6391317844390869,
      "learning_rate": 0.00018879140162670347,
      "loss": 0.8047,
      "step": 400
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.29901808500289917,
      "learning_rate": 0.00018811991075640223,
      "loss": 0.6567,
      "step": 410
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.38887596130371094,
      "learning_rate": 0.00018743015332477588,
      "loss": 0.7143,
      "step": 420
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.3101796805858612,
      "learning_rate": 0.00018672227231309068,
      "loss": 0.8227,
      "step": 430
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.3458477854728699,
      "learning_rate": 0.0001859964144594879,
      "loss": 0.821,
      "step": 440
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.3330483138561249,
      "learning_rate": 0.00018525273022856607,
      "loss": 0.7674,
      "step": 450
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.31087610125541687,
      "learning_rate": 0.00018449137378019094,
      "loss": 0.7585,
      "step": 460
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.2711242735385895,
      "learning_rate": 0.0001837125029375393,
      "loss": 0.7401,
      "step": 470
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.3290107250213623,
      "learning_rate": 0.00018291627915438348,
      "loss": 0.7243,
      "step": 480
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.23758123815059662,
      "learning_rate": 0.00018210286748162336,
      "loss": 0.8069,
      "step": 490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.3657529354095459,
      "learning_rate": 0.00018127243653307248,
      "loss": 0.7347,
      "step": 500
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.4346484839916229,
      "learning_rate": 0.00018042515845050576,
      "loss": 0.8131,
      "step": 510
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.41143354773521423,
      "learning_rate": 0.00017956120886797604,
      "loss": 0.875,
      "step": 520
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.28144243359565735,
      "learning_rate": 0.00017868076687540624,
      "loss": 0.7634,
      "step": 530
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.32069602608680725,
      "learning_rate": 0.0001777840149814657,
      "loss": 0.8427,
      "step": 540
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.3085995316505432,
      "learning_rate": 0.0001768711390757374,
      "loss": 0.7546,
      "step": 550
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.5097686052322388,
      "learning_rate": 0.0001759423283901846,
      "loss": 0.8421,
      "step": 560
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.36155766248703003,
      "learning_rate": 0.00017499777545992452,
      "loss": 0.9193,
      "step": 570
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.35534167289733887,
      "learning_rate": 0.00017403767608331733,
      "loss": 0.6827,
      "step": 580
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.2715034484863281,
      "learning_rate": 0.00017306222928137875,
      "loss": 0.7958,
      "step": 590
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.36948275566101074,
      "learning_rate": 0.00017207163725652445,
      "loss": 0.8107,
      "step": 600
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.2748259902000427,
      "learning_rate": 0.00017106610535065517,
      "loss": 0.8077,
      "step": 610
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.3159612715244293,
      "learning_rate": 0.00017004584200259107,
      "loss": 0.7385,
      "step": 620
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.4649145305156708,
      "learning_rate": 0.00016901105870486372,
      "loss": 0.7944,
      "step": 630
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.37353435158729553,
      "learning_rate": 0.0001679619699598757,
      "loss": 0.8015,
      "step": 640
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.38155683875083923,
      "learning_rate": 0.00016689879323543566,
      "loss": 0.755,
      "step": 650
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.36880365014076233,
      "learning_rate": 0.0001658217489196792,
      "loss": 0.8429,
      "step": 660
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.2522553503513336,
      "learning_rate": 0.00016473106027538393,
      "loss": 0.7809,
      "step": 670
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.3082441985607147,
      "learning_rate": 0.00016362695339368913,
      "loss": 0.8085,
      "step": 680
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.31108909845352173,
      "learning_rate": 0.0001625096571472285,
      "loss": 0.7718,
      "step": 690
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.2551320791244507,
      "learning_rate": 0.00016137940314268695,
      "loss": 0.7744,
      "step": 700
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.464951753616333,
      "learning_rate": 0.00016023642567279033,
      "loss": 0.7644,
      "step": 710
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.3701207637786865,
      "learning_rate": 0.00015908096166773817,
      "loss": 0.8018,
      "step": 720
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.35823848843574524,
      "learning_rate": 0.0001579132506460903,
      "loss": 0.734,
      "step": 730
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.3153184652328491,
      "learning_rate": 0.00015673353466511618,
      "loss": 0.7737,
      "step": 740
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.479140043258667,
      "learning_rate": 0.00015554205827061855,
      "loss": 0.7432,
      "step": 750
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.3092142939567566,
      "learning_rate": 0.0001543390684462409,
      "loss": 0.7843,
      "step": 760
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.29413464665412903,
      "learning_rate": 0.00015312481456226986,
      "loss": 0.7417,
      "step": 770
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.3873836398124695,
      "learning_rate": 0.00015189954832394266,
      "loss": 0.6785,
      "step": 780
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.27844709157943726,
      "learning_rate": 0.00015066352371927047,
      "loss": 0.7433,
      "step": 790
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.3248087465763092,
      "learning_rate": 0.00014941699696638887,
      "loss": 0.7826,
      "step": 800
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.4589260518550873,
      "learning_rate": 0.0001481602264604457,
      "loss": 0.5937,
      "step": 810
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.43225592374801636,
      "learning_rate": 0.00014689347272003813,
      "loss": 0.6577,
      "step": 820
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.3655654489994049,
      "learning_rate": 0.0001456169983332087,
      "loss": 0.7617,
      "step": 830
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.38454630970954895,
      "learning_rate": 0.0001443310679030132,
      "loss": 0.75,
      "step": 840
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.32866576313972473,
      "learning_rate": 0.00014303594799267065,
      "loss": 0.7065,
      "step": 850
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.3102375864982605,
      "learning_rate": 0.0001417319070703066,
      "loss": 0.709,
      "step": 860
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.31685346364974976,
      "learning_rate": 0.00014041921545330193,
      "loss": 0.6278,
      "step": 870
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.41316965222358704,
      "learning_rate": 0.0001390981452522581,
      "loss": 0.6928,
      "step": 880
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.27795812487602234,
      "learning_rate": 0.00013776897031459104,
      "loss": 0.6532,
      "step": 890
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.3567641079425812,
      "learning_rate": 0.00013643196616776432,
      "loss": 0.6885,
      "step": 900
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.35444095730781555,
      "learning_rate": 0.00013508740996217493,
      "loss": 0.7502,
      "step": 910
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.30794161558151245,
      "learning_rate": 0.00013373558041370178,
      "loss": 0.6926,
      "step": 920
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.3235280513763428,
      "learning_rate": 0.00013237675774593045,
      "loss": 0.7486,
      "step": 930
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.3137111961841583,
      "learning_rate": 0.00013101122363206488,
      "loss": 0.6275,
      "step": 940
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.38155290484428406,
      "learning_rate": 0.00012963926113653863,
      "loss": 0.7045,
      "step": 950
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.3092218041419983,
      "learning_rate": 0.0001282611546563382,
      "loss": 0.6741,
      "step": 960
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.3216797709465027,
      "learning_rate": 0.0001268771898620494,
      "loss": 0.7314,
      "step": 970
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.29412439465522766,
      "learning_rate": 0.00012548765363864036,
      "loss": 0.6737,
      "step": 980
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.3175409734249115,
      "learning_rate": 0.00012409283402599238,
      "loss": 0.6769,
      "step": 990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.28733646869659424,
      "learning_rate": 0.00012269302015919172,
      "loss": 0.6807,
      "step": 1000
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.38422247767448425,
      "learning_rate": 0.00012128850220859397,
      "loss": 0.7303,
      "step": 1010
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.41862329840660095,
      "learning_rate": 0.00011987957131967418,
      "loss": 0.6604,
      "step": 1020
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.441974401473999,
      "learning_rate": 0.00011846651955267463,
      "loss": 0.725,
      "step": 1030
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.39434128999710083,
      "learning_rate": 0.00011704963982206299,
      "loss": 0.6704,
      "step": 1040
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.38311144709587097,
      "learning_rate": 0.00011562922583581375,
      "loss": 0.7493,
      "step": 1050
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.2990461587905884,
      "learning_rate": 0.00011420557203452444,
      "loss": 0.6465,
      "step": 1060
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.48382312059402466,
      "learning_rate": 0.00011277897353038085,
      "loss": 0.706,
      "step": 1070
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.38215696811676025,
      "learning_rate": 0.00011134972604598224,
      "loss": 0.7051,
      "step": 1080
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.3464275598526001,
      "learning_rate": 0.00010991812585304069,
      "loss": 0.7317,
      "step": 1090
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.49595606327056885,
      "learning_rate": 0.00010848446971096606,
      "loss": 0.794,
      "step": 1100
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.3426777422428131,
      "learning_rate": 0.0001070490548053503,
      "loss": 0.7529,
      "step": 1110
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.360380619764328,
      "learning_rate": 0.00010561217868636313,
      "loss": 0.7559,
      "step": 1120
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.34159329533576965,
      "learning_rate": 0.00010417413920707222,
      "loss": 0.6584,
      "step": 1130
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.31752172112464905,
      "learning_rate": 0.0001027352344617007,
      "loss": 0.6879,
      "step": 1140
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.3257640302181244,
      "learning_rate": 0.00010129576272383445,
      "loss": 0.611,
      "step": 1150
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.3195342421531677,
      "learning_rate": 9.985602238459247e-05,
      "loss": 0.6952,
      "step": 1160
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.4234406352043152,
      "learning_rate": 9.841631189077269e-05,
      "loss": 0.6415,
      "step": 1170
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.5415201187133789,
      "learning_rate": 9.69769296829864e-05,
      "loss": 0.7547,
      "step": 1180
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.40822380781173706,
      "learning_rate": 9.553817413379356e-05,
      "loss": 0.6066,
      "step": 1190
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.42055565118789673,
      "learning_rate": 9.410034348585298e-05,
      "loss": 0.6866,
      "step": 1200
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.3358847200870514,
      "learning_rate": 9.266373579009867e-05,
      "loss": 0.6896,
      "step": 1210
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.33467164635658264,
      "learning_rate": 9.122864884395633e-05,
      "loss": 0.7666,
      "step": 1220
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.3602589964866638,
      "learning_rate": 8.979538012961221e-05,
      "loss": 0.6759,
      "step": 1230
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.4361417889595032,
      "learning_rate": 8.836422675234754e-05,
      "loss": 0.7082,
      "step": 1240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.36342713236808777,
      "learning_rate": 8.69354853789509e-05,
      "loss": 0.685,
      "step": 1250
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.3425538241863251,
      "learning_rate": 8.550945217622146e-05,
      "loss": 0.674,
      "step": 1260
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.30892619490623474,
      "learning_rate": 8.408642274957612e-05,
      "loss": 0.6827,
      "step": 1270
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.37456685304641724,
      "learning_rate": 8.266669208177252e-05,
      "loss": 0.6595,
      "step": 1280
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.39756691455841064,
      "learning_rate": 8.125055447176186e-05,
      "loss": 0.7125,
      "step": 1290
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.5731307864189148,
      "learning_rate": 7.983830347368276e-05,
      "loss": 0.6906,
      "step": 1300
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.4475215971469879,
      "learning_rate": 7.843023183600988e-05,
      "loss": 0.7039,
      "step": 1310
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.3520793914794922,
      "learning_rate": 7.702663144086957e-05,
      "loss": 0.7384,
      "step": 1320
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.3212534487247467,
      "learning_rate": 7.562779324353477e-05,
      "loss": 0.6999,
      "step": 1330
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.3983745574951172,
      "learning_rate": 7.42340072121126e-05,
      "loss": 0.6761,
      "step": 1340
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.43742135167121887,
      "learning_rate": 7.284556226743598e-05,
      "loss": 0.7246,
      "step": 1350
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.32233360409736633,
      "learning_rate": 7.146274622317288e-05,
      "loss": 0.6642,
      "step": 1360
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.39098963141441345,
      "learning_rate": 7.008584572616448e-05,
      "loss": 0.5978,
      "step": 1370
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.27206677198410034,
      "learning_rate": 6.871514619700594e-05,
      "loss": 0.6609,
      "step": 1380
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.40007513761520386,
      "learning_rate": 6.73509317708807e-05,
      "loss": 0.7811,
      "step": 1390
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.32098180055618286,
      "learning_rate": 6.599348523866155e-05,
      "loss": 0.6889,
      "step": 1400
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.27890294790267944,
      "learning_rate": 6.464308798829043e-05,
      "loss": 0.7069,
      "step": 1410
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.35317710041999817,
      "learning_rate": 6.33000199464487e-05,
      "loss": 0.6516,
      "step": 1420
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.4119522273540497,
      "learning_rate": 6.196455952053084e-05,
      "loss": 0.6984,
      "step": 1430
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.3454528748989105,
      "learning_rate": 6.063698354093255e-05,
      "loss": 0.5956,
      "step": 1440
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.4959975481033325,
      "learning_rate": 5.931756720366621e-05,
      "loss": 0.6204,
      "step": 1450
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.32263293862342834,
      "learning_rate": 5.800658401331467e-05,
      "loss": 0.6673,
      "step": 1460
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.43008068203926086,
      "learning_rate": 5.670430572633607e-05,
      "loss": 0.7116,
      "step": 1470
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.39953893423080444,
      "learning_rate": 5.5411002294730996e-05,
      "loss": 0.6483,
      "step": 1480
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.36131978034973145,
      "learning_rate": 5.412694181008329e-05,
      "loss": 0.7836,
      "step": 1490
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.3051702380180359,
      "learning_rate": 5.285239044798695e-05,
      "loss": 0.6546,
      "step": 1500
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.3886786103248596,
      "learning_rate": 5.1587612412869954e-05,
      "loss": 0.5754,
      "step": 1510
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.5250001549720764,
      "learning_rate": 5.0332869883226817e-05,
      "loss": 0.6183,
      "step": 1520
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.3651137351989746,
      "learning_rate": 4.9088422957271164e-05,
      "loss": 0.5887,
      "step": 1530
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.363803893327713,
      "learning_rate": 4.7854529599019213e-05,
      "loss": 0.5363,
      "step": 1540
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.3222568929195404,
      "learning_rate": 4.6631445584815926e-05,
      "loss": 0.477,
      "step": 1550
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.32762303948402405,
      "learning_rate": 4.54194244503147e-05,
      "loss": 0.561,
      "step": 1560
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.32190564274787903,
      "learning_rate": 4.4218717437921445e-05,
      "loss": 0.6375,
      "step": 1570
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.34437283873558044,
      "learning_rate": 4.302957344471383e-05,
      "loss": 0.6052,
      "step": 1580
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.3893696665763855,
      "learning_rate": 4.185223897084709e-05,
      "loss": 0.568,
      "step": 1590
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.4384823143482208,
      "learning_rate": 4.068695806845624e-05,
      "loss": 0.5698,
      "step": 1600
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.4423026442527771,
      "learning_rate": 3.953397229106636e-05,
      "loss": 0.5965,
      "step": 1610
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.5073160529136658,
      "learning_rate": 3.839352064352012e-05,
      "loss": 0.5779,
      "step": 1620
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.5143606066703796,
      "learning_rate": 3.7265839532434155e-05,
      "loss": 0.6058,
      "step": 1630
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.37774449586868286,
      "learning_rate": 3.615116271719378e-05,
      "loss": 0.5839,
      "step": 1640
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.5928183794021606,
      "learning_rate": 3.504972126149639e-05,
      "loss": 0.5971,
      "step": 1650
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.35983458161354065,
      "learning_rate": 3.396174348545413e-05,
      "loss": 0.5671,
      "step": 1660
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.4279085397720337,
      "learning_rate": 3.288745491826459e-05,
      "loss": 0.5261,
      "step": 1670
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.3986213803291321,
      "learning_rate": 3.182707825146056e-05,
      "loss": 0.5608,
      "step": 1680
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.4144465923309326,
      "learning_rate": 3.078083329274767e-05,
      "loss": 0.5381,
      "step": 1690
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.38864994049072266,
      "learning_rate": 2.9748936920440286e-05,
      "loss": 0.5563,
      "step": 1700
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.44306066632270813,
      "learning_rate": 2.873160303850405e-05,
      "loss": 0.6051,
      "step": 1710
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.34440863132476807,
      "learning_rate": 2.7729042532215456e-05,
      "loss": 0.6372,
      "step": 1720
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.3736667037010193,
      "learning_rate": 2.6741463224446926e-05,
      "loss": 0.5968,
      "step": 1730
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.40530911087989807,
      "learning_rate": 2.576906983258669e-05,
      "loss": 0.5283,
      "step": 1740
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.48786085844039917,
      "learning_rate": 2.481206392610278e-05,
      "loss": 0.5335,
      "step": 1750
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.49732986092567444,
      "learning_rate": 2.3870643884758913e-05,
      "loss": 0.5666,
      "step": 1760
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.48390766978263855,
      "learning_rate": 2.294500485749218e-05,
      "loss": 0.5843,
      "step": 1770
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.43961989879608154,
      "learning_rate": 2.203533872196003e-05,
      "loss": 0.569,
      "step": 1780
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.4861956536769867,
      "learning_rate": 2.1141834044765774e-05,
      "loss": 0.557,
      "step": 1790
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.4264695346355438,
      "learning_rate": 2.0264676042370025e-05,
      "loss": 0.5734,
      "step": 1800
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.41488051414489746,
      "learning_rate": 1.940404654269684e-05,
      "loss": 0.5894,
      "step": 1810
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.38597413897514343,
      "learning_rate": 1.8560123947442298e-05,
      "loss": 0.5292,
      "step": 1820
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.4131404161453247,
      "learning_rate": 1.7733083195093058e-05,
      "loss": 0.5746,
      "step": 1830
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.3645334839820862,
      "learning_rate": 1.6923095724663297e-05,
      "loss": 0.5528,
      "step": 1840
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.41974347829818726,
      "learning_rate": 1.6130329440156432e-05,
      "loss": 0.5685,
      "step": 1850
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.4032869338989258,
      "learning_rate": 1.535494867576016e-05,
      "loss": 0.5625,
      "step": 1860
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.4288334548473358,
      "learning_rate": 1.4597114161781188e-05,
      "loss": 0.6128,
      "step": 1870
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.46174854040145874,
      "learning_rate": 1.3856982991327128e-05,
      "loss": 0.4923,
      "step": 1880
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.3592867851257324,
      "learning_rate": 1.3134708587742362e-05,
      "loss": 0.5417,
      "step": 1890
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.42777302861213684,
      "learning_rate": 1.2430440672804545e-05,
      "loss": 0.571,
      "step": 1900
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.414920449256897,
      "learning_rate": 1.1744325235688536e-05,
      "loss": 0.6218,
      "step": 1910
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.48719242215156555,
      "learning_rate": 1.1076504502703867e-05,
      "loss": 0.5946,
      "step": 1920
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.5081984400749207,
      "learning_rate": 1.042711690781254e-05,
      "loss": 0.6738,
      "step": 1930
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.5230052471160889,
      "learning_rate": 9.796297063932614e-06,
      "loss": 0.6005,
      "step": 1940
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.3537907004356384,
      "learning_rate": 9.18417573503404e-06,
      "loss": 0.5975,
      "step": 1950
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.42471617460250854,
      "learning_rate": 8.590879809032349e-06,
      "loss": 0.5561,
      "step": 1960
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.4424245357513428,
      "learning_rate": 8.016532271485789e-06,
      "loss": 0.6184,
      "step": 1970
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.412121444940567,
      "learning_rate": 7.461252180101352e-06,
      "loss": 0.6484,
      "step": 1980
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.47399836778640747,
      "learning_rate": 6.92515464005511e-06,
      "loss": 0.5284,
      "step": 1990
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.3634008467197418,
      "learning_rate": 6.408350780131778e-06,
      "loss": 0.5946,
      "step": 2000
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.4915643632411957,
      "learning_rate": 0.00019438068984040365,
      "loss": 0.7339,
      "step": 2010
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.4361793100833893,
      "learning_rate": 0.00019430910987745654,
      "loss": 0.6955,
      "step": 2020
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.5832213163375854,
      "learning_rate": 0.00019423709024410198,
      "loss": 0.7149,
      "step": 2030
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.44411399960517883,
      "learning_rate": 0.00019416463127609656,
      "loss": 0.7105,
      "step": 2040
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.32994356751441956,
      "learning_rate": 0.000194091733311245,
      "loss": 0.6163,
      "step": 2050
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.45600298047065735,
      "learning_rate": 0.0001940183966893986,
      "loss": 0.6295,
      "step": 2060
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.4677063822746277,
      "learning_rate": 0.00019394462175245381,
      "loss": 0.6644,
      "step": 2070
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.43852677941322327,
      "learning_rate": 0.00019387040884435037,
      "loss": 0.6365,
      "step": 2080
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.5205081701278687,
      "learning_rate": 0.00019379575831106994,
      "loss": 0.6644,
      "step": 2090
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.5018613934516907,
      "learning_rate": 0.00019372067050063438,
      "loss": 0.6737,
      "step": 2100
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.5907057523727417,
      "learning_rate": 0.00019364514576310408,
      "loss": 0.703,
      "step": 2110
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.5111219882965088,
      "learning_rate": 0.0001935691844505765,
      "loss": 0.7211,
      "step": 2120
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.8512453436851501,
      "learning_rate": 0.00019349278691718427,
      "loss": 0.7074,
      "step": 2130
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.4399866759777069,
      "learning_rate": 0.00019341595351909385,
      "loss": 0.6814,
      "step": 2140
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.9060025811195374,
      "learning_rate": 0.0001933386846145036,
      "loss": 0.7289,
      "step": 2150
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.5565095543861389,
      "learning_rate": 0.00019326098056364222,
      "loss": 0.6768,
      "step": 2160
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.5529168248176575,
      "learning_rate": 0.0001931828417287672,
      "loss": 0.7315,
      "step": 2170
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.3568572700023651,
      "learning_rate": 0.00019310426847416275,
      "loss": 0.6487,
      "step": 2180
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.42870596051216125,
      "learning_rate": 0.00019302526116613864,
      "loss": 0.7236,
      "step": 2190
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.358267605304718,
      "learning_rate": 0.00019294582017302797,
      "loss": 0.676,
      "step": 2200
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.44829756021499634,
      "learning_rate": 0.00019286594586518575,
      "loss": 0.5876,
      "step": 2210
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.5046803951263428,
      "learning_rate": 0.00019278563861498723,
      "loss": 0.6495,
      "step": 2220
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.4391877353191376,
      "learning_rate": 0.00019270489879682592,
      "loss": 0.6797,
      "step": 2230
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.4172569811344147,
      "learning_rate": 0.00019262372678711197,
      "loss": 0.713,
      "step": 2240
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.5016885995864868,
      "learning_rate": 0.00019254212296427044,
      "loss": 0.6043,
      "step": 2250
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.5173116326332092,
      "learning_rate": 0.0001924600877087395,
      "loss": 0.6785,
      "step": 2260
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.3883545398712158,
      "learning_rate": 0.00019237762140296875,
      "loss": 0.678,
      "step": 2270
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.4427054524421692,
      "learning_rate": 0.0001922947244314172,
      "loss": 0.6718,
      "step": 2280
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.4031435251235962,
      "learning_rate": 0.0001922113971805517,
      "loss": 0.7178,
      "step": 2290
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.6622441411018372,
      "learning_rate": 0.0001921276400388451,
      "loss": 0.5982,
      "step": 2300
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.4741901159286499,
      "learning_rate": 0.00019204345339677442,
      "loss": 0.6928,
      "step": 2310
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.44821450114250183,
      "learning_rate": 0.00019195883764681893,
      "loss": 0.6631,
      "step": 2320
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.56986403465271,
      "learning_rate": 0.00019187379318345846,
      "loss": 0.7637,
      "step": 2330
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.41100049018859863,
      "learning_rate": 0.00019178832040317155,
      "loss": 0.6608,
      "step": 2340
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.40413978695869446,
      "learning_rate": 0.00019170241970443343,
      "loss": 0.7731,
      "step": 2350
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.4087977409362793,
      "learning_rate": 0.00019161609148771443,
      "loss": 0.6406,
      "step": 2360
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.42627575993537903,
      "learning_rate": 0.00019152933615547798,
      "loss": 0.6607,
      "step": 2370
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.5132619142532349,
      "learning_rate": 0.0001914421541121785,
      "loss": 0.718,
      "step": 2380
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.5938735604286194,
      "learning_rate": 0.0001913545457642601,
      "loss": 0.6966,
      "step": 2390
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.3958433270454407,
      "learning_rate": 0.00019126651152015403,
      "loss": 0.6913,
      "step": 2400
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.38630568981170654,
      "learning_rate": 0.00019117805179027722,
      "loss": 0.6947,
      "step": 2410
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.36915552616119385,
      "learning_rate": 0.00019108916698703013,
      "loss": 0.7092,
      "step": 2420
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.44653254747390747,
      "learning_rate": 0.00019099985752479506,
      "loss": 0.6785,
      "step": 2430
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.43162816762924194,
      "learning_rate": 0.0001909101238199339,
      "loss": 0.7793,
      "step": 2440
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.44084644317626953,
      "learning_rate": 0.00019081996629078657,
      "loss": 0.6874,
      "step": 2450
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.784134030342102,
      "learning_rate": 0.00019072938535766865,
      "loss": 0.7005,
      "step": 2460
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 0.40393367409706116,
      "learning_rate": 0.00019063838144286975,
      "loss": 0.679,
      "step": 2470
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.5422744154930115,
      "learning_rate": 0.00019054695497065143,
      "loss": 0.7252,
      "step": 2480
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.5047000646591187,
      "learning_rate": 0.0001904551063672452,
      "loss": 0.8597,
      "step": 2490
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.37347084283828735,
      "learning_rate": 0.00019036283606085053,
      "loss": 0.7519,
      "step": 2500
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 0.6675812602043152,
      "learning_rate": 0.00019027014448163296,
      "loss": 0.6745,
      "step": 2510
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.5624022483825684,
      "learning_rate": 0.00019017703206172185,
      "loss": 0.7467,
      "step": 2520
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 0.4572383463382721,
      "learning_rate": 0.0001900834992352087,
      "loss": 0.7024,
      "step": 2530
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.38173165917396545,
      "learning_rate": 0.00018998954643814484,
      "loss": 0.7042,
      "step": 2540
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.46453171968460083,
      "learning_rate": 0.00018989517410853955,
      "loss": 0.732,
      "step": 2550
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.42881882190704346,
      "learning_rate": 0.00018980038268635795,
      "loss": 0.7397,
      "step": 2560
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 0.4607093036174774,
      "learning_rate": 0.00018970517261351902,
      "loss": 0.7479,
      "step": 2570
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.4067201316356659,
      "learning_rate": 0.00018960954433389345,
      "loss": 0.7129,
      "step": 2580
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 0.46372726559638977,
      "learning_rate": 0.00018951349829330168,
      "loss": 0.7986,
      "step": 2590
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.4203251302242279,
      "learning_rate": 0.00018941703493951164,
      "loss": 0.7775,
      "step": 2600
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.5268875360488892,
      "learning_rate": 0.00018932015472223693,
      "loss": 0.7412,
      "step": 2610
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.38931992650032043,
      "learning_rate": 0.00018922285809313443,
      "loss": 0.6173,
      "step": 2620
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 0.5246401429176331,
      "learning_rate": 0.00018912514550580242,
      "loss": 0.6647,
      "step": 2630
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.36779066920280457,
      "learning_rate": 0.0001890270174157784,
      "loss": 0.717,
      "step": 2640
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.4370979070663452,
      "learning_rate": 0.00018892847428053693,
      "loss": 0.6884,
      "step": 2650
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.4278066158294678,
      "learning_rate": 0.0001888295165594874,
      "loss": 0.6846,
      "step": 2660
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.5058304071426392,
      "learning_rate": 0.00018873014471397224,
      "loss": 0.743,
      "step": 2670
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.440653920173645,
      "learning_rate": 0.00018863035920726432,
      "loss": 0.7286,
      "step": 2680
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 0.35406744480133057,
      "learning_rate": 0.0001885301605045651,
      "loss": 0.7007,
      "step": 2690
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.4558597207069397,
      "learning_rate": 0.00018842954907300236,
      "loss": 0.7299,
      "step": 2700
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 0.4290913939476013,
      "learning_rate": 0.00018832852538162804,
      "loss": 0.7519,
      "step": 2710
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.4135436415672302,
      "learning_rate": 0.000188227089901416,
      "loss": 0.6964,
      "step": 2720
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.3224584758281708,
      "learning_rate": 0.0001881252431052599,
      "loss": 0.7002,
      "step": 2730
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.4174780547618866,
      "learning_rate": 0.00018802298546797094,
      "loss": 0.7298,
      "step": 2740
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.5944914221763611,
      "learning_rate": 0.00018792031746627563,
      "loss": 0.5973,
      "step": 2750
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.6938247680664062,
      "learning_rate": 0.00018781723957881372,
      "loss": 0.6828,
      "step": 2760
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 0.3967666029930115,
      "learning_rate": 0.00018771375228613578,
      "loss": 0.7255,
      "step": 2770
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.4823635518550873,
      "learning_rate": 0.000187609856070701,
      "loss": 0.6767,
      "step": 2780
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.37980467081069946,
      "learning_rate": 0.000187505551416875,
      "loss": 0.7492,
      "step": 2790
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.44082868099212646,
      "learning_rate": 0.0001874008388109276,
      "loss": 0.6667,
      "step": 2800
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 0.4847465753555298,
      "learning_rate": 0.0001872957187410304,
      "loss": 0.7812,
      "step": 2810
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.5754965543746948,
      "learning_rate": 0.00018719019169725472,
      "loss": 0.7593,
      "step": 2820
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 0.5163543224334717,
      "learning_rate": 0.0001870842581715691,
      "loss": 0.8004,
      "step": 2830
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.5029464364051819,
      "learning_rate": 0.00018697791865783712,
      "loss": 0.7271,
      "step": 2840
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.43414944410324097,
      "learning_rate": 0.00018687117365181512,
      "loss": 0.749,
      "step": 2850
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.6050747632980347,
      "learning_rate": 0.00018676402365114982,
      "loss": 0.7147,
      "step": 2860
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 0.45726290345191956,
      "learning_rate": 0.00018665646915537608,
      "loss": 0.6817,
      "step": 2870
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.587476372718811,
      "learning_rate": 0.00018654851066591448,
      "loss": 0.6795,
      "step": 2880
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 0.5582399964332581,
      "learning_rate": 0.00018644014868606895,
      "loss": 0.7083,
      "step": 2890
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.3663420081138611,
      "learning_rate": 0.00018633138372102468,
      "loss": 0.7246,
      "step": 2900
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.3770778179168701,
      "learning_rate": 0.0001862222162778454,
      "loss": 0.6317,
      "step": 2910
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.4949782192707062,
      "learning_rate": 0.00018611264686547134,
      "loss": 0.756,
      "step": 2920
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 0.43961480259895325,
      "learning_rate": 0.0001860026759947166,
      "loss": 0.6588,
      "step": 2930
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.4971274435520172,
      "learning_rate": 0.00018589230417826697,
      "loss": 0.7341,
      "step": 2940
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.45380058884620667,
      "learning_rate": 0.00018578153193067745,
      "loss": 0.7546,
      "step": 2950
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.4255065321922302,
      "learning_rate": 0.00018567035976836975,
      "loss": 0.8097,
      "step": 2960
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.41361361742019653,
      "learning_rate": 0.00018555878820963013,
      "loss": 0.6766,
      "step": 2970
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.520681619644165,
      "learning_rate": 0.00018544681777460674,
      "loss": 0.6935,
      "step": 2980
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 0.6141211986541748,
      "learning_rate": 0.0001853344489853074,
      "loss": 0.7594,
      "step": 2990
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.4807531237602234,
      "learning_rate": 0.00018522168236559695,
      "loss": 0.6895,
      "step": 3000
    },
    {
      "epoch": 2.006666666666667,
      "grad_norm": 0.36146029829978943,
      "learning_rate": 0.00018510851844119494,
      "loss": 0.5695,
      "step": 3010
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.49981924891471863,
      "learning_rate": 0.00018499495773967325,
      "loss": 0.6148,
      "step": 3020
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.5503932237625122,
      "learning_rate": 0.00018488100079045344,
      "loss": 0.5625,
      "step": 3030
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.5422536730766296,
      "learning_rate": 0.00018476664812480448,
      "loss": 0.6003,
      "step": 3040
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.4403873383998871,
      "learning_rate": 0.00018465190027584005,
      "loss": 0.6224,
      "step": 3050
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.3741668462753296,
      "learning_rate": 0.00018453675777851627,
      "loss": 0.5414,
      "step": 3060
    },
    {
      "epoch": 2.046666666666667,
      "grad_norm": 0.5968436598777771,
      "learning_rate": 0.0001844212211696291,
      "loss": 0.647,
      "step": 3070
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.3679375648498535,
      "learning_rate": 0.0001843052909878119,
      "loss": 0.5676,
      "step": 3080
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.5729814767837524,
      "learning_rate": 0.0001841889677735327,
      "loss": 0.5324,
      "step": 3090
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.5805933475494385,
      "learning_rate": 0.00018407225206909208,
      "loss": 0.5827,
      "step": 3100
    },
    {
      "epoch": 2.0733333333333333,
      "grad_norm": 0.4691943824291229,
      "learning_rate": 0.00018395514441862026,
      "loss": 0.5526,
      "step": 3110
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.36965975165367126,
      "learning_rate": 0.00018383764536807485,
      "loss": 0.5972,
      "step": 3120
    },
    {
      "epoch": 2.086666666666667,
      "grad_norm": 0.46257829666137695,
      "learning_rate": 0.00018371975546523794,
      "loss": 0.6423,
      "step": 3130
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.4446243941783905,
      "learning_rate": 0.00018360147525971402,
      "loss": 0.5978,
      "step": 3140
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.4933130443096161,
      "learning_rate": 0.00018348280530292713,
      "loss": 0.5832,
      "step": 3150
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.5800439715385437,
      "learning_rate": 0.0001833637461481182,
      "loss": 0.6317,
      "step": 3160
    },
    {
      "epoch": 2.1133333333333333,
      "grad_norm": 0.39199748635292053,
      "learning_rate": 0.00018324429835034275,
      "loss": 0.6286,
      "step": 3170
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.5610737204551697,
      "learning_rate": 0.0001831244624664681,
      "loss": 0.5962,
      "step": 3180
    },
    {
      "epoch": 2.1266666666666665,
      "grad_norm": 0.4906185269355774,
      "learning_rate": 0.0001830042390551708,
      "loss": 0.542,
      "step": 3190
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.45451635122299194,
      "learning_rate": 0.00018288362867693414,
      "loss": 0.6166,
      "step": 3200
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.6579099893569946,
      "learning_rate": 0.0001827626318940454,
      "loss": 0.6231,
      "step": 3210
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.37701335549354553,
      "learning_rate": 0.0001826412492705933,
      "loss": 0.5651,
      "step": 3220
    },
    {
      "epoch": 2.1533333333333333,
      "grad_norm": 0.6145634651184082,
      "learning_rate": 0.00018251948137246537,
      "loss": 0.6912,
      "step": 3230
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.5528450608253479,
      "learning_rate": 0.00018239732876734527,
      "loss": 0.6091,
      "step": 3240
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.44744107127189636,
      "learning_rate": 0.00018227479202471015,
      "loss": 0.598,
      "step": 3250
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.4719462990760803,
      "learning_rate": 0.00018215187171582802,
      "loss": 0.6143,
      "step": 3260
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.601976215839386,
      "learning_rate": 0.00018202856841375518,
      "loss": 0.594,
      "step": 3270
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.6492568254470825,
      "learning_rate": 0.00018190488269333334,
      "loss": 0.6222,
      "step": 3280
    },
    {
      "epoch": 2.1933333333333334,
      "grad_norm": 0.5627527236938477,
      "learning_rate": 0.00018178081513118706,
      "loss": 0.5428,
      "step": 3290
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.4127953350543976,
      "learning_rate": 0.0001816563663057211,
      "loss": 0.6152,
      "step": 3300
    },
    {
      "epoch": 2.2066666666666666,
      "grad_norm": 0.4524516463279724,
      "learning_rate": 0.00018153153679711763,
      "loss": 0.6603,
      "step": 3310
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.38373345136642456,
      "learning_rate": 0.0001814063271873336,
      "loss": 0.5226,
      "step": 3320
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.5634677410125732,
      "learning_rate": 0.000181280738060098,
      "loss": 0.6621,
      "step": 3330
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.39349886775016785,
      "learning_rate": 0.00018115477000090908,
      "loss": 0.5354,
      "step": 3340
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.5610257983207703,
      "learning_rate": 0.00018102842359703176,
      "loss": 0.6472,
      "step": 3350
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.49447280168533325,
      "learning_rate": 0.00018090169943749476,
      "loss": 0.6552,
      "step": 3360
    },
    {
      "epoch": 2.2466666666666666,
      "grad_norm": 0.5446059703826904,
      "learning_rate": 0.00018077459811308787,
      "loss": 0.5711,
      "step": 3370
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.5658404231071472,
      "learning_rate": 0.00018064712021635934,
      "loss": 0.59,
      "step": 3380
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.7136204242706299,
      "learning_rate": 0.00018051926634161282,
      "loss": 0.6541,
      "step": 3390
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.4437832236289978,
      "learning_rate": 0.000180391037084905,
      "loss": 0.5602,
      "step": 3400
    },
    {
      "epoch": 2.2733333333333334,
      "grad_norm": 0.4453965723514557,
      "learning_rate": 0.00018026243304404245,
      "loss": 0.5531,
      "step": 3410
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.42575228214263916,
      "learning_rate": 0.00018013345481857903,
      "loss": 0.6318,
      "step": 3420
    },
    {
      "epoch": 2.2866666666666666,
      "grad_norm": 0.600197970867157,
      "learning_rate": 0.00018000410300981302,
      "loss": 0.6751,
      "step": 3430
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.4305298328399658,
      "learning_rate": 0.00017987437822078442,
      "loss": 0.5867,
      "step": 3440
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.680357038974762,
      "learning_rate": 0.00017974428105627208,
      "loss": 0.6141,
      "step": 3450
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.7304561138153076,
      "learning_rate": 0.00017961381212279077,
      "loss": 0.5877,
      "step": 3460
    },
    {
      "epoch": 2.3133333333333335,
      "grad_norm": 0.5927037000656128,
      "learning_rate": 0.00017948297202858852,
      "loss": 0.667,
      "step": 3470
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.4395958185195923,
      "learning_rate": 0.0001793517613836437,
      "loss": 0.6535,
      "step": 3480
    },
    {
      "epoch": 2.3266666666666667,
      "grad_norm": 0.41141119599342346,
      "learning_rate": 0.0001792201807996622,
      "loss": 0.589,
      "step": 3490
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.5011242628097534,
      "learning_rate": 0.00017908823089007457,
      "loss": 0.5944,
      "step": 3500
    }
  ],
  "logging_steps": 10,
  "max_steps": 15000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.380366188752691e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
