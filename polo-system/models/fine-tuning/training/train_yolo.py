#!/usr/bin/env python3
"""
YOLO ë…¼ë¬¸ Easy ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
íŒŒë¼ë¯¸í„° ì •ë³´ì™€ í•¨ê»˜ í•™ìŠµ ì§„í–‰
"""
import os
import sys
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model
import json

def analyze_model_parameters(model_path: str = "meta-llama/Llama-3.2-3B-Instruct"):
    """ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¶„ì„"""
    print("ğŸ” ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¶„ì„ ì¤‘...")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    print(f"ğŸ“ í† í¬ë‚˜ì´ì € vocab í¬ê¸°: {len(tokenizer)}")
    
    # ëª¨ë¸ ë¡œë“œ (CPUì—ì„œ íŒŒë¼ë¯¸í„°ë§Œ í™•ì¸)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float32,  # ì •í™•í•œ ê³„ì‚°ì„ ìœ„í•´ float32
        device_map="cpu",
        trust_remote_code=True
    )
    
    # ì „ì²´ íŒŒë¼ë¯¸í„° ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,} ({total_params/1e9:.2f}B)")
    
    # ë ˆì´ì–´ë³„ íŒŒë¼ë¯¸í„° ë¶„ì„
    print("\nğŸ“‹ ë ˆì´ì–´ë³„ íŒŒë¼ë¯¸í„° ë¶„ì„:")
    layer_params = {}
    for name, param in model.named_parameters():
        layer_type = name.split('.')[0] if '.' in name else name
        if layer_type not in layer_params:
            layer_params[layer_type] = 0
        layer_params[layer_type] += param.numel()
    
    for layer, count in sorted(layer_params.items()):
        print(f"  {layer}: {count:,} ({count/total_params*100:.1f}%)")
    
    # LoRA ì„¤ì • ì‹œë®¬ë ˆì´ì…˜
    print("\nğŸ”§ LoRA ì„¤ì • ì‹œë®¬ë ˆì´ì…˜:")
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
    lora_r = 16
    lora_alpha = 32
    
    # ê° íƒ€ê²Ÿ ëª¨ë“ˆì˜ íŒŒë¼ë¯¸í„° ê³„ì‚°
    lora_params = 0
    for name, param in model.named_parameters():
        if any(target in name for target in target_modules):
            # LoRA íŒŒë¼ë¯¸í„° = 2 * rank * (input_dim + output_dim)
            # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•˜ì§€ë§Œ ê·¼ì‚¬ì¹˜
            if 'weight' in name:
                input_dim = param.shape[1]
                output_dim = param.shape[0]
                lora_param_count = 2 * lora_r * (input_dim + output_dim)
                lora_params += lora_param_count
                print(f"  {name}: {lora_param_count:,} LoRA íŒŒë¼ë¯¸í„°")
    
    print(f"\nğŸ“ˆ LoRA íŒŒë¼ë¯¸í„° ì˜ˆìƒ: {lora_params:,} ({lora_params/total_params*100:.3f}%)")
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì ˆì•½: {(total_params - lora_params)/total_params*100:.1f}%")
    
    return total_params, lora_params

def check_yolo_data(json_path: str = "training/yolo_v1.json"):
    """YOLO ë°ì´í„° ë¶„ì„"""
    print(f"\nğŸ“š YOLO ë°ì´í„° ë¶„ì„: {json_path}")
    
    if not os.path.exists(json_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
        return
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“„ ì´ ë¬¸ì¥ ìˆ˜: {len(data)}")
    
    # ì„¹ì…˜ë³„ ë¶„ì„
    sections = {}
    for item in data:
        major = item['section_major']
        minor = item['section_minor']
        key = f"{major}.{minor}"
        if key not in sections:
            sections[key] = 0
        sections[key] += 1
    
    print("ğŸ“Š ì„¹ì…˜ë³„ ë¬¸ì¥ ìˆ˜:")
    for key in sorted(sections.keys()):
        print(f"  {key}: {sections[key]} ë¬¸ì¥")
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
    original_lengths = [len(item['original']) for item in data]
    simplified_lengths = [len(item['simplified']) for item in data]
    
    print(f"\nğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„:")
    print(f"  ì›ë¬¸ í‰ê· : {sum(original_lengths)/len(original_lengths):.0f} ë¬¸ì")
    print(f"  ê°„ì†Œí™” í‰ê· : {sum(simplified_lengths)/len(simplified_lengths):.0f} ë¬¸ì")
    print(f"  í™•ì¥ ë¹„ìœ¨: {sum(simplified_lengths)/sum(original_lengths):.2f}x")

def main():
    print("ğŸš€ YOLO ë…¼ë¬¸ Easy ëª¨ë¸ íŒŒì¸íŠœë‹ ì¤€ë¹„")
    print("=" * 50)
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¶„ì„
    total_params, lora_params = analyze_model_parameters()
    
    # YOLO ë°ì´í„° ë¶„ì„
    check_yolo_data()
    
    print("\n" + "=" * 50)
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print("\nğŸ“‹ í•™ìŠµ ì„¤ì • ìš”ì•½:")
    print("  - ëª¨ë¸: meta-llama/Llama-3.2-3B-Instruct")
    print("  - ì „ì²´ íŒŒë¼ë¯¸í„°: {:,} ({:.2f}B)".format(total_params, total_params/1e9))
    print("  - LoRA íŒŒë¼ë¯¸í„°: {:,} ({:.3f}%)".format(lora_params, lora_params/total_params*100))
    print("  - í•™ìŠµ ë°©ì‹: QLoRA (4-bit quantization)")
    print("  - íƒ€ê²Ÿ ëª¨ë“ˆ: q_proj, k_proj, v_proj, o_proj")
    print("  - LoRA rank: 16, alpha: 32")
    
    print("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. ì²´í¬í¬ì¸íŠ¸ í™•ì¸: python check_checkpoint.py")
    print("2. í•™ìŠµ ì‹œì‘: docker-compose up yolo-train")
    print("3. í•™ìŠµ ëª¨ë‹ˆí„°ë§: docker logs -f yolo-train")

if __name__ == "__main__":
    main()
