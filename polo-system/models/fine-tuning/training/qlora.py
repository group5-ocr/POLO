"""
í•„ìˆ˜ íŒ¨í‚¤ì§€ ë²„ì „(ê¶Œì¥):
  transformers==4.44.2, accelerate==0.33.0, peft==0.11.1, trl==0.9.6,
  datasets==2.20.0, bitsandbytes==0.43.1, sentencepiece, einops, evaluate, scipy
"""

from __future__ import annotations
import os
import json
import argparse
from typing import Dict, List, Optional

import torch
from datasets import load_dataset, Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    BitsAndBytesConfig,
)
from peft import LoraConfig, PeftModel
from trl import SFTTrainer

# =============================
# 1) ë°ì´í„° ì „ì²˜ë¦¬ ìœ í‹¸
# =============================

SYS_PROMPT = (
    "ë‹¹ì‹ ì€ ì–´ë ¤ìš´ ë…¼ë¬¸ì„ ì‰½ê²Œ í’€ì–´ ì„¤ëª…í•˜ëŠ” ê³¼í•™ ì»¤ë®¤ë‹ˆì¼€ì´í„°ì…ë‹ˆë‹¤. "
    "í•µì‹¬ë§Œ ì •í™•íˆ, ì‰¬ìš´ í•œêµ­ì–´ë¡œ ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”."
)

INSTRUCT_TEMPLATE = (
    "<|system|>\n{sys}\n</|system|>\n"
    "<|user|>\n{instr}\n{inp}\n</|user|>\n"
    "<|assistant|>\n{out}\n</|assistant|>"
)

PLAIN_TEMPLATE = "<|system|>\n{sys}\n</|system|>\n<|user|>\n{txt}\n</|user|>\n<|assistant|>\n"


def build_text_from_row(row: Dict) -> str:
    """ì…ë ¥ JSON ë ˆì½”ë“œë¥¼ Qwen í¬ë§· í…ìŠ¤íŠ¸ë¡œ ë³€í™˜.
    - ì§€ì‹œí˜•(instruction/input/output) ë˜ëŠ” ë‹¨ì¼ text ì§€ì›
    """
    if all(k in row for k in ("instruction", "output")):
        instr = str(row.get("instruction", "")).strip()
        inp = str(row.get("input", "")).strip()
        out = str(row.get("output", "")).strip()
        inp_block = f"\nì…ë ¥:\n{inp}" if inp else ""
        return INSTRUCT_TEMPLATE.format(sys=SYS_PROMPT, instr=instr, inp=inp_block, out=out)
    elif "text" in row:
        return PLAIN_TEMPLATE.format(sys=SYS_PROMPT, txt=str(row["text"]).strip())
    else:
        # ìµœì†Œí•œì˜ í´ë°±: ëª¨ë“  ê°’ì„ ì´ì–´ë¶™ì„
        txt = json.dumps(row, ensure_ascii=False)
        return PLAIN_TEMPLATE.format(sys=SYS_PROMPT, txt=txt)


# =============================
# 2) ëª¨ë¸/í† í¬ë‚˜ì´ì € ì¤€ë¹„ (4bit QLoRA)
# =============================

def prepare_model_and_tokenizer(model_name_or_path: str,
                                use_bf16: bool = True,
                                load_in_4bit: bool = True,
                                bnb_quant_type: str = "nf4",
                                bnb_compute_dtype: str = "bfloat16",
                                gradient_checkpointing: bool = True):
    # í† í¬ë‚˜ì´ì €
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)
    if tokenizer.pad_token is None:
        # Qwenì€ ë³´í†µ eos_tokenì„ padë¡œ ì¬ì‚¬ìš©
        tokenizer.pad_token = tokenizer.eos_token

    # 4bit ì–‘ìí™” ì„¤ì •
    quant_dtype = torch.bfloat16 if bnb_compute_dtype.lower().startswith("bf") else torch.float16
    quant_config = None
    if load_in_4bit:
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=bnb_quant_type,  # "nf4" ê¶Œì¥
            bnb_4bit_compute_dtype=quant_dtype,
            bnb_4bit_use_double_quant=True,
        )

    # ë””ë°”ì´ìŠ¤ ê°•ì œ (CUDA í•„ìˆ˜)
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA(GPU)ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. NVIDIA ë“œë¼ì´ë²„/torch cu124 ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    selected_device_map = "cuda"
    try:
        torch.cuda.set_device(0)
    except Exception:
        pass
    print(f"[device] cuda_available={torch.cuda.is_available()} | device_map={selected_device_map} | name={torch.cuda.get_device_name(0)}")

    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,
        device_map=selected_device_map,
        torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,
        quantization_config=quant_config,
        trust_remote_code=True,
    )

    if gradient_checkpointing:
        model.gradient_checkpointing_enable()
        model.config.use_cache = False  # grad ckpt ì‹œ ê¶Œì¥

    return model, tokenizer


# =============================
# 3) ë°ì´í„°ì…‹ ë¡œë”©
# =============================

def load_training_dataset(dataset_name: Optional[str], train_file: Optional[str]) -> Dataset:
    if dataset_name:
        ds = load_dataset(dataset_name)
        ds = ds["train"] if "train" in ds else list(ds.values())[0]
    elif train_file:
        ext = os.path.splitext(train_file)[1].lower()
        if ext in [".jsonl", ".json"]:
            ds = load_dataset("json", data_files=train_file)["train"]
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” train_file í™•ì¥ìì…ë‹ˆë‹¤. json/jsonlë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
    else:
        raise ValueError("--dataset_name ë˜ëŠ” --train_file ì¤‘ í•˜ë‚˜ëŠ” ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

    # text í•„ë“œ ìƒì„±
    def _mapper(ex):
        return {"text": build_text_from_row(ex)}

    return ds.map(_mapper, remove_columns=[c for c in ds.column_names if c != "text"])  # textë§Œ ë‚¨ê¹€


# =============================
# 4) íŠ¸ë ˆì´ë„ˆ êµ¬ì„± ë° í•™ìŠµ
# =============================

def train(args):
    # ëŸ°íƒ€ì„ CUDA ìƒíƒœ ë¡œê·¸
    print(
        "[cuda] avail=", torch.cuda.is_available(),
        " version=", torch.version.cuda,
        " device=", (torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"),
    )
    model, tokenizer = prepare_model_and_tokenizer(
        args.model_name_or_path,
        use_bf16=args.bf16,
        load_in_4bit=args.bnb_4bit,
        bnb_quant_type=args.bnb_4bit_quant_type,
        bnb_compute_dtype=args.bnb_4bit_compute_dtype,
        gradient_checkpointing=args.gradient_checkpointing,
    )

    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=args.target_modules.split(",") if args.target_modules else None,
    )

    dataset = load_training_dataset(args.dataset_name, args.train_file)

    # ë°ì´í„° ìƒ˜í”Œë§: train_fraction ë¹„ìœ¨ë¡œ ì‚¬ìš© (ì…”í”Œ í›„ ì•ë¶€ë¶„ ì„ íƒ)
    if args.train_fraction is not None and 0.0 < args.train_fraction < 1.0:
        dataset = dataset.shuffle(seed=args.seed)
        target_size = max(1, int(len(dataset) * args.train_fraction))
        dataset = dataset.select(range(target_size))
        print(f"[info] Using train_fraction={args.train_fraction:.2f} => {target_size} samples out of {len(dataset)}")

    # íŒ¨í‚¹: ì—¬ëŸ¬ ìƒ˜í”Œì„ í•˜ë‚˜ì˜ ê¸´ ì‹œí€€ìŠ¤ë¡œ ì´ì–´ë¶™ì—¬ íš¨ìœ¨ ìƒìŠ¹
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        num_train_epochs=args.num_train_epochs,
        logging_steps=args.logging_steps,
        save_steps=args.save_every_steps,
        save_total_limit=args.save_total_limit,
        bf16=args.bf16,
        fp16=(not args.bf16),
        lr_scheduler_type=args.lr_scheduler_type,
        warmup_ratio=args.warmup_ratio,
        optim="paged_adamw_8bit",  # 4bitì™€ ê¶í•© ì¢‹ìŒ
        report_to=["tensorboard"] if args.report_to_tensorboard else [],
    )

    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        peft_config=lora_config,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=args.max_seq_length,
        packing=args.packing,
        args=training_args,
    )

    trainer.train()
    trainer.model.save_pretrained(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)

    print(f"\n[ì™„ë£Œ] LoRA ì–´ëŒ‘í„°ê°€ '{args.output_dir}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# =============================
# 5) ì–´ëŒ‘í„° ë³‘í•©(ë¨¸ì§€)
# =============================

def merge_adapters(args):
    if not args.adapter_path:
        raise ValueError("--adapter_path ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    base = args.model_name_or_path
    model = AutoModelForCausalLM.from_pretrained(
        base,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True,
    )
    model = PeftModel.from_pretrained(model, args.adapter_path)
    merged = model.merge_and_unload()
    os.makedirs(args.merge_save_dir, exist_ok=True)
    merged.save_pretrained(args.merge_save_dir, safe_serialization=True)

    tok = AutoTokenizer.from_pretrained(base, use_fast=True)
    tok.save_pretrained(args.merge_save_dir)
    print(f"[ì™„ë£Œ] ë³‘í•©ëœ ëª¨ë¸ì´ '{args.merge_save_dir}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# =============================
# 6) CLI
# =============================

def build_parser():
    p = argparse.ArgumentParser(description="Llama-3.2-3B QLoRA fine-tuning")

    # ë°ì´í„°/ëª¨ë¸
    p.add_argument("--model_name_or_path", type=str, default="meta-llama/Llama-3.2-3B-Instruct",
                   help="ê¸°ë³¸ê°’: meta-llama/Llama-3.2-3B-Instruct")
    p.add_argument("--dataset_name", type=str, default=None, help="ğŸ¤— datasets ì´ë¦„")
    p.add_argument("--train_file", type=str, default=None, help="ë¡œì»¬ JSON/JSONL íŒŒì¼")

    # ì €ì¥/ë¡œê·¸
    p.add_argument("--output_dir", type=str, default="./outputs/llama32-qlora")
    p.add_argument("--logging_steps", type=int, default=10)
    p.add_argument("--save_every_steps", type=int, default=1000)
    p.add_argument("--save_total_limit", type=int, default=3)
    p.add_argument("--report_to_tensorboard", action="store_true")

    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    p.add_argument("--num_train_epochs", type=float, default=5.0)
    p.add_argument("--per_device_train_batch_size", type=int, default=1)
    p.add_argument("--gradient_accumulation_steps", type=int, default=8)
    p.add_argument("--learning_rate", type=float, default=2e-4)
    p.add_argument("--lr_scheduler_type", type=str, default="cosine")
    p.add_argument("--warmup_ratio", type=float, default=0.03)
    p.add_argument("--max_seq_length", type=int, default=1024)
    p.add_argument("--packing", action="store_true")

    # ë°ì´í„° ì‚¬ìš© ë¹„ìœ¨ ë° ì‹œë“œ
    p.add_argument("--train_fraction", type=float, default=0.3, help="0~1 ì‚¬ì´ ë¹„ìœ¨, ì˜ˆ: 0.3ëŠ” 30% ì‚¬ìš©")
    p.add_argument("--seed", type=int, default=42)

    # QLoRA/bitsandbytes
    p.add_argument("--bnb_4bit", type=bool, default=True)
    p.add_argument("--bnb_4bit_quant_type", type=str, default="nf4", choices=["nf4", "fp4"]) 
    p.add_argument("--bnb_4bit_compute_dtype", type=str, default="bfloat16", choices=["bfloat16", "float16"]) 
    p.add_argument("--bf16", type=bool, default=True)
    p.add_argument("--gradient_checkpointing", type=bool, default=True)

    # LoRA ì„¤ì •
    p.add_argument("--lora_r", type=int, default=64)
    p.add_argument("--lora_alpha", type=int, default=32)
    p.add_argument("--lora_dropout", type=float, default=0.05)
    p.add_argument("--target_modules", type=str, default="", help="ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ëª¨ë“ˆ ëª©ë¡(Qwenì€ ë³´í†µ ìë™íƒì§€)")

    # ë³‘í•© ì „ìš©
    p.add_argument("--merge_adapters", action="store_true", help="LoRA ë³‘í•©ë§Œ ìˆ˜í–‰")
    p.add_argument("--adapter_path", type=str, default=None)
    p.add_argument("--merge_save_dir", type=str, default="./outputs/llama32-merged")

    return p


def main():
    args = build_parser().parse_args()

    if args.merge_adapters:
        merge_adapters(args)
    else:
        train(args)


if __name__ == "__main__":
    main()
