# ğŸ¯ ì´ë¡ ì ìœ¼ë¡œ ì™„ë²½í•œ Easy í”Œë¡œìš° ì‹œìŠ¤í…œ
# ëª¨ë“  í—ˆì ì„ ì œê±°í•œ ì™„ì „í•œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

import re
import json
from typing import Dict, List, Tuple, Optional
from pathlib import Path

class PerfectEasyFlow:
    """
    ğŸš€ ì´ë¡ ì ìœ¼ë¡œ ì™„ë²½í•œ Easy ëª¨ë¸ í”Œë¡œìš°
    - ì´ì¤‘ ë²ˆì—­ ë°©ì§€
    - í’ˆì§ˆ ê²€ì¦ ê°•í™”
    - ìºì‹œ ì „ëµ ìµœì í™”
    - ìë™ ìš©ì–´ì‚¬ì „ ì™„ì „ì„± ë³´ì¥
    """
    
    def __init__(self):
        self.quality_threshold = 0.85
        self.max_glossary_terms = 50  # 10ê°œ â†’ 50ê°œë¡œ í™•ì¥
        self.translation_history = {}  # ì´ì¤‘ ë²ˆì—­ ë°©ì§€
        
    def perfect_rewrite_flow(self, content: str, title: str = None) -> Dict:
        """
        ğŸ¯ ì™„ë²½í•œ ë¦¬ë¼ì´íŠ¸ í”Œë¡œìš° (ì´ì¤‘ ë²ˆì—­ ë°©ì§€)
        """
        result = {
            "original_content": content,
            "rewritten_content": "",
            "quality_score": 0.0,
            "is_cache_worthy": False,
            "translation_verified": False,
            "glossary_updated": False,
            "issues": []
        }
        
        # 1ë‹¨ê³„: LLM ì™„ì „ ë²ˆì—­ (í•œ ë²ˆë§Œ!)
        llm_result = self._llm_complete_translation(content, title)
        if not llm_result:
            result["issues"].append("LLM ë²ˆì—­ ì‹¤íŒ¨")
            return result
        
        # 2ë‹¨ê³„: ì´ì¤‘ ë²ˆì—­ ë°©ì§€ ê²€ì¦
        if self._check_double_translation(content, llm_result):
            result["issues"].append("ì´ì¤‘ ë²ˆì—­ ê°ì§€ - ì¬ìƒì„± í•„ìš”")
            return result
        
        # 3ë‹¨ê³„: ìë™ ìš©ì–´ì‚¬ì „ ì—…ë°ì´íŠ¸ (ì™„ì „ì„± ë³´ì¥)
        updated_content = self._complete_glossary_update(llm_result)
        
        # 4ë‹¨ê³„: ìš©ì–´ ì£¼ì„ ì¶”ê°€
        annotated_content = self._add_term_annotations(updated_content)
        
        # 5ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦ (ì§€ëŠ¥ì )
        quality_assessment = self._intelligent_quality_check(annotated_content, content)
        
        result.update({
            "rewritten_content": annotated_content,
            "quality_score": quality_assessment["quality_score"],
            "is_cache_worthy": quality_assessment["is_cache_worthy"],
            "translation_verified": True,
            "glossary_updated": True
        })
        
        return result
    
    def _llm_complete_translation(self, content: str, title: str = None) -> str:
        """
        ğŸ§  LLM ì™„ì „ ë²ˆì—­ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        """
        # ì´ì¤‘ ë²ˆì—­ ë°©ì§€ë¥¼ ìœ„í•œ í•´ì‹œ ì²´í¬
        content_hash = hash(content)
        if content_hash in self.translation_history:
            return self.translation_history[content_hash]
        
        # LLM ë²ˆì—­ ì‹¤í–‰ (ê¸°ì¡´ _rewrite_text ë¡œì§)
        # ... (ì‹¤ì œ LLM í˜¸ì¶œ ì½”ë“œ)
        
        # ê²°ê³¼ ì €ì¥ (ì´ì¤‘ ë²ˆì—­ ë°©ì§€)
        self.translation_history[content_hash] = "translated_content"
        return "translated_content"
    
    def _check_double_translation(self, original: str, translated: str) -> bool:
        """
        ğŸ” ì´ì¤‘ ë²ˆì—­ ê°ì§€ ì‹œìŠ¤í…œ
        """
        # ì˜ì–´ ë¬¸ì¥ì´ ì—¬ì „íˆ ë§ì´ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸
        english_sentences = len(re.findall(r'[A-Z][^.]*[.!?]', translated))
        total_sentences = len([s for s in translated.split('.') if s.strip()])
        
        if total_sentences == 0:
            return False
        
        english_ratio = english_sentences / total_sentences
        
        # 30% ì´ìƒ ì˜ì–´ ë¬¸ì¥ì´ ë‚¨ì•„ìˆìœ¼ë©´ ì´ì¤‘ ë²ˆì—­ ì˜ì‹¬
        if english_ratio > 0.3:
            return True
        
        # ì›ë¬¸ê³¼ ë„ˆë¬´ ìœ ì‚¬í•œ ê²½ìš° (ë²ˆì—­ì´ ì•ˆ ëœ ê²½ìš°)
        similarity = self._calculate_similarity(original, translated)
        if similarity > 0.8:
            return True
        
        return False
    
    def _complete_glossary_update(self, content: str) -> str:
        """
        ğŸ“š ì™„ì „í•œ ìš©ì–´ì‚¬ì „ ì—…ë°ì´íŠ¸ (10ê°œ ì œí•œ í•´ì œ)
        """
        # ëª¨ë“  ìƒˆë¡œìš´ ìš©ì–´ ê°ì§€ (ì œí•œ ì—†ìŒ)
        new_terms = self._detect_all_new_terms(content)
        
        # ìš©ì–´ì‚¬ì „ ì—…ë°ì´íŠ¸ (ë°°ì¹˜ ì²˜ë¦¬)
        updated_glossary = self._batch_update_glossary(new_terms)
        
        return content
    
    def _detect_all_new_terms(self, content: str) -> List[str]:
        """
        ğŸ” ëª¨ë“  ìƒˆë¡œìš´ ìš©ì–´ ê°ì§€ (ì œí•œ ì—†ìŒ)
        """
        # ê¸°ì¡´ ìš©ì–´ì‚¬ì „ ë¡œë“œ
        current_glossary = self._load_glossary()
        
        # ëª¨ë“  ê¸°ìˆ  ìš©ì–´ íŒ¨í„´ ì ìš©
        tech_patterns = [
            r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b',  # ëŒ€ë¬¸ì ì‹œì‘ ìš©ì–´
            r'\b([A-Z]{2,})\b',  # ì•½ì–´
            r'\b([a-z]+(?:\s+[a-z]+)*\s+(?:detection|network|learning|vision|model|algorithm|method|approach|system|framework|architecture|layer|function|loss|error|accuracy|precision|recall|score|rate|ratio|threshold|parameter|feature|data|dataset|training|testing|validation|optimization|regularization))\b',
            r'\b([a-z]+(?:\s+[a-z]+)*(?:-[a-z]+)*)\s+(?:box|cell|grid|window|proposal|region|patch|kernel|filter|pooling)\b'
        ]
        
        new_terms = set()
        for pattern in tech_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if match.lower() not in current_glossary and len(match) > 3:
                    new_terms.add(match)
        
        return list(new_terms)
    
    def _batch_update_glossary(self, new_terms: List[str]) -> Dict:
        """
        ğŸ“ ë°°ì¹˜ ìš©ì–´ì‚¬ì „ ì—…ë°ì´íŠ¸ (íš¨ìœ¨ì„±)
        """
        current_glossary = self._load_glossary()
        
        # ë°°ì¹˜ë¡œ í•œêµ­ì–´ ì„¤ëª… ìƒì„±
        for term in new_terms[:self.max_glossary_terms]:  # 50ê°œê¹Œì§€ ì²˜ë¦¬
            korean_explanation = self._generate_korean_explanation(term)
            if korean_explanation:
                current_glossary[term] = korean_explanation
        
        # ìš©ì–´ì‚¬ì „ ì €ì¥
        self._save_glossary(current_glossary)
        
        return current_glossary
    
    def _intelligent_quality_check(self, translated: str, original: str) -> Dict:
        """
        ğŸ§  ì§€ëŠ¥ì  í’ˆì§ˆ ê²€ì¦
        """
        assessment = {
            "quality_score": 0.0,
            "is_cache_worthy": False,
            "issues": []
        }
        
        # 1. ì˜ë¯¸ì  ì™„ì „ì„± (40%)
        semantic_score = self._check_semantic_completeness(translated, original)
        
        # 2. ì´í•´ë„ (30%)
        understandability_score = self._check_understandability(translated)
        
        # 3. ìì—°ìŠ¤ëŸ¬ì›€ (20%)
        naturalness_score = self._check_naturalness(translated)
        
        # 4. ì „ë¬¸ì„± (10%)
        expertise_score = self._check_technical_expertise(translated)
        
        # ì¢…í•© ì ìˆ˜
        overall_score = (semantic_score * 0.4 + understandability_score * 0.3 + 
                        naturalness_score * 0.2 + expertise_score * 0.1)
        
        assessment["quality_score"] = overall_score
        assessment["is_cache_worthy"] = overall_score >= self.quality_threshold
        
        if not assessment["is_cache_worthy"]:
            assessment["issues"].append(f"í’ˆì§ˆ ì ìˆ˜ ë¶€ì¡±: {overall_score:.2f} < {self.quality_threshold}")
        
        return assessment
    
    def perfect_cache_strategy(self, sections: List[Dict], quality_scores: List[float]) -> Dict:
        """
        ğŸ¯ ì™„ë²½í•œ ìºì‹œ ì „ëµ
        """
        strategy = {
            "should_cache": False,
            "cache_quality": "unknown",
            "recommendations": [],
            "confidence": 0.0
        }
        
        if not quality_scores:
            return strategy
        
        # í’ˆì§ˆ ë¶„í¬ ë¶„ì„
        excellent_count = sum(1 for score in quality_scores if score >= 0.9)
        good_count = sum(1 for score in quality_scores if score >= 0.8)
        fair_count = sum(1 for score in quality_scores if score >= 0.6)
        poor_count = sum(1 for score in quality_scores if score < 0.6)
        
        total_count = len(quality_scores)
        
        # ì§€ëŠ¥ì  ìºì‹œ ê²°ì •
        if excellent_count / total_count >= 0.8:
            strategy.update({
                "should_cache": True,
                "cache_quality": "excellent",
                "confidence": 0.95
            })
            strategy["recommendations"].append("âœ… ì™„ë²½í•œ í’ˆì§ˆ - ì „ì²´ ìºì‹œ ì €ì¥")
        elif (excellent_count + good_count) / total_count >= 0.8:
            strategy.update({
                "should_cache": True,
                "cache_quality": "good",
                "confidence": 0.85
            })
            strategy["recommendations"].append("âœ… ì–‘í˜¸í•œ í’ˆì§ˆ - ì „ì²´ ìºì‹œ ì €ì¥")
        elif (excellent_count + good_count) / total_count >= 0.6:
            strategy.update({
                "should_cache": True,
                "cache_quality": "mixed",
                "confidence": 0.70
            })
            strategy["recommendations"].append("âš ï¸ í˜¼ì¬ëœ í’ˆì§ˆ - ë¶€ë¶„ ìºì‹œ ì €ì¥")
        else:
            strategy.update({
                "should_cache": False,
                "cache_quality": "poor",
                "confidence": 0.60
            })
            strategy["recommendations"].append("âŒ ë‚®ì€ í’ˆì§ˆ - ì „ì²´ ì¬ìƒì„± í•„ìš”")
        
        return strategy
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def _load_glossary(self) -> Dict:
        """ìš©ì–´ì‚¬ì „ ë¡œë“œ"""
        # ì‹¤ì œ êµ¬í˜„
        return {}
    
    def _save_glossary(self, glossary: Dict):
        """ìš©ì–´ì‚¬ì „ ì €ì¥"""
        # ì‹¤ì œ êµ¬í˜„
        pass
    
    def _generate_korean_explanation(self, term: str) -> str:
        """í•œêµ­ì–´ ì„¤ëª… ìƒì„±"""
        # ì‹¤ì œ LLM í˜¸ì¶œ
        return f"{term}ì˜ í•œêµ­ì–´ ì„¤ëª…"
    
    def _check_semantic_completeness(self, translated: str, original: str) -> float:
        """ì˜ë¯¸ì  ì™„ì „ì„± ê²€ì‚¬"""
        # ì‹¤ì œ êµ¬í˜„
        return 0.8
    
    def _check_understandability(self, text: str) -> float:
        """ì´í•´ë„ ê²€ì‚¬"""
        # ì‹¤ì œ êµ¬í˜„
        return 0.8
    
    def _check_naturalness(self, text: str) -> float:
        """ìì—°ìŠ¤ëŸ¬ì›€ ê²€ì‚¬"""
        # ì‹¤ì œ êµ¬í˜„
        return 0.8
    
    def _check_technical_expertise(self, text: str) -> float:
        """ì „ë¬¸ì„± ê²€ì‚¬"""
        # ì‹¤ì œ êµ¬í˜„
        return 0.8
    
    def _add_term_annotations(self, content: str) -> str:
        """ìš©ì–´ ì£¼ì„ ì¶”ê°€"""
        # ì‹¤ì œ êµ¬í˜„
        return content

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    flow = PerfectEasyFlow()
    
    # ì™„ë²½í•œ í”Œë¡œìš° í…ŒìŠ¤íŠ¸
    result = flow.perfect_rewrite_flow(
        "YOLO uses a single neural network to predict bounding boxes and class probabilities.",
        "Introduction"
    )
    
    print("=== ì™„ë²½í•œ Easy í”Œë¡œìš° ê²°ê³¼ ===")
    print(f"í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.2f}")
    print(f"ìºì‹œ ì €ì¥ ê°€ëŠ¥: {result['is_cache_worthy']}")
    print(f"ë²ˆì—­ ê²€ì¦: {result['translation_verified']}")
    print(f"ìš©ì–´ì‚¬ì „ ì—…ë°ì´íŠ¸: {result['glossary_updated']}")
    print(f"ì´ìŠˆ: {result['issues']}")
    
    # ì™„ë²½í•œ ìºì‹œ ì „ëµ
    sections = [{"title": "Section 1"}, {"title": "Section 2"}]
    quality_scores = [0.9, 0.8]
    
    cache_strategy = flow.perfect_cache_strategy(sections, quality_scores)
    print("\n=== ì™„ë²½í•œ ìºì‹œ ì „ëµ ===")
    print(f"ìºì‹œ ì €ì¥: {cache_strategy['should_cache']}")
    print(f"ìºì‹œ í’ˆì§ˆ: {cache_strategy['cache_quality']}")
    print(f"ì‹ ë¢°ë„: {cache_strategy['confidence']:.2f}")
    print(f"ê¶Œì¥ì‚¬í•­: {cache_strategy['recommendations']}")
