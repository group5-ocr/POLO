# ğŸ§  ì§€ëŠ¥ì  ê²€ì¦ ì‹œìŠ¤í…œ - ì£¼ê´€ì  íŒë‹¨ ê¸°ë°˜
# ìˆ˜ì¹˜ê°€ ì•„ë‹Œ ì˜ë¯¸ì  í’ˆì§ˆ í‰ê°€

import re
import json
from typing import Dict, List, Tuple, Optional
from pathlib import Path

def intelligent_quality_assessment(text: str, original_content: str) -> Dict:
    """
    ğŸ§  ì§€ëŠ¥ì  í’ˆì§ˆ í‰ê°€ - ì£¼ê´€ì  íŒë‹¨ ê¸°ë°˜
    
    Args:
        text: LLMì´ ìƒì„±í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸
        original_content: ì›ë³¸ ì˜ì–´ í…ìŠ¤íŠ¸
        
    Returns:
        Dict: ì§€ëŠ¥ì  í‰ê°€ ê²°ê³¼
    """
    assessment = {
        "is_well_translated": False,
        "translation_quality": "unknown",
        "key_issues": [],
        "strengths": [],
        "needs_regeneration": False,
        "confidence_score": 0.0
    }
    
    if not text or not text.strip():
        assessment["key_issues"].append("ë¹ˆ ë‚´ìš©")
        assessment["needs_regeneration"] = True
        return assessment
    
    # 1. ì˜ë¯¸ì  ì™„ì „ì„± ê²€ì¦ (ê°€ì¥ ì¤‘ìš”)
    completeness_score = _assess_semantic_completeness(text, original_content)
    
    # 2. ì´í•´ë„ ê²€ì¦
    understandability_score = _assess_understandability(text)
    
    # 3. ìì—°ìŠ¤ëŸ¬ì›€ ê²€ì¦
    naturalness_score = _assess_naturalness(text)
    
    # 4. ì „ë¬¸ì„± ê²€ì¦
    expertise_score = _assess_technical_expertise(text)
    
    # ì¢…í•© íŒë‹¨
    overall_score = (completeness_score + understandability_score + 
                    naturalness_score + expertise_score) / 4
    
    assessment["confidence_score"] = overall_score
    
    # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
    if overall_score >= 0.8:
        assessment["translation_quality"] = "excellent"
        assessment["is_well_translated"] = True
        assessment["strengths"].append("ì™„ë²½í•œ ë²ˆì—­")
    elif overall_score >= 0.6:
        assessment["translation_quality"] = "good"
        assessment["is_well_translated"] = True
        assessment["strengths"].append("ì–‘í˜¸í•œ ë²ˆì—­")
    elif overall_score >= 0.4:
        assessment["translation_quality"] = "fair"
        assessment["key_issues"].append("ë¶€ë¶„ì  ê°œì„  í•„ìš”")
    else:
        assessment["translation_quality"] = "poor"
        assessment["needs_regeneration"] = True
        assessment["key_issues"].append("ì „ì²´ ì¬ìƒì„± í•„ìš”")
    
    return assessment

def _assess_semantic_completeness(text: str, original_content: str) -> float:
    """
    ì˜ë¯¸ì  ì™„ì „ì„± ê²€ì¦ - ì›ë¬¸ì˜ í•µì‹¬ ì˜ë¯¸ê°€ ë³´ì¡´ë˜ì—ˆëŠ”ê°€?
    """
    if not original_content:
        return 1.0
    
    # ì›ë¬¸ì˜ í•µì‹¬ ê°œë… ì¶”ì¶œ
    original_concepts = _extract_key_concepts(original_content)
    translated_concepts = _extract_key_concepts(text)
    
    # ê°œë… ë³´ì¡´ë„ ê³„ì‚°
    preserved_concepts = 0
    for concept in original_concepts:
        if _concept_preserved(concept, text):
            preserved_concepts += 1
    
    completeness_ratio = preserved_concepts / max(len(original_concepts), 1)
    
    # ì¶”ê°€ ê²€ì¦: ì›ë¬¸ì˜ ë…¼ë¦¬ì  êµ¬ì¡° ë³´ì¡´
    logical_structure_preserved = _check_logical_structure(original_content, text)
    
    return (completeness_ratio + logical_structure_preserved) / 2

def _extract_key_concepts(text: str) -> List[str]:
    """í•µì‹¬ ê°œë… ì¶”ì¶œ"""
    concepts = []
    
    # ê¸°ìˆ ì  ê°œë…
    tech_concepts = re.findall(r'\b(?:neural|network|detection|bounding|box|class|probability|convolutional|feature|learning|model|algorithm|architecture|framework|system|method|approach|optimization|training|testing|validation|accuracy|precision|recall|score|data|dataset|image|object|vision|computer|deep|machine|artificial|intelligence|pattern|recognition|classification|regression|clustering|segmentation)\b', text, re.IGNORECASE)
    concepts.extend(tech_concepts)
    
    # ìˆ˜ì¹˜ì  ê°œë…
    numerical_concepts = re.findall(r'\b\d+\.?\d*%|\d+\.?\d*fps|\d+\.?\d*mAP|\d+\.?\d*ms\b', text)
    concepts.extend(numerical_concepts)
    
    # ëª¨ë¸ëª…/ë°ì´í„°ì…‹ëª…
    proper_nouns = re.findall(r'\b(?:YOLO|R-CNN|Fast R-CNN|Faster R-CNN|DPM|PASCAL VOC|ImageNet|COCO|Titan X|GPU)\b', text)
    concepts.extend(proper_nouns)
    
    return list(set(concepts))

def _concept_preserved(concept: str, translated_text: str) -> bool:
    """ê°œë…ì´ ë²ˆì—­ì—ì„œ ë³´ì¡´ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    # ì§ì ‘ ë§¤ì¹­
    if concept.lower() in translated_text.lower():
        return True
    
    # í•œêµ­ì–´ ë²ˆì—­ ë§¤ì¹­
    korean_translations = {
        'neural': 'ì‹ ê²½ë§', 'network': 'ë„¤íŠ¸ì›Œí¬', 'detection': 'íƒì§€', 'bounding': 'ê²½ê³„',
        'box': 'ìƒì', 'class': 'í´ë˜ìŠ¤', 'probability': 'í™•ë¥ ', 'convolutional': 'í•©ì„±ê³±',
        'feature': 'íŠ¹ì§•', 'learning': 'í•™ìŠµ', 'model': 'ëª¨ë¸', 'algorithm': 'ì•Œê³ ë¦¬ì¦˜',
        'architecture': 'êµ¬ì¡°', 'framework': 'í”„ë ˆì„ì›Œí¬', 'system': 'ì‹œìŠ¤í…œ',
        'method': 'ë°©ë²•', 'approach': 'ì ‘ê·¼', 'optimization': 'ìµœì í™”',
        'training': 'í›ˆë ¨', 'testing': 'í…ŒìŠ¤íŠ¸', 'validation': 'ê²€ì¦',
        'accuracy': 'ì •í™•ë„', 'precision': 'ì •ë°€ë„', 'recall': 'ì¬í˜„ìœ¨',
        'score': 'ì ìˆ˜', 'data': 'ë°ì´í„°', 'dataset': 'ë°ì´í„°ì…‹',
        'image': 'ì´ë¯¸ì§€', 'object': 'ê°ì²´', 'vision': 'ë¹„ì „',
        'computer': 'ì»´í“¨í„°', 'deep': 'ë”¥', 'machine': 'ë¨¸ì‹ ',
        'artificial': 'ì¸ê³µ', 'intelligence': 'ì§€ëŠ¥', 'pattern': 'íŒ¨í„´',
        'recognition': 'ì¸ì‹', 'classification': 'ë¶„ë¥˜', 'regression': 'íšŒê·€',
        'clustering': 'í´ëŸ¬ìŠ¤í„°ë§', 'segmentation': 'ë¶„í• '
    }
    
    if concept.lower() in korean_translations:
        korean_term = korean_translations[concept.lower()]
        if korean_term in translated_text:
            return True
    
    return False

def _check_logical_structure(original: str, translated: str) -> float:
    """ë…¼ë¦¬ì  êµ¬ì¡° ë³´ì¡´ í™•ì¸"""
    # ì›ë¬¸ì˜ ë¬¸ì¥ ìˆ˜
    original_sentences = len([s for s in original.split('.') if s.strip()])
    translated_sentences = len([s for s in translated.split('.') if s.strip()])
    
    # ë¬¸ì¥ ìˆ˜ ë¹„ìœ¨ì´ ë¹„ìŠ·í•œì§€ í™•ì¸
    if original_sentences == 0:
        return 1.0
    
    sentence_ratio = min(translated_sentences, original_sentences) / max(translated_sentences, original_sentences)
    
    # ë…¼ë¦¬ì  ì—°ê²°ì–´ í™•ì¸
    logical_connectors = ['ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë˜í•œ', 'ë˜í•œ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ê·¸ë¦¬ê³ ', 'ë˜ëŠ”']
    connector_preserved = any(connector in translated for connector in logical_connectors)
    
    return (sentence_ratio + (0.5 if connector_preserved else 0)) / 1.5

def _assess_understandability(text: str) -> float:
    """
    ì´í•´ë„ ê²€ì¦ - ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆëŠ”ê°€?
    """
    # ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡° ê²€ì‚¬
    complex_sentences = len(re.findall(r'[^.!?]*[,;][^.!?]*[,;][^.!?]*[.!?]', text))
    total_sentences = len([s for s in text.split('.') if s.strip()])
    
    if total_sentences == 0:
        return 0.0
    
    complexity_ratio = complex_sentences / total_sentences
    
    # ì„¤ëª…ì´ í¬í•¨ëœ ìš©ì–´ ë¹„ìœ¨
    explained_terms = len(re.findall(r'\b\w+\([^)]+\)', text))
    total_terms = len(re.findall(r'\b(?:neural|network|detection|bounding|box|class|probability|convolutional|feature|learning|model|algorithm|architecture|framework|system|method|approach|optimization|training|testing|validation|accuracy|precision|recall|score|data|dataset|image|object|vision|computer|deep|machine|artificial|intelligence|pattern|recognition|classification|regression|clustering|segmentation)\b', text, re.IGNORECASE))
    
    explanation_ratio = explained_terms / max(total_terms, 1)
    
    # ì´í•´ë„ ì ìˆ˜ ê³„ì‚°
    understandability_score = 1.0 - complexity_ratio + explanation_ratio * 0.5
    
    return min(understandability_score, 1.0)

def _assess_naturalness(text: str) -> float:
    """
    ìì—°ìŠ¤ëŸ¬ì›€ ê²€ì¦ - í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ê°€?
    """
    # í•œêµ­ì–´ ë¹„ìœ¨
    hangul_chars = len(re.findall(r'[ê°€-í£]', text))
    total_chars = len(re.sub(r'\s', '', text))
    hangul_ratio = hangul_chars / max(total_chars, 1)
    
    # ì˜ì–´ ë¬¸ì¥ ê²€ì‚¬
    english_sentences = len(re.findall(r'[A-Z][^.]*[.!?]', text))
    total_sentences = len([s for s in text.split('.') if s.strip()])
    english_ratio = english_sentences / max(total_sentences, 1)
    
    # ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„ ê²€ì‚¬
    natural_korean_patterns = [
        r'[ê°€-í£]+ì€\s+[ê°€-í£]+',  # "YOLOëŠ” ë‹¨ì¼"
        r'[ê°€-í£]+ì„\s+[ê°€-í£]+',  # "ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬"
        r'[ê°€-í£]+ì—ì„œ\s+[ê°€-í£]+',  # "ì´ë¯¸ì§€ì—ì„œ"
        r'[ê°€-í£]+ë¡œ\s+[ê°€-í£]+',  # "ì§ì ‘ë¡œ ì˜ˆì¸¡"
    ]
    
    natural_patterns = sum(len(re.findall(pattern, text)) for pattern in natural_korean_patterns)
    
    # ìì—°ìŠ¤ëŸ¬ì›€ ì ìˆ˜ ê³„ì‚°
    naturalness_score = (hangul_ratio * 0.4 + 
                        (1 - english_ratio) * 0.3 + 
                        min(natural_patterns / max(total_sentences, 1), 1.0) * 0.3)
    
    return min(naturalness_score, 1.0)

def _assess_technical_expertise(text: str) -> float:
    """
    ì „ë¬¸ì„± ê²€ì¦ - ê¸°ìˆ ì  ì •í™•ì„±ì´ ìˆëŠ”ê°€?
    """
    # ê¸°ìˆ  ìš©ì–´ì˜ ì •í™•ì„±
    tech_terms = re.findall(r'\b(?:neural|network|detection|bounding|box|class|probability|convolutional|feature|learning|model|algorithm|architecture|framework|system|method|approach|optimization|training|testing|validation|accuracy|precision|recall|score|data|dataset|image|object|vision|computer|deep|machine|artificial|intelligence|pattern|recognition|classification|regression|clustering|segmentation)\b', text, re.IGNORECASE)
    
    # ì„¤ëª…ì´ í¬í•¨ëœ ìš©ì–´
    explained_terms = len(re.findall(r'\b\w+\([^)]+\)', text))
    
    # ì „ë¬¸ì„± ì ìˆ˜ ê³„ì‚°
    if len(tech_terms) == 0:
        return 0.0
    
    expertise_score = explained_terms / len(tech_terms)
    
    # ì¶”ê°€: ìˆ˜ì¹˜ì˜ ì •í™•ì„±
    numerical_accuracy = _check_numerical_accuracy(text)
    
    return (expertise_score + numerical_accuracy) / 2

def _check_numerical_accuracy(text: str) -> float:
    """ìˆ˜ì¹˜ ì •í™•ì„± ê²€ì‚¬"""
    # ìˆ˜ì¹˜ê°€ í¬í•¨ëœ ë¬¸ì¥ì—ì„œ ì •í™•ì„± í™•ì¸
    numerical_sentences = re.findall(r'[^.!?]*\d+[^.!?]*[.!?]', text)
    
    if not numerical_sentences:
        return 1.0
    
    # ìˆ˜ì¹˜ê°€ ì˜¬ë°”ë¥´ê²Œ ë²ˆì—­ë˜ì—ˆëŠ”ì§€ í™•ì¸
    accurate_sentences = 0
    for sentence in numerical_sentences:
        # ìˆ˜ì¹˜ê°€ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if re.search(r'\d+[ê°€-í£]+', sentence) or re.search(r'[ê°€-í£]+\d+', sentence):
            accurate_sentences += 1
    
    return accurate_sentences / len(numerical_sentences)

def smart_cache_decision(assessments: List[Dict]) -> Dict:
    """
    ğŸ§  ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê²°ì • - ì§€ëŠ¥ì  íŒë‹¨ ê¸°ë°˜
    """
    decision = {
        "should_cache": False,
        "cache_quality": "unknown",
        "recommendations": [],
        "confidence": 0.0
    }
    
    if not assessments:
        return decision
    
    # ì „ì²´ í’ˆì§ˆ í‰ê°€
    excellent_count = sum(1 for a in assessments if a["translation_quality"] == "excellent")
    good_count = sum(1 for a in assessments if a["translation_quality"] == "good")
    fair_count = sum(1 for a in assessments if a["translation_quality"] == "fair")
    poor_count = sum(1 for a in assessments if a["translation_quality"] == "poor")
    
    total_count = len(assessments)
    
    # í’ˆì§ˆ ë¶„í¬
    excellent_ratio = excellent_count / total_count
    good_ratio = good_count / total_count
    fair_ratio = fair_count / total_count
    poor_ratio = poor_count / total_count
    
    # ìºì‹œ ê²°ì • ë¡œì§
    if excellent_ratio >= 0.8:
        decision["should_cache"] = True
        decision["cache_quality"] = "excellent"
        decision["recommendations"].append("âœ… ì™„ë²½í•œ í’ˆì§ˆ - ì „ì²´ ìºì‹œ ì €ì¥ ê¶Œì¥")
    elif excellent_ratio + good_ratio >= 0.8:
        decision["should_cache"] = True
        decision["cache_quality"] = "good"
        decision["recommendations"].append("âœ… ì–‘í˜¸í•œ í’ˆì§ˆ - ì „ì²´ ìºì‹œ ì €ì¥ ê¶Œì¥")
    elif excellent_ratio + good_ratio >= 0.6:
        decision["should_cache"] = True
        decision["cache_quality"] = "mixed"
        decision["recommendations"].append("âš ï¸ í˜¼ì¬ëœ í’ˆì§ˆ - ë¶€ë¶„ ìºì‹œ ì €ì¥ (ì €í’ˆì§ˆ ì„¹ì…˜ ì¬ìƒì„±)")
    else:
        decision["should_cache"] = False
        decision["cache_quality"] = "poor"
        decision["recommendations"].append("âŒ ë‚®ì€ í’ˆì§ˆ - ì „ì²´ ì¬ìƒì„± í•„ìš”")
    
    # ì‹ ë¢°ë„ ê³„ì‚°
    decision["confidence"] = (excellent_ratio * 1.0 + good_ratio * 0.8 + 
                            fair_ratio * 0.5 + poor_ratio * 0.2)
    
    # ì¶”ê°€ ê¶Œì¥ì‚¬í•­
    if poor_count > 0:
        decision["recommendations"].append(f"ğŸ”„ {poor_count}ê°œ ì„¹ì…˜ ì¬ìƒì„± í•„ìš”")
    
    if fair_count > 0:
        decision["recommendations"].append(f"ğŸ”§ {fair_count}ê°œ ì„¹ì…˜ ë¶€ë¶„ ê°œì„  í•„ìš”")
    
    return decision

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_text = """
    YOLOëŠ” ë‹¨ì¼ neural network(ë‡Œ êµ¬ì¡°ë¥¼ ë³¸ëœ¬ ì¸ê³µ ì‹ ê²½ë§)ë¥¼ ì‚¬ìš©í•˜ì—¬ 
    bounding box(ë¬¼ì²´ë¥¼ ë‘˜ëŸ¬ì‹¸ëŠ” ë„¤ëª¨ ìƒì)ì™€ class probability(ê° ë¬¼ì²´ê°€ íŠ¹ì • í´ë˜ìŠ¤ì¼ í™•ë¥ )ë¥¼ 
    ì§ì ‘ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ í†µí•© ëª¨ë¸ì€ ì „í†µì ì¸ object detection(ì´ë¯¸ì§€ ì†ì—ì„œ ë¬¼ì²´ì˜ ìœ„ì¹˜ì™€ ì¢…ë¥˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ì‘ì—…) 
    ë°©ë²•ë“¤ì— ë¹„í•´ ì—¬ëŸ¬ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.
    """
    
    original_content = """
    YOLO uses a single neural network to predict bounding boxes and class probabilities 
    directly from full images in one evaluation. This unified model has several advantages 
    over traditional object detection methods.
    """
    
    # ì§€ëŠ¥ì  í‰ê°€ ì‹¤í–‰
    assessment = intelligent_quality_assessment(test_text, original_content)
    print("=== ì§€ëŠ¥ì  í’ˆì§ˆ í‰ê°€ ===")
    print(f"ë²ˆì—­ í’ˆì§ˆ: {assessment['translation_quality']}")
    print(f"ì˜ ë²ˆì—­ë¨: {assessment['is_well_translated']}")
    print(f"ì‹ ë¢°ë„: {assessment['confidence_score']:.2f}")
    print(f"ê°•ì : {assessment['strengths']}")
    print(f"ì´ìŠˆ: {assessment['key_issues']}")
    
    # ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê²°ì •
    assessments = [assessment, assessment, assessment]  # 3ê°œ ì„¹ì…˜ ê°€ì •
    decision = smart_cache_decision(assessments)
    print("\n=== ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê²°ì • ===")
    print(f"ìºì‹œ ì €ì¥: {decision['should_cache']}")
    print(f"ìºì‹œ í’ˆì§ˆ: {decision['cache_quality']}")
    print(f"ì‹ ë¢°ë„: {decision['confidence']:.2f}")
    print(f"ê¶Œì¥ì‚¬í•­: {decision['recommendations']}")
