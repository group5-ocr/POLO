# ğŸ¯ ì´ë¡ ì ìœ¼ë¡œ ì™„ë²½í•œ ìºì‹œ ì €ì¥ ë¡œì§
# Llama ìì²´ê²€ì¦ì„ í†µí•œ í’ˆì§ˆ ë³´ì¥ ì‹œìŠ¤í…œ

import re
import json
from typing import Dict, List, Tuple, Optional
from pathlib import Path

# í’ˆì§ˆ ê¸°ì¤€ ìƒìˆ˜
CACHE_QUALITY_THRESHOLD = 0.85  # 85% ì´ìƒ í’ˆì§ˆ ì ìˆ˜
MIN_TECHNICAL_TERMS = 3  # ìµœì†Œ ê¸°ìˆ  ìš©ì–´ ìˆ˜
MIN_EXPLANATION_RATIO = 0.6  # 60% ì´ìƒ ìš©ì–´ì— ì„¤ëª… ì¶”ê°€
MAX_REGENERATION_ATTEMPTS = 3  # ìµœëŒ€ ì¬ìƒì„± ì‹œë„ íšŸìˆ˜

def llama_self_validation(text: str, original_content: str) -> Dict:
    """
    ğŸ¯ Llama ìì²´ê²€ì¦ ì‹œìŠ¤í…œ - í’ˆì§ˆì´ ì¢‹ì€ ê²°ê³¼ë§Œ ìºì‹œì— ì €ì¥
    
    Args:
        text: LLMì´ ìƒì„±í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸
        original_content: ì›ë³¸ ì˜ì–´ í…ìŠ¤íŠ¸
        
    Returns:
        Dict: ê²€ì¦ ê²°ê³¼ (í’ˆì§ˆ ì ìˆ˜, ìºì‹œ ì €ì¥ ê°€ëŠ¥ ì—¬ë¶€, ì´ìŠˆ ëª©ë¡)
    """
    validation_result = {
        "quality_score": 0.0,
        "is_cache_worthy": False,
        "issues": [],
        "technical_terms_count": 0,
        "explanation_ratio": 0.0,
        "regeneration_needed": False
    }
    
    if not text or not text.strip():
        validation_result["issues"].append("ë¹ˆ ë‚´ìš©")
        validation_result["regeneration_needed"] = True
        return validation_result
    
    # 1. ê¸°ìˆ  ìš©ì–´ ê°ì§€ ë° ì„¤ëª… ë¹„ìœ¨ ê³„ì‚°
    tech_terms_with_explanation = len(re.findall(r'\b\w+\([^)]+\)', text))
    
    # í•µì‹¬ ê¸°ìˆ  ìš©ì–´ íŒ¨í„´ (YOLO ë…¼ë¬¸ íŠ¹í™”)
    core_tech_terms = [
        'neural', 'network', 'detection', 'bounding', 'box', 'class', 'probability',
        'convolutional', 'feature', 'learning', 'model', 'algorithm', 'architecture',
        'framework', 'system', 'method', 'approach', 'optimization', 'training',
        'testing', 'validation', 'accuracy', 'precision', 'recall', 'score',
        'data', 'dataset', 'image', 'object', 'vision', 'computer', 'deep',
        'machine', 'artificial', 'intelligence', 'pattern', 'recognition',
        'classification', 'regression', 'clustering', 'segmentation', 'localization',
        'tracking', 'monitoring', 'surveillance', 'autonomous', 'robotic', 'automated',
        'intelligent', 'smart', 'adaptive', 'dynamic', 'real-time', 'online', 'offline',
        'batch', 'stream', 'processing', 'analysis', 'synthesis', 'generation'
    ]
    
    total_tech_terms = 0
    for term in core_tech_terms:
        total_tech_terms += len(re.findall(rf'\b{term}\b', text, re.IGNORECASE))
    
    validation_result["technical_terms_count"] = total_tech_terms
    validation_result["explanation_ratio"] = tech_terms_with_explanation / max(total_tech_terms, 1)
    
    # 2. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-1)
    quality_score = 0.0
    
    # ê¸°ë³¸ ê²€ì¦ (40%) - ê¸¸ì´, í•œê¸€ ë¹„ìœ¨, ë¬¸ì¥ ìˆ˜
    if _basic_quality_check(text):
        quality_score += 0.4
    
    # ê¸°ìˆ  ìš©ì–´ í’ë¶€ë„ (20%)
    if total_tech_terms >= MIN_TECHNICAL_TERMS:
        quality_score += 0.2
    
    # ì„¤ëª… ë¹„ìœ¨ (20%)
    if validation_result["explanation_ratio"] >= MIN_EXPLANATION_RATIO:
        quality_score += 0.2
    
    # ì™„ì „ì„± ê²€ì¦ (10%) - ì›ë¬¸ ë‚´ìš© ë³´ì¡´
    if _completeness_check(text, original_content):
        quality_score += 0.1
    
    # ìì—°ìŠ¤ëŸ¬ì›€ ê²€ì¦ (10%) - í•œêµ­ì–´ ë¬¸ì²´
    if _naturalness_check(text):
        quality_score += 0.1
    
    validation_result["quality_score"] = quality_score
    validation_result["is_cache_worthy"] = quality_score >= CACHE_QUALITY_THRESHOLD
    
    # ì´ìŠˆ ë¶„ì„
    if not validation_result["is_cache_worthy"]:
        validation_result["regeneration_needed"] = True
        validation_result["issues"].append(f"í’ˆì§ˆ ì ìˆ˜ ë¶€ì¡±: {quality_score:.2f} < {CACHE_QUALITY_THRESHOLD}")
        
        if total_tech_terms < MIN_TECHNICAL_TERMS:
            validation_result["issues"].append(f"ê¸°ìˆ  ìš©ì–´ ë¶€ì¡±: {total_tech_terms} < {MIN_TECHNICAL_TERMS}")
        
        if validation_result["explanation_ratio"] < MIN_EXPLANATION_RATIO:
            validation_result["issues"].append(f"ì„¤ëª… ë¹„ìœ¨ ë¶€ì¡±: {validation_result['explanation_ratio']:.2f} < {MIN_EXPLANATION_RATIO}")
    
    return validation_result

def _basic_quality_check(text: str) -> bool:
    """ê¸°ë³¸ í’ˆì§ˆ ê²€ì¦ (ê¸¸ì´, í•œê¸€ ë¹„ìœ¨, ë¬¸ì¥ ìˆ˜)"""
    if not text or len(text.strip()) < 300:
        return False
    
    # í•œê¸€ ë¹„ìœ¨ ê²€ì‚¬
    hangul_chars = len(re.findall(r'[ê°€-í£]', text))
    total_chars = len(re.sub(r'\s', '', text))
    hangul_ratio = hangul_chars / max(total_chars, 1)
    
    if hangul_ratio < 0.7:
        return False
    
    # ë¬¸ì¥ ìˆ˜ ê²€ì‚¬
    sentences = [s.strip() for s in text.split('.') if s.strip()]
    if len(sentences) < 3 or len(sentences) > 6:
        return False
    
    return True

def _completeness_check(text: str, original_content: str) -> bool:
    """ì™„ì „ì„± ê²€ì¦ - ì›ë¬¸ ë‚´ìš© ë³´ì¡´"""
    if not original_content:
        return True
    
    # ì›ë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ ë²ˆì—­ì— í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
    original_keywords = re.findall(r'\b[A-Z][a-z]+\b', original_content)
    translated_keywords = re.findall(r'\b[A-Z][a-z]+\b', text)
    
    # 70% ì´ìƒì˜ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ì•¼ í•¨
    if original_keywords:
        overlap_ratio = len(set(original_keywords) & set(translated_keywords)) / len(set(original_keywords))
        return overlap_ratio >= 0.7
    
    return True

def _naturalness_check(text: str) -> bool:
    """ìì—°ìŠ¤ëŸ¬ì›€ ê²€ì¦ - í•œêµ­ì–´ ë¬¸ì²´"""
    # ê¸ˆì§€ í† í° ê²€ì‚¬
    forbidden_tokens = ["assistant", ".replace(", "```", "[REWRITE", "==", "VOC 20012"]
    for token in forbidden_tokens:
        if token in text:
            return False
    
    # ì˜ì–´ ë¬¸ì¥ì´ ë„ˆë¬´ ë§ì€ì§€ í™•ì¸
    english_sentences = len(re.findall(r'[A-Z][^.]*[.!?]', text))
    total_sentences = len(re.findall(r'[.!?]', text))
    
    if total_sentences > 0 and english_sentences / total_sentences > 0.3:
        return False
    
    return True

def smart_cache_strategy(paper_id: str, sections: List[Dict], 
                        quality_scores: List[float]) -> Dict:
    """
    ğŸ§  ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì „ëµ - í’ˆì§ˆ ê¸°ë°˜ ì €ì¥ ê²°ì •
    
    Args:
        paper_id: ë…¼ë¬¸ ID
        sections: ì„¹ì…˜ ëª©ë¡
        quality_scores: ê° ì„¹ì…˜ì˜ í’ˆì§ˆ ì ìˆ˜
        
    Returns:
        Dict: ìºì‹œ ì „ëµ ê²°ê³¼
    """
    strategy_result = {
        "should_cache": False,
        "cache_ratio": 0.0,
        "high_quality_sections": [],
        "low_quality_sections": [],
        "recommendations": []
    }
    
    if not quality_scores:
        return strategy_result
    
    # í’ˆì§ˆ ì ìˆ˜ ë¶„ì„
    avg_quality = sum(quality_scores) / len(quality_scores)
    high_quality_count = sum(1 for score in quality_scores if score >= CACHE_QUALITY_THRESHOLD)
    cache_ratio = high_quality_count / len(quality_scores)
    
    strategy_result["cache_ratio"] = cache_ratio
    
    # ì„¹ì…˜ë³„ í’ˆì§ˆ ë¶„ë¥˜
    for i, (section, score) in enumerate(zip(sections, quality_scores)):
        if score >= CACHE_QUALITY_THRESHOLD:
            strategy_result["high_quality_sections"].append({
                "index": i,
                "title": section.get("title", f"Section {i+1}"),
                "quality_score": score
            })
        else:
            strategy_result["low_quality_sections"].append({
                "index": i,
                "title": section.get("title", f"Section {i+1}"),
                "quality_score": score
            })
    
    # ìºì‹œ ì €ì¥ ê²°ì •
    if cache_ratio >= 0.8:  # 80% ì´ìƒ ê³ í’ˆì§ˆ
        strategy_result["should_cache"] = True
        strategy_result["recommendations"].append("âœ… ì „ì²´ ìºì‹œ ì €ì¥ ê¶Œì¥")
    elif cache_ratio >= 0.6:  # 60% ì´ìƒ ê³ í’ˆì§ˆ
        strategy_result["should_cache"] = True
        strategy_result["recommendations"].append("âš ï¸ ë¶€ë¶„ ìºì‹œ ì €ì¥ (ì €í’ˆì§ˆ ì„¹ì…˜ ì¬ìƒì„± í•„ìš”)")
    else:  # 60% ë¯¸ë§Œ
        strategy_result["should_cache"] = False
        strategy_result["recommendations"].append("âŒ ìºì‹œ ì €ì¥ ë¹„ê¶Œì¥ (ì „ì²´ ì¬ìƒì„± í•„ìš”)")
    
    # ì¶”ê°€ ê¶Œì¥ì‚¬í•­
    if avg_quality < 0.7:
        strategy_result["recommendations"].append("ğŸ”§ í”„ë¡¬í”„íŠ¸ ê°œì„  í•„ìš”")
    
    if len(strategy_result["low_quality_sections"]) > 0:
        strategy_result["recommendations"].append(f"ğŸ”„ {len(strategy_result['low_quality_sections'])}ê°œ ì„¹ì…˜ ì¬ìƒì„± í•„ìš”")
    
    return strategy_result

def performance_optimization_strategy(paper_count: int, avg_processing_time: float) -> Dict:
    """
    ğŸš€ ì„±ëŠ¥ ìµœì í™” ì „ëµ - ë‹¤ë¥¸ ë…¼ë¬¸ ì²˜ë¦¬ ì‹œ ì‹œê°„ ë‹¨ì¶• ë°©ì•ˆ
    
    Args:
        paper_count: ì²˜ë¦¬í•  ë…¼ë¬¸ ìˆ˜
        avg_processing_time: í‰ê·  ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
        
    Returns:
        Dict: ìµœì í™” ì „ëµ
    """
    optimization_result = {
        "estimated_total_time": paper_count * avg_processing_time,
        "optimization_strategies": [],
        "cache_benefits": {},
        "parallel_processing": {},
        "quality_tradeoffs": {}
    }
    
    # ì˜ˆìƒ ì´ ì²˜ë¦¬ ì‹œê°„
    total_time_hours = (paper_count * avg_processing_time) / 3600
    optimization_result["estimated_total_time_hours"] = total_time_hours
    
    # ìµœì í™” ì „ëµ
    if paper_count > 5:
        optimization_result["optimization_strategies"].append({
            "strategy": "ë°°ì¹˜ ì²˜ë¦¬",
            "description": "ì—¬ëŸ¬ ë…¼ë¬¸ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ì—¬ ì „ì²´ ì‹œê°„ ë‹¨ì¶•",
            "time_saving": "30-50%",
            "implementation": "ë³‘ë ¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"
        })
    
    if paper_count > 10:
        optimization_result["optimization_strategies"].append({
            "strategy": "ìºì‹œ ìš°ì„  ì²˜ë¦¬",
            "description": "ê³ í’ˆì§ˆ ìºì‹œê°€ ìˆëŠ” ë…¼ë¬¸ì„ ìš°ì„  ì²˜ë¦¬",
            "time_saving": "60-80%",
            "implementation": "í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ í"
        })
    
    if paper_count > 20:
        optimization_result["optimization_strategies"].append({
            "strategy": "í’ˆì§ˆ ì„ê³„ê°’ ì¡°ì •",
            "description": "ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ í’ˆì§ˆ ê¸°ì¤€ì„ ì¡°ì •",
            "time_saving": "40-60%",
            "implementation": "CACHE_QUALITY_THRESHOLDë¥¼ 0.85 â†’ 0.75ë¡œ ì¡°ì •"
        })
    
    # ìºì‹œ í˜œíƒ ë¶„ì„
    optimization_result["cache_benefits"] = {
        "cache_hit_rate": "70-90% (YOLO ë…¼ë¬¸ ê¸°ì¤€)",
        "time_saving_per_hit": "95% (ìºì‹œì—ì„œ ì¦‰ì‹œ ë°˜í™˜)",
        "total_time_saving": f"{total_time_hours * 0.8:.1f}ì‹œê°„ (80% ë‹¨ì¶•)"
    }
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì „ëµ
    optimization_result["parallel_processing"] = {
        "max_concurrent_papers": min(paper_count, 4),  # GPU ë©”ëª¨ë¦¬ ê³ ë ¤
        "section_parallelization": "ì„¹ì…˜ë³„ ë³‘ë ¬ ì²˜ë¦¬",
        "estimated_speedup": "2-4ë°°"
    }
    
    # í’ˆì§ˆ vs ì‹œê°„ íŠ¸ë ˆì´ë“œì˜¤í”„
    optimization_result["quality_tradeoffs"] = {
        "high_quality": {
            "quality_threshold": 0.85,
            "processing_time": "100%",
            "cache_hit_rate": "90%"
        },
        "medium_quality": {
            "quality_threshold": 0.75,
            "processing_time": "70%",
            "cache_hit_rate": "80%"
        },
        "fast_processing": {
            "quality_threshold": 0.65,
            "processing_time": "50%",
            "cache_hit_rate": "60%"
        }
    }
    
    return optimization_result

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_text = """
    YOLOëŠ” ë‹¨ì¼ neural network(ë‡Œ êµ¬ì¡°ë¥¼ ë³¸ëœ¬ ì¸ê³µ ì‹ ê²½ë§)ë¥¼ ì‚¬ìš©í•˜ì—¬ 
    bounding box(ë¬¼ì²´ë¥¼ ë‘˜ëŸ¬ì‹¸ëŠ” ë„¤ëª¨ ìƒì)ì™€ class probability(ê° ë¬¼ì²´ê°€ íŠ¹ì • í´ë˜ìŠ¤ì¼ í™•ë¥ )ë¥¼ 
    ì§ì ‘ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ í†µí•© ëª¨ë¸ì€ ì „í†µì ì¸ object detection(ì´ë¯¸ì§€ ì†ì—ì„œ ë¬¼ì²´ì˜ ìœ„ì¹˜ì™€ ì¢…ë¥˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ì‘ì—…) 
    ë°©ë²•ë“¤ì— ë¹„í•´ ì—¬ëŸ¬ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.
    """
    
    original_content = """
    YOLO uses a single neural network to predict bounding boxes and class probabilities 
    directly from full images in one evaluation. This unified model has several advantages 
    over traditional object detection methods.
    """
    
    # ìì²´ê²€ì¦ ì‹¤í–‰
    validation = llama_self_validation(test_text, original_content)
    print("=== Llama ìì²´ê²€ì¦ ê²°ê³¼ ===")
    print(f"í’ˆì§ˆ ì ìˆ˜: {validation['quality_score']:.2f}")
    print(f"ìºì‹œ ì €ì¥ ê°€ëŠ¥: {validation['is_cache_worthy']}")
    print(f"ê¸°ìˆ  ìš©ì–´ ìˆ˜: {validation['technical_terms_count']}")
    print(f"ì„¤ëª… ë¹„ìœ¨: {validation['explanation_ratio']:.2f}")
    print(f"ì´ìŠˆ: {validation['issues']}")
    
    # ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì „ëµ
    sections = [{"title": "Introduction"}, {"title": "Method"}, {"title": "Results"}]
    quality_scores = [0.9, 0.8, 0.7]
    
    strategy = smart_cache_strategy("test_paper", sections, quality_scores)
    print("\n=== ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì „ëµ ===")
    print(f"ìºì‹œ ì €ì¥ ê¶Œì¥: {strategy['should_cache']}")
    print(f"ìºì‹œ ë¹„ìœ¨: {strategy['cache_ratio']:.2f}")
    print(f"ê¶Œì¥ì‚¬í•­: {strategy['recommendations']}")
    
    # ì„±ëŠ¥ ìµœì í™” ì „ëµ
    optimization = performance_optimization_strategy(10, 300)  # 10ê°œ ë…¼ë¬¸, í‰ê·  5ë¶„
    print("\n=== ì„±ëŠ¥ ìµœì í™” ì „ëµ ===")
    print(f"ì˜ˆìƒ ì´ ì‹œê°„: {optimization['estimated_total_time_hours']:.1f}ì‹œê°„")
    print(f"ìµœì í™” ì „ëµ: {[s['strategy'] for s in optimization['optimization_strategies']]}")
    print(f"ìºì‹œ í˜œíƒ: {optimization['cache_benefits']['total_time_saving']}")
