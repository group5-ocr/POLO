# -*- coding: utf-8 -*-
"""
POLO Easy Model - Grounded JSON Generator (Patched)
- Fix(1): token-based slicing (ì²« ê¸€ì ì˜ë¦¼ ë°©ì§€)
- Fix(2): decoding for stability (do_sample=False, no_repeat_ngram_size=4, repetition_penalty=1.2)
- Fix(3): explicit stop markers + safe cut
- Fix(4): LaTeX/table strip keeps sentence boundaries (" . ")
- Fix(5): sanitize repeats (e.g., *flops spam*)
- Fix(6): prompt policy (no invention; 'not stated in the original text'), + style=three_para_ko
"""

from __future__ import annotations

import os, re, json, time, base64, gzip, sys, asyncio, logging
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple

import anyio, httpx, torch, uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, ConfigDict, Field
from dotenv import load_dotenv

# -------------------- .env --------------------
ROOT_ENV = Path(__file__).resolve().parents[2] / ".env"
if ROOT_ENV.exists():
    load_dotenv(dotenv_path=str(ROOT_ENV), override=True)

# -------------------- ENV / ê¸°ë³¸ê°’ --------------------
HF_TOKEN           = os.getenv("HUGGINGFACE_TOKEN", "")
BASE_MODEL         = os.getenv("EASY_BASE_MODEL", "meta-llama/Llama-3.2-3B-Instruct")
ADAPTER_DIR        = os.getenv("EASY_ADAPTER_DIR", str(Path(__file__).resolve().parent.parent / "fine-tuning" / "outputs" / "llama32-3b-qlora" / "checkpoint-4000"))
MAX_NEW_TOKENS     = int(os.getenv("EASY_MAX_NEW_TOKENS", "1200"))
VIZ_MODEL_URL      = os.getenv("VIZ_MODEL_URL", "http://localhost:5005")
EASY_CONCURRENCY   = int(os.getenv("EASY_CONCURRENCY", "8"))
EASY_BATCH_TIMEOUT = int(os.getenv("EASY_BATCH_TIMEOUT", "1800"))
EASY_VIZ_TIMEOUT   = float(os.getenv("EASY_VIZ_TIMEOUT", "1800"))   # ê¸°ë³¸ 30ë¶„
EASY_VIZ_HEALTH_TIMEOUT = float(os.getenv("EASY_VIZ_HEALTH_TIMEOUT", "5"))

EASY_STRIP_MATH = os.getenv("EASY_STRIP_MATH", "1").lower() in ("1", "true", "yes")
EASY_FORCE_KO   = os.getenv("EASY_FORCE_KO", "1").lower() in ("1", "true", "yes")
EASY_AUTO_BOLD  = os.getenv("EASY_AUTO_BOLD", "1").lower() in ("1", "true", "yes")
EASY_HILITE     = os.getenv("EASY_HILITE", "1").lower() in ("1", "true", "yes")

MAX_INPUT_TOKENS = 2048
_RETRY_TOKENS = (1536, 1024, 768)

# -------------------- HF cache pin --------------------
SAFE_CACHE_DIR = Path(__file__).resolve().parent / "hf_cache"
SAFE_CACHE_DIR.mkdir(parents=True, exist_ok=True)
for k in ("HF_HOME","TRANSFORMERS_CACHE","HF_DATASETS_CACHE","HF_HUB_CACHE"):
    os.environ[k] = str(SAFE_CACHE_DIR)
CACHE_DIR = os.environ["HF_HOME"]

from transformers import AutoModelForCausalLM, AutoTokenizer  # noqa: E402
from peft import PeftModel  # noqa: E402

# -------------------- Logger --------------------
logger = logging.getLogger("polo.easy")
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")

# Windows ì†Œì¼“ ì¢…ë£Œ ê´€ë ¨ ê²½ê³ /ì—ëŸ¬ ì™„í™”
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    logging.getLogger("asyncio").setLevel(logging.WARNING)

# -------------------- FastAPI --------------------
app = FastAPI(title="POLO Easy Model", version="1.5.0-patched")

# -------------------- Global state --------------------
model = None
tokenizer = None
device = "cuda" if torch.cuda.is_available() else "cpu"
gpu_available = torch.cuda.is_available()
safe_dtype = torch.float16 if gpu_available else torch.float32

# -------------------- Utils --------------------
def _pick_attn_impl() -> str:
    try:
        import flash_attn  # noqa
        logger.info("âœ… flash_attn ì‚¬ìš©: flash_attention_2")
        return "flash_attention_2"
    except Exception:
        pass
    try:
        from torch.nn.functional import scaled_dot_product_attention  # noqa
        logger.info("â„¹ï¸ sdpa ì‚¬ìš© ê°€ëŠ¥")
        return "sdpa"
    except Exception:
        logger.info("â„¹ï¸ sdpa ë¶ˆê°€ â†’ eager")
        return "eager"

def _coerce_json(text: str) -> Dict[str, Any]:
    s = text.find("{"); e = text.rfind("}")
    if s != -1 and e != -1 and e > s:
        text = text[s:e+1]
    return json.loads(text)

def _contains_hangul(text: str) -> bool:
    return bool(re.search(r"[ê°€-í£]", text or ""))

def _detect_lang_safe(text: str) -> str:
    if not text or not text.strip():
        return "en"
    if _contains_hangul(text): return "ko"
    latin_chars = len(re.findall(r"[A-Za-z]", text))
    total_chars = len(re.findall(r"[A-Za-zê°€-í£]", text))
    if total_chars == 0: return "en"
    return "ko" if (latin_chars/total_chars) < 0.5 else "en"

# === Fix(4): Keep sentence boundaries when stripping ===
def _strip_math_all(s: str) -> str:
    if not s: return s
    rep = " . "
    s = re.sub(r"\$\$[\s\S]*?\$\$", rep, s)
    s = re.sub(r"\\\[[\s\S]*?\\\]", rep, s)
    s = re.sub(r"\\\([\s\S]*?\\\)", rep, s)
    s = re.sub(r"\$(?!\$)(?:[^$\\]|\\.)+\$", rep, s)
    s = re.sub(r"\\begin\{(equation\*?|align\*?|gather\*?|multline\*?|eqnarray\*?)\}[\s\S]*?\\end\{\1\}", rep, s)
    return s

def _strip_tables_figures_all(s: str) -> str:
    if not s: return s
    rep = " . "
    s = re.sub(r"\\begin\{table\*?\}[\s\S]*?\\end\{table\*?\}", rep, s)
    s = re.sub(r"\\begin\{tabularx?\*?\}[\s\S]*?\\end\{tabularx?\*?\}", rep, s)
    s = re.sub(r"\\begin\{figure\*?\}[\s\S]*?\\end\{figure\*?\}", rep, s)
    s = re.sub(r"\\includegraphics(?:\[[^\]]*\])?\{[^}]*\}", rep, s)
    s = re.sub(r"\\caption\{[^}]*\}", rep, s)
    s = re.sub(r"(?i)\b(?:figure|table)\s*\d+\s*[:.]\s*", rep, s)
    s = re.sub(r"(?:í‘œ|ê·¸ë¦¼)\s*\d+\s*[:.]\s*", rep, s)
    return s

def _normalize_bracket_tokens(text: str) -> str:
    if not text: return text
    # LRB/RRB íŒ¨í„´ì„ ê´„í˜¸ë¡œ ì¹˜í™˜ (ëŒ€ì†Œë¬¸ì ìƒê´€ì—†ì´)
    text = re.sub(r"(?i)\bL\s*R\s*B\b", "(", text)
    text = re.sub(r"(?i)\bL\s*L\s*B\b", "(", text)
    text = re.sub(r"(?i)\bR\s*R\s*B\b", ")", text)
    
    # ì¶”ê°€ ê´„í˜¸ íŒ¨í„´ë“¤ (ëŒ€ì†Œë¬¸ì ìƒê´€ì—†ì´)
    text = re.sub(r"(?i)\blrb\b", "(", text)
    text = re.sub(r"(?i)\brrb\b", ")", text)
    text = re.sub(r"(?i)\bl\s*r\s*b\b", "(", text)
    text = re.sub(r"(?i)\br\s*r\s*b\b", ")", text)
    
    return text

def _postprocess_terms(text: str) -> str:
    if not text: return text
    text = re.sub(r"(?i)\bY\s*O\s*L\s*O\b", "YOLO", text)
    text = re.sub(r"(?i)\bO\s*L\s*O\b", "YOLO", text)
    text = re.sub(r"(?i)\bm\s*A\s*P\b", "mAP", text)
    text = re.sub(r"(?i)\bR\s*-?\s*CNN\b", "R-CNN", text)
    text = re.sub(r"(?i)\bFast\s*-?\s*R\s*-?\s*CNN\b", "Fast R-CNN", text)
    text = re.sub(r"(?i)\bFaster\s*-?\s*R\s*-?\s*CNN\b", "Faster R-CNN", text)
    return text

def _auto_bold_terms(text: str) -> str:
    if not text or not EASY_AUTO_BOLD: return text
    patterns = [
        r"\b(?:YOLOv?\d+|YOLO|RetinaNet|Faster\s*R-?CNN|Fast\s*R-?CNN|R-?CNN|ResNet-?\d+|EfficientNet[-A-Z0-9]*)\b",
        r"\b(?:Transformer|ViT|CLIP|BERT|GPT(?:-\d+)?)\b",
        r"\b(?:mAP|IoU|F1|AP50|AP75|AUC|ROC|BLEU|ROUGE|PSNR|SSIM|CER|WER)\b",
        r"\b\d+\s?[xÃ—]\s?\d+\b",
        r"\b(?:Top-1|Top-5|SOTA|SotA|FPS|GFLOPs?|Params?)\b",
        r"\b(?:Llama|LLaMA|Mistral|Phi|Qwen|Gemma|Whisper)\b",
    ]
    def wrap(m: re.Match) -> str:
        g = m.group(0)
        if g.startswith("**") and g.endswith("**"): return g
        return f"**{g}**"
    for pat in patterns:
        text = re.sub(pat, wrap, text)
    return text

def _hilite_sentences(text: str, max_marks: int = 2) -> str:
    if not text or not EASY_HILITE: return text
    parts = re.split(r"(?<=[.!?])\s+|(?<=ë‹¤\.)\s+", text.strip())
    marks = 0; out = []
    key = re.compile(r"(?:ê¸°ì—¬|í•µì‹¬|ìš”ì•½|ê²°ë¡ |ì„±ëŠ¥|ê°œì„ |í–¥ìƒ|ì •í™•ë„|ìš°ìˆ˜|ë‹¬ì„±|ì¦ê°€|ê°ì†Œ|í•œê³„|ì œì•ˆ|ìš°ë¦¬\s*ëª¨ë¸|ë³¸\s*ì—°êµ¬|SOTA|improv|achiev|propos|contribut)", re.IGNORECASE)
    has_number = re.compile(r"\d+(?:\.\d+)?\s*(?:%|ì |ë°°|ms|s|fps|GFLOPs?|M|K)?", re.IGNORECASE)
    for sent in parts:
        s = sent.strip()
        if not s: continue
        if marks < max_marks and (key.search(s) or has_number.search(s)):
            out.append(f"=={s}=="); marks += 1
        else:
            out.append(s)
    return " ".join(out)

# === Fix(5): sanitize pathological repeats (e.g., flops spam) ===
def _sanitize_repeats(text: str) -> str:
    if not text: 
        return text
    # ê°™ì€ ë‹¨ì–´ê°€ 4ë²ˆ ì´ìƒ ì—°ì† â†’ 3ë²ˆìœ¼ë¡œ ì¶•ì†Œ
    text = re.sub(r"\b(\w{2,})\b(?:\s+\1\b){3,}", r"\1 \1 \1", text, flags=re.IGNORECASE)

    # 'flops'ë¥˜ 6íšŒ ì´ìƒ ë“œë¡  â†’ ì œê±°
    text = re.sub(r"(?:\b[\w\-]*flops\b[\s,.;:]*){6,}", " ", text, flags=re.IGNORECASE)

    # 'yolo' ë³€ì¢… í† í°(yolov, yoloz, yowl...)ì´ 6íšŒ ì´ìƒ ì—°ì† â†’ 2íšŒë¡œ ì¶•ì†Œ
    text = re.sub(r"(?:\byo[a-z]{1,6}\w*\b[\s,.;:]*){6,}", " yolo yolo ", text, flags=re.IGNORECASE)

    # ì˜ë¯¸ ì—†ëŠ” ê¸€ì ê¼¬ì´ê¸°: ììŒ/ëª¨ìŒ+ë°˜ë³µ
    text = re.sub(r"(?:\b[bcdfghjklmnpqrstvwxyz]{2,}\b[\s]*){5,}", " ", text, flags=re.IGNORECASE)

    # ì•„ì£¼ ê¸´ ë™ì¼ ì ‘ë‘ì‚¬ ë°˜ë³µ(ì˜ˆ: yolov yolov2 yolov3 ... 15+) â†’ ì••ì¶•
    text = re.sub(r"((?:\byolo\w{0,6}\b[\s]*){8,})", " yolo yolo ", text, flags=re.IGNORECASE)

    # ê³µë°± ì •ë¦¬
    text = re.sub(r"\s{2,}", " ", text).strip()
    return text

# === PART 1/4 END ===

# === PART 2/4 START ===
# -------------------- Translation (LLM) --------------------
def _ensure_korean(text: str) -> str:
    if not text or not text.strip(): return text
    lang = _detect_lang_safe(text)
    latin = len(re.findall(r"[A-Za-z]", text))
    hangul = len(re.findall(r"[ê°€-í£]", text))
    high_latin_ratio = latin > 0 and (hangul == 0 or latin / max(1, latin + hangul) >= 0.4)
    
    # í•œêµ­ì–´ê°€ ê±°ì˜ ì—†ìœ¼ë©´ ê°•ì œ ë²ˆì—­
    if hangul < 10 or (latin > 0 and hangul / max(1, latin + hangul) < 0.3):
        return _translate_to_korean(text)
    
    if lang == "ko" and not (EASY_FORCE_KO and high_latin_ratio): return text
    if lang == "en" or (EASY_FORCE_KO and high_latin_ratio): return _translate_to_korean(text)
    if EASY_FORCE_KO and hangul < latin: return _translate_to_korean(text)
    return text

def _translate_to_korean(text: str) -> str:
    """Papago APIë¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´ ë²ˆì—­"""
    try:
        if not text or not text.strip(): return ""
        
        # Papago API ì„¤ì •
        client_id = os.getenv("PAPAGO_CLIENT_ID", "")
        client_secret = os.getenv("PAPAGO_CLIENT_SECRET", "")
        use_chain = os.getenv("PAPAGO_USE_CHAIN", "false").lower() in ("true", "1", "yes")
        
        if not client_id or not client_secret:
            logger.warning("Papago API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ â†’ LLM ë²ˆì—­ ì‚¬ìš©")
            return _translate_with_llm(text)
        
        # ë²ˆì—­ ë°©ë²• ì„ íƒ
        if use_chain:
            logger.info("[PAPAGO] ì²´ì¸ ë²ˆì—­ ì‚¬ìš©: ì˜ì–´ â†’ ì¼ë³¸ì–´ â†’ í•œêµ­ì–´")
            result = _translate_chain_en_ja_ko(text, client_id, client_secret)
        else:
            logger.info("[PAPAGO] ë‹¨ì¼ ë²ˆì—­ ì‚¬ìš©: ì˜ì–´ â†’ í•œêµ­ì–´")
            result = _translate_with_papago(text, client_id, client_secret)
        
        if result:
            return result
        
        # Papago ì‹¤íŒ¨ ì‹œ LLM ë²ˆì—­ìœ¼ë¡œ í´ë°±
        logger.warning("Papago ë²ˆì—­ ì‹¤íŒ¨ â†’ LLM ë²ˆì—­ ì‚¬ìš©")
        return _translate_with_llm(text)
        
    except Exception as e:
        logger.warning(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return text

def _translate_with_papago(text: str, client_id: str, client_secret: str) -> str:
    """Papago APIë¥¼ ì‚¬ìš©í•œ ë‹¨ì¼ ë²ˆì—­"""
    try:
        import requests
        
        url = "https://openapi.naver.com/v1/papago/n2mt"
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }
        data = {
            "source": "en",
            "target": "ko",
            "text": text[:4000]  # Papago API ì œí•œ
        }
        
        response = requests.post(url, headers=headers, data=data)
        if response.status_code == 200:
            result = response.json()
            translated_text = result.get("message", {}).get("result", {}).get("translatedText", "")
            if translated_text:
                logger.info(f"[PAPAGO] ë²ˆì—­ ì™„ë£Œ: {len(text)}ì â†’ {len(translated_text)}ì")
                return translated_text
        
        logger.warning(f"Papago API ì˜¤ë¥˜: {response.status_code}")
        return ""
        
    except Exception as e:
        logger.warning(f"Papago API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return ""

def _translate_chain_en_ja_ko(text: str, client_id: str, client_secret: str) -> str:
    """ì˜ì–´ â†’ ì¼ë³¸ì–´ â†’ í•œêµ­ì–´ ì²´ì¸ ë²ˆì—­"""
    try:
        import requests
        
        url = "https://openapi.naver.com/v1/papago/n2mt"
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }
        
        # 1ë‹¨ê³„: ì˜ì–´ â†’ ì¼ë³¸ì–´
        data1 = {
            "source": "en",
            "target": "ja",
            "text": text[:4000]
        }
        response1 = requests.post(url, headers=headers, data=data1)
        if response1.status_code != 200:
            return ""
        
        japanese = response1.json().get("message", {}).get("result", {}).get("translatedText", "")
        if not japanese:
            return ""
        
        # 2ë‹¨ê³„: ì¼ë³¸ì–´ â†’ í•œêµ­ì–´
        data2 = {
            "source": "ja",
            "target": "ko",
            "text": japanese[:4000]
        }
        response2 = requests.post(url, headers=headers, data=data2)
        if response2.status_code != 200:
            return ""
        
        korean = response2.json().get("message", {}).get("result", {}).get("translatedText", "")
        if korean:
            logger.info(f"[PAPAGO_CHAIN] ì²´ì¸ ë²ˆì—­ ì™„ë£Œ: {len(text)}ì â†’ {len(korean)}ì")
            return korean
        
        return ""
        
    except Exception as e:
        logger.warning(f"Papago ì²´ì¸ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return ""

def _translate_with_llm(text: str) -> str:
    """LLMì„ ì‚¬ìš©í•œ ë²ˆì—­ (í´ë°±)"""
    try:
        if model is None or tokenizer is None:
            logger.warning("ëª¨ë¸ ë¯¸ë¡œë“œ â†’ ë²ˆì—­ ìŠ¤í‚µ")
            return text
        
        translate_prompt = (
            "<|begin_of_text|>\n"
            "<|start_header_id|>system<|end_header_id|>\n"
            "ë„ˆëŠ” í•™ìˆ  ë…¼ë¬¸ ì „ë¬¸ ë²ˆì—­ê°€ë‹¤. ì˜ì–´ ë…¼ë¬¸ì„ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ë¼.\n"
            "ë²ˆì—­ ê·œì¹™:\n"
            "1. ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì˜ë¯¸ë¥¼ ì •í™•íˆ ì „ë‹¬\n"
            "2. ì „ë¬¸ ìš©ì–´ëŠ” í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ë˜, í•„ìš”ì‹œ ì›ì–´ë¥¼ ê´„í˜¸ì— í‘œê¸°\n"
            "3. ìˆ˜ì‹, í‘œ, ê·¸ë¦¼ ì–¸ê¸‰ì€ ì œê±°í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ìœ ì§€\n"
            "4. í•™ìˆ ì  í†¤ì„ ìœ ì§€í•˜ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ë²ˆì—­\n"
            "5. ë…¼ë¬¸ì˜ ë…¼ë¦¬ì  íë¦„ê³¼ êµ¬ì¡°ë¥¼ ë³´ì¡´\n"
            "<|eot_id|>\n"
            "<|start_header_id|>user<|end_header_id|>\n"
            f"[ì˜ì–´ ë…¼ë¬¸ í…ìŠ¤íŠ¸]\n{text}\n\n[í•œêµ­ì–´ ë²ˆì—­]\n"
            "<|eot_id|>\n"
        )
        
        inputs = tokenizer(translate_prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=min(MAX_NEW_TOKENS, int(os.getenv("EASY_SECTION_CAP", "1000"))),
                do_sample=False,
                use_cache=True,
                repetition_penalty=1.2,
                no_repeat_ngram_size=4,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        seq = outputs[0]
        inp_len = inputs["input_ids"].shape[1]
        gen_tokens = seq[inp_len:]
        result = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()
        
        for stop in ("[/OUTPUT]", "[/output]", "<|eot_id|>"):
            p = result.find(stop)
            if p != -1:
                result = result[:p].strip()
                break
        
        result = _postprocess_terms(result)
        result = _strip_math_all(result)
        result = _strip_tables_figures_all(result)
        result = _sanitize_repeats(result)
        return result or text
        
    except Exception as e:
        logger.warning(f"LLM ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return text

# -------------------- I/O Models --------------------
class TextRequest(BaseModel):
    text: str
    translate: Optional[bool] = False
    style: Optional[str] = Field(default="default", description="easy ìŠ¤íƒ€ì¼: 'default' | 'three_para_ko'")
    model_config = ConfigDict(extra="allow")

class TextResponse(BaseModel):
    simplified_text: str
    translated_text: Optional[str] = None

class BatchRequest(BaseModel):
    paper_id: str = Field(..., description="ê²°ê³¼ íŒŒì¼/ê²½ë¡œ ì‹ë³„ì")
    chunks_jsonl: str = Field(..., description="JSONL ë‚´ìš© ë¬¸ìì—´ ë˜ëŠ” ê²½ë¡œ(íŒŒì¼/ë””ë ‰í† ë¦¬/tex)")
    output_dir: str = Field(..., description="ê²°ê³¼ ì €ì¥ ë£¨íŠ¸")
    style: Optional[str] = Field(default="three_para_ko", description="easy ìŠ¤íƒ€ì¼ (default|three_para_ko)")

class VizResult(BaseModel):
    ok: bool = True
    index: int
    image_path: Optional[str] = None
    error: Optional[str] = None
    easy_text: Optional[str] = None
    section_title: Optional[str] = None
    section_type: Optional[str] = None
    original_images: Optional[List[Dict[str, Any]]] = None

class BatchResult(BaseModel):
    ok: bool
    paper_id: str
    count: int
    success: int
    failed: int
    out_dir: str
    images: List[VizResult]

class TransportRequest(BaseModel):
    paper_id: str
    transport_path: str
    output_dir: Optional[str] = None
    style: Optional[str] = Field(default="three_para_ko")

# -------------------- Model Load --------------------
def load_model():
    global model, tokenizer, gpu_available, device, safe_dtype
    logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”©: {BASE_MODEL}")
    logger.info(f"EASY_ADAPTER_DIR={ADAPTER_DIR}")
    logger.info(f"HF_HOME={os.getenv('HF_HOME')}")
    if torch.cuda.is_available():
        gpu_available = True; device = "cuda"; safe_dtype = torch.float16
        logger.info(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
    else:
        gpu_available = False; device = "cpu"; safe_dtype = torch.float32
        logger.info("âš ï¸ CPU ëª¨ë“œ")

    tokenizer_local = AutoTokenizer.from_pretrained(BASE_MODEL, token=HF_TOKEN, trust_remote_code=True, cache_dir=CACHE_DIR)
    if tokenizer_local.pad_token_id is None:
        tokenizer_local.pad_token = tokenizer_local.eos_token

    attn_impl = _pick_attn_impl()
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    try:
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL, torch_dtype=safe_dtype, trust_remote_code=True,
            attn_implementation=attn_impl, low_cpu_mem_usage=True, token=HF_TOKEN, cache_dir=CACHE_DIR,
        )
    except Exception as e:
        logger.warning(f"attn='{attn_impl}' ì‹¤íŒ¨({e}) â†’ eager")
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL, torch_dtype=safe_dtype, trust_remote_code=True,
            attn_implementation="eager", low_cpu_mem_usage=True, token=HF_TOKEN, cache_dir=CACHE_DIR,
        )

    m = base
    if ADAPTER_DIR and os.path.exists(ADAPTER_DIR):
        try:
            m = PeftModel.from_pretrained(base, os.path.abspath(ADAPTER_DIR), is_trainable=False, local_files_only=True)
            logger.info("âœ… ì–´ëŒ‘í„° ë¡œë”© OK")
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° ë¡œë”© ì‹¤íŒ¨: {e} â†’ ë² ì´ìŠ¤ ì‚¬ìš©")
            m = base

    m.eval(); m = m.to(safe_dtype).to(device)
    globals()["model"] = m; globals()["tokenizer"] = tokenizer_local
    logger.info("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")

@app.on_event("startup")
async def startup_event():
    load_model()

# -------------------- Prompts --------------------
def _clean_latex_text(text: str) -> str:
    if not text: return ""
    s = text
    if EASY_STRIP_MATH: s = _strip_math_all(s)
    s = _strip_tables_figures_all(s)
    s = re.sub(r"\\cite\{[^}]*\}|\\label\{[^}]*\}|\\ref\{[^}]*\}", " ", s)
    s = re.sub(r"\\url\{([^}]*)\}", r"(\1)", s)
    s = re.sub(r"\\(textbf|textit|emph)\{([^}]*)\}", r"\2", s)
    s = re.sub(r"^\\(sub)*section\{[^}]*\}\s*", "", s, flags=re.MULTILINE)
    s = re.sub(r"\s+", " ", s); s = re.sub(r"\s*\n\s*", "\n", s)
    return s.strip()

def _build_easy_prompt(text: str, section_title: str | None = None) -> str:
    title_line = f"[Section] {section_title}\n\n" if section_title else ""
    system_block = (
        "<|begin_of_text|>\n"
        "<|start_header_id|>system<|end_header_id|>\n"
        "You are a professional science communicator.\n"
        "Rewrite to simple, readable English for beginners.\n\n"
        "Policy (DO NOT VIOLATE):\n"
        "- Use ONLY information from [Original Text].\n"
        "- Never invent numbers, tables, versions, or comparisons not present.\n"
        "- If something is not in [Original Text], write: 'not stated in the original text'.\n"
        "- Exclude equations/tables/figures.\n"
        "<|eot_id|>\n"
    )
    user_block = (
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{title_line}[Original Text]\n{text}\n\n[OUTPUT]\n"
        "<|eot_id|>\n"
    )
    return system_block + user_block

def _build_easy_prompt_three_para_ko(text: str, section_title: str | None = None) -> str:
    title_line = f"[ì„¹ì…˜] {section_title}\n\n" if section_title else ""
    system = (
        "<|begin_of_text|>\n"
        "<|start_header_id|>system<|end_header_id|>\n"
        "ë„ˆëŠ” ê³¼í•™/AI ë‚´ìš©ì„ ì¤‘í•™ìƒë„ ì´í•´í•˜ê²Œ í•œêµ­ì–´ë¡œ ì•„ì£¼ ì‰½ê²Œ ì„¤ëª…í•œë‹¤.\n"
        "ì •í™•íˆ 3ë‹¨ë½(ê° 1~2ë¬¸ì¥). 1)ë¬´ì—‡ 2)ì™œ(ì¥ì /ë¹ ë¦„) 3)ì–´ë””ì— ì“°ë‚˜.\n"
        "ì›ë¬¸ ë°– ì •ë³´ ê¸ˆì§€. ì›ë¬¸ì— ì—†ìœ¼ë©´ 'ì›ë¬¸ì— ì—†ìŒ'ì´ë¼ê³  ì ëŠ”ë‹¤.\n"
        "<|eot_id|>\n"
    )
    user = (
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{title_line}[ì›ë¬¸]\n{text}\n\n[OUTPUT]\n"
        "<|eot_id|>\n"
    )
    return system + user

# === PART 2/4 END ===

# === PART 3/4 START ===
# -------------------- Core Rewriter --------------------
async def _rewrite_text(text: str, section_title: str = None, context_info: str = None, style: str = "default") -> str:
    if model is None or tokenizer is None:
        raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    cleaned_text = _clean_latex_text(text)
    if context_info:
        cleaned_text = f"[ë¬¸ë§¥] {context_info}\n\n" + cleaned_text

    if style == "three_para_ko":
        prompt = _build_easy_prompt_three_para_ko(cleaned_text, section_title)
    else:
        prompt = _build_easy_prompt(cleaned_text, section_title)

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    logger.info(f"[EASY] ğŸŸ¦ rewrite START: title='{section_title or 'N/A'}' style={style}")

    if torch.cuda.is_available():
        torch.cuda.synchronize()
    t0 = time.time()

    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=min(MAX_NEW_TOKENS, 1000),
            do_sample=False,          # deterministic
            use_cache=True,
            repetition_penalty=1.2,
            no_repeat_ngram_size=4,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    if torch.cuda.is_available():
        torch.cuda.synchronize()
    elapsed = time.time() - t0

    seq = outputs[0]
    inp_len = inputs["input_ids"].shape[1]
    gen_tokens = seq[inp_len:]
    gen_tok_count = int(gen_tokens.shape[0])

    if torch.cuda.is_available():
        mem_alloc = torch.cuda.memory_allocated() / (1024**2)
        mem_reserved = torch.cuda.memory_reserved() / (1024**2)
        mem_line = f" | GPU mem alloc={mem_alloc:.1f}MB reserved={mem_reserved:.1f}MB"
    else:
        mem_line = ""

    logger.info(f"[EASY] âœ… rewrite DONE: tokens={gen_tok_count} elapsed={elapsed:.2f}s{mem_line}")

    result = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()
    for stop in ("[/OUTPUT]", "[/output]", "<|eot_id|>"):
        p = result.find(stop)
        if p != -1:
            result = result[:p].strip()
            break

    result = _normalize_bracket_tokens(result)
    result = _postprocess_terms(result)
    if EASY_STRIP_MATH:
        result = _strip_math_all(result)
    result = _strip_tables_figures_all(result)
    result = _auto_bold_terms(result)
    result = _hilite_sentences(result, max_marks=2)
    result = _sanitize_repeats(result)
    return result

# -------------------- HTML helpers --------------------
def _get_current_datetime() -> str:
    from datetime import datetime
    return datetime.now().strftime("%Yë…„ %mì›” %dì¼ %H:%M:%S")

def _slugify(s: str, fallback: str) -> str:
    s = re.sub(r"[^0-9A-Za-zê°€-í£\- ]", "", s or "")
    s = s.strip().replace(" ", "-")
    return s if s else fallback

def _dedup_titles(sections: List[dict]) -> List[dict]:
    seen: Dict[str, int] = {}
    out: List[dict] = []
    for s in sections:
        s = dict(s)
        t = (s.get("title") or "").strip() or "Section"
        c = seen.get(t, 0) + 1
        seen[t] = c
        s["title"] = f"{t} ({c})" if c > 1 else t
        out.append(s)
    return out

def _slugify_unique(titles: List[str]) -> List[str]:
    used: set = set()
    slugs: List[str] = []
    for t in titles:
        base = _slugify(t, "sec")
        cand = base; k = 2
        while cand in used:
            cand = f"{base}-{k}"; k += 1
        used.add(cand); slugs.append(cand)
    return slugs

def _md_to_html(md: str) -> str:
    if not md: return ""
    def _codeblock_repl(m): return f"<pre><code>{m.group(1)}</code></pre>"
    md = re.sub(r"```([\s\S]*?)```", _codeblock_repl, md)
    md = re.sub(r"^###\s*(.+)$", r"<h3>\1</h3>", md, flags=re.MULTILINE)
    md = re.sub(r"^##\s*(.+)$",  r"<h2>\1</h2>", md, flags=re.MULTILINE)
    md = re.sub(r"^#\s*(.+)$",   r"<h1>\1</h1>", md, flags=re.MULTILINE)
    md = re.sub(r"(?<!\$)\*\*([^*$]+?)\*\*(?!\$)", r"<strong>\1</strong>", md)
    md = re.sub(r"(?<!\$)\*([^*$]+?)\*(?!\$)",     r"<em>\1</em>", md)
    md = re.sub(r"`([^`]+?)`",                    r"<code>\1</code>", md)
    md = re.sub(r"==([^=]+?))==?",                r"<mark>\1</mark>", md)
    md = re.sub(r"\[([^\]]+)\]\((https?://[^\s)]+)\)", r'<a href="\2" target="_blank">\1</a>', md)
    md = re.sub(r"(https?://[^\s<>\"']+)", r'<a href="\1" target="_blank">\1</a>', md)
    lines = md.splitlines(); out, in_ul = [], False
    for ln in lines:
        if re.match(r"^\s*[-â€¢]\s+.+", ln):
            if not in_ul: out.append("<ul>")
            in_ul = True; out.append("<li>"+re.sub(r"^\s*[-â€¢]\s+", "", ln).strip()+"</li>")
        else:
            if in_ul: out.append("</ul>"); in_ul = False
            out.append(ln)
    if in_ul: out.append("</ul>")
    html = "\n".join(out)
    blocks = [b.strip() for b in re.split(r"\n\s*\n", html) if b.strip()]
    wrapped = []
    for b in blocks:
        if b.startswith(("<h1","<h2","<h3","<ul","<pre","<table","<blockquote")):
            wrapped.append(b)
        else:
            wrapped.append(f"<p>{b}</p>")
    return "\n".join(wrapped)

def _render_rich_html(text: str) -> str:
    if not text: return ""
    html_text = _md_to_html(text)
    html_text = re.sub(r'\\textbf\{([^}]+)\}', r'<strong>\1</strong>', html_text)
    html_text = re.sub(r'\\textit\{([^}]+)\}', r'<em>\1</em>', html_text)
    html_text = re.sub(r'\\(sub)*section\{([^}]+)\}', r'<h2>\2</h2>', html_text)
    html_text = re.sub(r' +', ' ', html_text)
    return html_text

def _starts_with_same_heading(html: str, title: str) -> bool:
    if not title or not html: return False
    plain = re.sub(r"<[^>]+>", "", html).strip().lower()
    t = (title or "").strip().lower()
    return plain.startswith(t) or plain[:120].startswith(t + ":")

def _save_html_results(sections: List[dict], results: List[VizResult], output_path: Path, paper_id: str):
    toc_items: List[Tuple[str, str]] = []

    def _split_paragraphs_ko(text: str, min_s: int = 3, max_s: int = 5, max_chars: int = 700) -> str:
        import re as _re
        if not text:
            return ""
        s = _re.sub(r"\s+", " ", str(text).strip())
        parts = _re.split(r"(?<=[.!?])\s+|(?<=ë‹¤\.)\s+", s)
        parts = [p.strip() for p in parts if p and p.strip()]
        paras: List[str] = []
        cur: List[str] = []
        cur_len = 0
        for sent in parts:
            start_new = (len(cur) >= min_s and (len(cur) >= max_s or cur_len + len(sent) > max_chars))
            if start_new and cur:
                paras.append(" ".join(cur))
                cur = [sent]
                cur_len = len(sent)
            else:
                cur.append(sent)
                cur_len += len(sent)
        if cur:
            paras.append(" ".join(cur))
        out: List[str] = []
        for p in paras:
            if out and len(p) < 80:
                out[-1] = (out[-1] + " " + p).strip()
            else:
                out.append(p)
        return "\n\n".join(out)

    sections = _dedup_titles(list(sections))
    _titles = [(sec.get("title") or f"Section {i+1}").strip() for i, sec in enumerate(sections)]
    _unique_slugs = _slugify_unique(_titles)

    gen_at = _get_current_datetime()
    html_header = """
<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>POLO - ì‰¬ìš´ ë…¼ë¬¸ ì„¤ëª…</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
<style>
:root {
  --bg:#f7fafc; --card:#ffffff; --text:#111827; --muted:#6b7280; --brand:#2563eb; --hi:#fff7a1;
  --code-bg:#0f172a; --code-fg:#e5e7eb;
  --sidebar-w:260px;
}
* { box-sizing:border-box; }
html { scroll-behavior:smooth; }
body {
  font-family:'Noto Sans KR','Malgun Gothic','Apple SD Gothic Neo','Segoe UI',Roboto,Arial,sans-serif;
  background:var(--bg); color:var(--text); line-height:1.85; margin:0; padding:24px;
}
.wrapper { max-width:1100px; margin:0 auto; }
.header {
  background:var(--card); border:1px solid #e5e7eb; border-radius:12px;
  padding:22px 24px; margin-bottom:16px;
}
.title { font-weight:700; font-size:22pt; margin:0 0 6px; }
.subtitle { color:var(--muted); font-size:12.5pt; }
.meta { color:var(--muted); font-size:11pt; margin-top:6px; }

/* ë ˆì´ì•„ì›ƒ: ì¢Œì¸¡ ê³ ì • TOC + ìš°ì¸¡ ë³¸ë¬¸ */
.layout { display:grid; grid-template-columns:minmax(200px,var(--sidebar-w)) 1fr; gap:24px; align-items:start; }
.toc-sidebar {
  position:sticky; top:20px; max-height:calc(100vh - 40px); overflow:auto;
  background:var(--card); border:1px solid #e5e7eb; border-radius:12px; padding:16px 14px;
}
.toc-title { font-weight:700; font-size:13.5pt; margin:2px 0 10px; }
.toc-list, .toc-sublist { list-style:none; margin:0; padding:0; }
.toc-item { margin:6px 0; }
.toc-link {
  display:block; text-decoration:none; color:#1f2937; padding:2px 4px; border-radius:6px;
  font-weight:600; font-size:12.5pt;
}
.toc-link .num { color:var(--muted); margin-right:6px; font-weight:700; }
.toc-sublist .toc-link { font-weight:500; font-size:11.5pt; color:#374151; padding-left:6px; }
.toc-link:hover { background:#f3f4f6; }
.toc-link.active { background:#eef2ff; color:#111827; box-shadow:0 0 0 2px rgba(37,99,235,.15) inset; }

.content-area { min-width:0; }

.section-card {
  background:var(--card); border:1px solid #e5e7eb; border-radius:12px;
  padding:22px 22px; margin-bottom:18px;
}
.section-card.section { border-left:4px solid var(--brand); box-shadow:0 2px 8px rgba(37,99,235,.08); }
.section-card.subsection { border-left:4px solid #10b981; box-shadow:0 2px 8px rgba(16,185,129,.08); margin-left:0; margin-right:0; }
.section-card h2 { font-size:19pt; margin:2px 0 12px; font-weight:700; letter-spacing:.2px; }
.section-card.subsection h2 { font-size:16pt; color:#059669; position:relative; padding-left:0; }
.content p { margin:10px 0 12px; }
.content ul { margin:8px 0 12px 18px; }
.content li { margin:4px 0; }
.content code {
  background:#f6f8fa; padding:2px 6px; border-radius:4px;
  font-family:ui-monospace,SFMono-Regular,Menlo,Consolas,'Liberation Mono',monospace; font-size:12pt;
}
.content pre { background:var(--code-bg); color:var(--code-fg); padding:14px; border-radius:8px; overflow:auto; }
.content strong { font-weight:700; }
.content mark { background:var(--hi); padding:0 .2em; border-radius:3px; box-shadow:0 0 0 2px rgba(255,247,161,.4) inset; }

.image-container { text-align:center; margin:18px 0 6px; padding:12px; border:1px dashed #e5e7eb; border-radius:10px; }
.image-caption { margin-top:6px; color:var(--muted); font-size:10.5pt; font-style:italic; }

.footer-actions { position:fixed; bottom:16px; right:16px; display:flex; gap:8px; z-index:5; }
.btn { appearance:none; border:none; padding:9px 14px; border-radius:10px; font-weight:700; color:white; background:#2563eb; cursor:pointer; box-shadow:0 6px 18px rgba(37,99,235,.24); }
.btn.secondary { background:#111827; }

@media print {
  .footer-actions, .toc-sidebar { display:none; }
  body { background:white; padding:0; }
  .header, .section-card { border:none; box-shadow:none; }
}
@media (max-width:980px){
  .layout { grid-template-columns:1fr; }
  .toc-sidebar { position:relative; top:auto; max-height:none; }
}
/* í˜•ê´‘íœ í† ê¸€ */
.hide-hilite mark { background:transparent; box-shadow:none; }
</style>
<script>
function downloadHTML(){
  const b=new Blob([document.documentElement.outerHTML],{type:'text/html'});
  const u=URL.createObjectURL(b); const a=document.createElement('a');
  a.href=u; a.download='polo_easy_explanation___PAPER_ID__.html';
  document.body.appendChild(a); a.click(); a.remove(); URL.revokeObjectURL(u);
}
function toggleHighlights(){
  document.body.classList.toggle('hide-hilite');
  const on=!document.body.classList.contains('hide-hilite');
  document.getElementById('toggleHi').innerText=on?'í˜•ê´‘íœ ë„ê¸°':'í˜•ê´‘íœ ì¼œê¸°';
}
document.addEventListener('DOMContentLoaded',()=>{
  const links=[...document.querySelectorAll('.toc-link')];
  const map=new Map(links.map(a=>[a.getAttribute('href').slice(1),a]));
  const obs=new IntersectionObserver(entries=>{
    entries.forEach(e=>{
      const id=e.target.id, a=map.get(id);
      if(!a) return;
      if(e.isIntersecting){
        links.forEach(x=>x.classList.remove('active'));
        a.classList.add('active');
      }
    });
  },{rootMargin:'0px 0px -70% 0px',threshold:0.1});
  window.__attachObserver=()=>document.querySelectorAll('.section-card[id]').forEach(sec=>obs.observe(sec));
});
</script>
</head>
<body>
<div class="wrapper">
  <div class="header">
    <div class="title">POLO ë…¼ë¬¸ ì´í•´ ë„ìš°ë¯¸ ë³€í™˜ ê²°ê³¼</div>
    <div class="subtitle">ë³µì¡í•œ ë…¼ë¬¸ì„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì¬êµ¬ì„±í•œ ê²°ê³¼</div>
    <div class="meta">ë…¼ë¬¸ ID: __PAPER_ID__ | ìƒì„±: __GEN_AT__</div>
  </div>
"""
    html: List[str] = []
    html.append(html_header.replace("__PAPER_ID__", paper_id).replace("__GEN_AT__", gen_at))

    html.append('<div class="layout">')

    # ===== ì¢Œì¸¡ ê³ ì • TOC =====
    html.append('<aside class="toc-sidebar">')
    html.append('<div class="toc-title">ëª©ì°¨</div>')
    html.append('<ol class="toc-list">')

    section_num = 0
    open_sub = False
    subsection_num = 0

    for i, sec in enumerate(sections):
        title = (sec.get("title") or f"Section {i+1}").strip()
        sid = _unique_slugs[i]
        section_type = sec.get("section_type", "section")
        toc_items.append((sid, title))

        if section_type == "section":
            if open_sub:
                html.append('</ol></li>')
                open_sub = False
            section_num += 1
            subsection_num = 0
            html.append(
                f'<li class="toc-item"><a class="toc-link" href="#{sid}">'
                f'<span class="num">{section_num}.</span>{title}</a>'
            )
            html.append('<ol class="toc-sublist">')
            open_sub = True
        else:
            subsection_num += 1
            html.append(
                f'<li class="toc-item"><a class="toc-link" href="#{sid}">'
                f'<span class="num">{section_num}.{subsection_num}</span>{title}</a></li>'
            )

    if open_sub:
        html.append('</ol></li>')
    html.append('</ol>')
    html.append('</aside>')

    # ===== ìš°ì¸¡ ë³¸ë¬¸ ì‹œì‘ =====
    html.append('<main class="content-area">')

    for i, (sec, res) in enumerate(zip(sections, results)):
        title = (sec.get("title") or f"Section {i+1}").strip()
        sid = toc_items[i][0]
        section_type = sec.get("section_type", "section")

        raw_text = (getattr(res, "easy_text", None) or sec.get("content") or "").strip()
        processed_text = _split_paragraphs_ko(raw_text)
        content_html = _render_rich_html(processed_text)
        header_html = "" if _starts_with_same_heading(content_html, title) else f"<h2>{title}</h2>"

        html.append(
            f'<div class="section-card {section_type}" id="{sid}">{header_html}'
            f'<div class="content">{content_html}</div>'
        )

        # ìƒì„±ëœ ì‹œê°í™” ì´ë¯¸ì§€ í¬í•¨
        if res.ok and res.image_path and Path(res.image_path).exists():
            src_path = Path(res.image_path)
            dst_path = output_path.parent / src_path.name
            try:
                import shutil
                shutil.copy2(src_path, dst_path)
                logger.info(f"ğŸ“Š [EASY] ì‹œê°í™” ì´ë¯¸ì§€ ë³µì‚¬ ì™„ë£Œ: {src_path.name}")
            except Exception as e:
                logger.warning(f"ğŸ“Š [EASY] ì‹œê°í™” ì´ë¯¸ì§€ ë³µì‚¬ ì‹¤íŒ¨: {e}")
                dst_path = Path("../../viz") / paper_id / src_path.name

            html.append(f"""
<div class="image-container">
  <img src="{dst_path.name}" alt="{title} ê´€ë ¨ ì‹œê°í™”" style="max-width:100%; height:auto; border-radius:8px;" />
  <div class="image-caption">ê·¸ë¦¼ {i+1}: {title} ê´€ë ¨ ì‹œê°í™”</div>
</div>""")

        # ì›ë³¸ ë…¼ë¬¸ ì´ë¯¸ì§€ í¬í•¨
        if res.original_images:
            for img_idx, img_info in enumerate(res.original_images):
                src_path = Path(img_info["path"])
                dst_path = output_path.parent / img_info["filename"]
                try:
                    import shutil
                    shutil.copy2(src_path, dst_path)
                    logger.info(f"ğŸ“Š [EASY] ì›ë³¸ ì´ë¯¸ì§€ ë³µì‚¬ ì™„ë£Œ: {img_info['filename']}")
                except Exception as e:
                    logger.warning(f"ğŸ“Š [EASY] ì›ë³¸ ì´ë¯¸ì§€ ë³µì‚¬ ì‹¤íŒ¨: {e}")
                    continue

                caption = img_info.get("caption", f"ì›ë³¸ ë…¼ë¬¸ ê·¸ë¦¼ {img_idx+1}")
                html.append(f"""
<div class="image-container">
  <img src="{img_info['filename']}" alt="{caption}" style="max-width:100%; height:auto; border-radius:8px;" />
  <div class="image-caption">ì›ë³¸ ë…¼ë¬¸ ê·¸ë¦¼ {img_idx+1}: {caption}</div>
</div>""")

        html.append("</div>")

    html.append("""
<div class="footer-actions">
  <button class="btn" onclick="downloadHTML()">HTML ì €ì¥</button>
  <button class="btn secondary" id="toggleHi" onclick="toggleHighlights()">í˜•ê´‘íœ ë„ê¸°</button>
</div>
""")

    html.append('</main></div>')
    html.append("<script>if(window.__attachObserver) window.__attachObserver();</script>")
    html.append("</div></body></html>")

    output_path.write_text("".join(html), encoding="utf-8")

# ---------- LaTeX íŒŒì„œ & í‘œ ì¶”ì¶œ ----------
def _extract_table_data(text: str) -> List[dict]:
    tables = []
    pat = r'\\begin\{tabular\}[^{]*\{([^}]+)\}(.*?)\\end\{tabular\}'
    for m in re.finditer(pat, text, re.DOTALL):
        content = m.group(2)
        lines = [ln.strip() for ln in content.splitlines() if ln.strip() and not ln.strip().startswith('\\hline')]
        if len(lines) < 2:
            continue
        headers = [h.strip().rstrip('\\') for h in lines[0].split('&')]
        rows = []
        for ln in lines[1:]:
            if '&' in ln:
                row = [c.strip().rstrip('\\') for c in ln.split('&')]
                if len(row) == len(headers):
                    rows.append(row)
        if not rows:
            continue
        tables.append({"type": "metric_table", "headers": headers, "rows": rows})
    return tables

def _parse_latex_sections(tex_path: Path) -> List[dict]:
    content = tex_path.read_text(encoding="utf-8", errors="ignore")
    sections: List[dict] = []
    cur_title = None
    cur_buf: List[str] = []
    cur_raw: List[str] = []
    section_type = "section"

    def _flush():
        if cur_title is None:
            return
        raw_txt = "\n".join(cur_raw).strip()
        clean = _clean_latex_text("\n".join(cur_buf))
        sections.append({
            "index": len(sections),
            "title": cur_title,
            "content": clean,
            "raw_content": raw_txt,
            "table_data": _extract_table_data(raw_txt),
            "section_type": section_type,
        })

    sec_pat  = re.compile(r"^\s*\\section\*?\{([^}]+)\}\s*$")
    sub_pat  = re.compile(r"^\s*\\subsection\*?\{([^}]+)\}\s*$")
    abs_beg  = re.compile(r"^\s*\\begin\{abstract\}")
    abs_end  = re.compile(r"^\s*\\end\{abstract\}")

    in_abstract = False
    lines = content.splitlines()
    for ln in lines:
        if abs_beg.match(ln):
            _flush(); cur_title = "Abstract"; cur_buf, cur_raw = [], []; section_type = "section"; in_abstract = True; continue
        if abs_end.match(ln):
            _flush(); cur_title = None; cur_buf, cur_raw = [], []; in_abstract = False; continue

        m1 = sec_pat.match(ln)
        m2 = sub_pat.match(ln)
        if m1:
            _flush(); cur_title = m1.group(1).strip(); cur_buf, cur_raw = [], []; section_type = "section"; continue
        if m2:
            _flush(); cur_title = m2.group(1).strip(); cur_buf, cur_raw = [], []; section_type = "subsection"; continue

        if cur_title is None:
            if not any(k in ln.lower() for k in ["\\title", "\\author", "\\date", "\\maketitle"]):
                cur_title = "Introduction"; section_type = "section"
        if cur_title is not None:
            cur_buf.append(ln); cur_raw.append(ln)

    _flush()

    if not sections:
        clean = _clean_latex_text(content)
        sections = [{
            "index": 0, "title": "Full Document", "content": clean, "raw_content": content,
            "table_data": _extract_table_data(content), "section_type": "section",
        }]

    logger.info(f"[EASY] Parsed {len(sections)} sections from LaTeX")
    return sections

# ---------- (ì˜µì…˜) ì‹œê°í™” ì—”ì§„ í˜¸ì¶œ ----------
def _httpx_client(timeout_total: float = 1200.0) -> httpx.AsyncClient:
    """
    HTTP/2 ë„ê³ , keep-alive ë„ê³ , ì½ê¸° íƒ€ì„ì•„ì›ƒì€ total ê°’ìœ¼ë¡œ ì„¤ì •.
    Windows ì†Œì¼“ ì¢…ë£Œ ì´ìŠˆ ì™„í™” ëª©ì ì˜ ì„¤ì • í¬í•¨.
    """
    try:
        to = httpx.Timeout(timeout_total)
    except TypeError:
        to = timeout_total
    return httpx.AsyncClient(
        timeout=to,
        http2=False,
        headers={"Connection": "close"},
        limits=httpx.Limits(max_keepalive_connections=0, max_connections=20),
        follow_redirects=False,
    )

async def _send_to_viz(
    paper_id: str,
    index: int,
    easy_text_ko: str,
    out_dir: Path,
    table_data: List[dict] = None,
    *,
    health_checked: bool = False
) -> VizResult:
    if not EASY_VIZ_ENABLED:
        return VizResult(ok=True, index=index, image_path=None, easy_text=easy_text_ko)

    if not health_checked:
        try:
            async with _httpx_client(EASY_VIZ_HEALTH_TIMEOUT) as client:
                r = await client.get(f"{VIZ_MODEL_URL.rstrip('/')}/health")
                if r.status_code != 200:
                    return VizResult(ok=True, index=index, image_path=None, easy_text=easy_text_ko)
        except Exception:
            return VizResult(ok=True, index=index, image_path=None, easy_text=easy_text_ko)

    payload = {
        "paper_id": paper_id,
        "index": index,
        "rewritten_text": easy_text_ko,
        "target_lang": "ko",
        "bilingual": "missing",
        "text_type": "easy_korean",
    }
    if table_data:
        payload["tables"] = table_data

    try:
        async with _httpx_client(EASY_VIZ_TIMEOUT) as client:
            r = await client.post(f"{VIZ_MODEL_URL.rstrip('/')}/viz", json=payload)
            if r.status_code != 200:
                return VizResult(ok=True, index=index, image_path=None, easy_text=easy_text_ko)

            data = r.json()
            if data.get("image_base64"):
                img_path = out_dir / f"{index:06d}.png"
                img_path.write_bytes(base64.b64decode(data["image_base64"]))
                return VizResult(ok=True, index=index, image_path=str(img_path), easy_text=easy_text_ko)

            if data.get("image_path"):
                return VizResult(ok=True, index=index, image_path=data["image_path"], easy_text=easy_text_ko)

            return VizResult(ok=True, index=index, image_path=None, easy_text=easy_text_ko)

    except Exception as e:
        logger.warning(f"[EASY] viz error: {e}")
        return VizResult(ok=False, index=index, image_path=None, easy_text=easy_text_ko)

# === PART 3/4 END ===

# === PART 4/4 START ===
# -------------------- Endpoints (patched) --------------------
# ì‘ì€ í—¬í¼ë“¤ (ì´ ì„¹ì…˜ ì•ˆì—ì„œë§Œ ì“°ì„)
MAX_INPUT_TOKENS = 2048
_RETRY_TOKENS = (1536, 1024, 768)

def _need_ko(text: str) -> bool:
    if not text or not text.strip():
        return False
    hangul = len(re.findall(r"[ê°€-í£]", text or ""))
    latin  = len(re.findall(r"[A-Za-z]", text or ""))
    total  = hangul + latin
    
    # í•œêµ­ì–´ê°€ ê±°ì˜ ì—†ìœ¼ë©´ ë²ˆì—­ í•„ìš”
    if hangul < 5:
        return True
    
    # í•œêµ­ì–´ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ ë²ˆì—­ í•„ìš”
    if total > 0 and hangul / total < 0.3:
        return True
        
    return False

def _safe_tokenize(prompt: str, max_len: int = MAX_INPUT_TOKENS):
    for L in (max_len, *_RETRY_TOKENS):
        enc = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=L)
        if enc["input_ids"].shape[1] <= L:
            return {k: v.to(device) for k, v in enc.items()}
    # ìµœí›„ìˆ˜ë‹¨: ë’¤ 8000ìë§Œ ì‚¬ìš©
    enc = tokenizer(prompt[-8000:], return_tensors="pt", truncation=True, max_length=max_len)
    return {k: v.to(device) for k, v in enc.items()}

def _merge_with_schema(data: dict, schema: dict) -> dict:
    if not isinstance(data, dict):
        return json.loads(json.dumps(schema))
    out = json.loads(json.dumps(schema))  # deep copy
    for k, v in data.items():
        if k in out and isinstance(out[k], dict) and isinstance(v, dict):
            out[k] = _merge_with_schema(v, out[k])
        else:
            out[k] = v
    return out

@app.get("/")
async def root():
    return {"message": "POLO Easy Model API", "model": BASE_MODEL}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if (model is not None and tokenizer is not None) else "unavailable",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None,
        "gpu_available": gpu_available,
        "gpu_device": torch.cuda.get_device_name(0) if gpu_available else None,
        "model_name": BASE_MODEL,
        "max_new_tokens": MAX_NEW_TOKENS,
        "cache_dir": str(CACHE_DIR),
        # ì¶”ê°€ ì§„ë‹¨ ì •ë³´
        "torch": torch.__version__,
        "cuda": (torch.version.cuda if torch.cuda.is_available() else None),
        "dtype": str(getattr(getattr(model, "dtype", None), "name", getattr(model, "dtype", None))),
    }

@app.get("/healthz")
async def healthz():
    return await health()

@app.post("/easy", response_model=TextResponse)
async def simplify_text(request: TextRequest):
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    style = (request.style or "default")
    simplified_text = await _rewrite_text(request.text, style=style)

    # âœ… ë¬´ì¡°ê±´ í•œê¸€ í´ë°±(í•œê¸€ ë¹„ìœ¨ ë‚®ìœ¼ë©´ ê°•ì œ ë²ˆì—­)
    was_translated = False
    if _need_ko(simplified_text):
        simplified_text = _ensure_korean(simplified_text)
        was_translated = True

    # TextResponse ìŠ¤í‚¤ë§ˆì— metaê°€ ì—†ë‹¤ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜(í˜¸í™˜ì„± ìœ ì§€)
    # í•„ìš”í•˜ë©´ translated_text ìë¦¬ì— í”Œë˜ê·¸/ì›ë¬¸ ë²ˆì—­ë³¸ì„ ë„£ë„ë¡ ëª¨ë¸ì„ í™•ì¥í•˜ì„¸ìš”.
    return TextResponse(simplified_text=simplified_text, translated_text=None)

@app.post("/generate")
async def generate_json(request: TextRequest):
    start_total = time.time()
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    # ê°„ì´ ì„¹ì…˜ ì¶”ì¶œ(ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    def _extract_sections(src: str) -> dict:
        sections = {k: "" for k in ["abstract","introduction","methods","results","discussion","conclusion"]}
        headers = [
            ("abstract", r"^\s*abstract\b[:\-]?"),
            ("introduction", r"^\s*introduction\b[:\-]?"),
            ("methods", r"^\s*methods?\b[:\-]?|^\s*materials?\s+and\s+methods\b[:\-]?"),
            ("results", r"^\s*results?\b[:\-]?"),
            ("discussion", r"^\s*discussion\b[:\-]?"),
            ("conclusion", r"^\s*conclusion[s]?\b[:\-]?|^\s*concluding\s+remarks\b[:\-]?"),
        ]
        lines = src.splitlines(); idxs = []
        for i, line in enumerate(lines):
            for key, pat in headers:
                if re.match(pat, line.strip(), flags=re.IGNORECASE):
                    idxs.append((i, key)); break
        idxs.sort()
        for j, (start_i, key) in enumerate(idxs):
            end_i = idxs[j+1][0] if j+1 < len(idxs) else len(lines)
            chunk = "\n".join(lines[start_i+1:end_i]).strip()[:2000]
            if EASY_STRIP_MATH: chunk = _strip_math_all(chunk)
            chunk = _strip_tables_figures_all(chunk)
            sections[key] = chunk
        return sections

    extracted = _extract_sections(request.text)

    # ìŠ¤í‚¤ë§ˆ(ê¸°ë³¸í˜•)
    GROUND_SCHEMA = {
        "title": "",
        "authors": [],
        "abstract": {"original": "", "easy": ""},
        "introduction": {"original": "", "easy": ""},
        "methods": {"original": "", "easy": ""},
        "results": {"original": "", "easy": ""},
        "discussion": {"original": "", "easy": ""},
        "conclusion": {"original": "", "easy": ""},
        "keywords": [],
        "figures_tables": [],
        "references": [],
        "contributions": [],
        "limitations": [],
        "glossary": [],
        "plain_summary": "",
    }
    data_schema = json.loads(json.dumps(GROUND_SCHEMA))
    for k in ["abstract","introduction","methods","results","discussion","conclusion"]:
        data_schema[k]["original"] = extracted.get(k, "")

    instruction = (
        "ë„ˆëŠ” ì¹œê·¼í•œ ê³¼í•™ ì„ ìƒë‹˜ì´ë‹¤. ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆì˜ 'í‚¤/êµ¬ì¡°'ë¥¼ ê·¸ëŒ€ë¡œ ë‘ê³  'ê°’'ë§Œ ì±„ì›Œë¼. "
        "ì™¸ë¶€ ì§€ì‹/ì¶”ì¸¡ ê¸ˆì§€. ì›ë¬¸ì— ì—†ìœ¼ë©´ 'ì›ë¬¸ì— ì—†ìŒ'ì´ë¼ê³  ì ì–´ë¼. "
        "ì¶œë ¥ì€ ì˜¤ì§ 'ìœ íš¨í•œ JSON' í•˜ë‚˜ë§Œ í—ˆìš©ëœë‹¤."
    )
    schema_str = json.dumps(data_schema, ensure_ascii=False, indent=2)
    context_only = {k: extracted[k] for k in ["abstract","introduction","methods","results","discussion","conclusion"]}
    context_str = json.dumps(context_only, ensure_ascii=False, indent=2)
    prompt = f"{instruction}\n\n=== ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ===\n{schema_str}\n\n=== ì„¹ì…˜ ì›ë¬¸ ===\n{context_str}\n\n[OUTPUT]\n"

    # âœ… ì•ˆì „ í† í¬ë‚˜ì´ì¦ˆ(ë°±ì˜¤í”„)
    inputs = _safe_tokenize(prompt, max_len=MAX_INPUT_TOKENS)

    t0 = time.time()
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            use_cache=True,
            repetition_penalty=1.2,   # === Fix: ë°˜ë³µ ì–µì œ
            no_repeat_ngram_size=4,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    inference_time = time.time() - t0

    # === token-based slicing ===
    seq = outputs[0]
    inp_len = inputs["input_ids"].shape[1]
    gen_tokens = seq[inp_len:]
    raw = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()

    # === stop cut ===
    for stop in ("[/OUTPUT]", "[/output]", "<|eot_id|>"):
        p = raw.find(stop)
        if p != -1:
            raw = raw[:p].strip()
            break

    def _is_meaningful(d: dict) -> bool:
        try:
            sections = ["abstract","introduction","methods","results","discussion","conclusion"]
            return any(len((d.get(s, {}) or {}).get("easy", "")) > 10 for s in sections)
        except Exception:
            return False

    # 1ì°¨ íŒŒì‹±
    try:
        data = _coerce_json(raw)
        if not _is_meaningful(data):
            raise ValueError("empty_json")
    except Exception:
        # 2ì°¨ ì—„ê²© í”„ë¡¬í”„íŠ¸
        strict_instruction = (
            "ìŠ¤í‚¤ë§ˆì˜ í‚¤/êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³  ê°’ë§Œ ì±„ìš´ 'ìœ íš¨í•œ JSON'ë§Œ ì¶œë ¥í•˜ë¼. ë°˜ë“œì‹œ '{'ë¡œ ì‹œì‘í•´ '}'ë¡œ ëë‚˜ì•¼ í•œë‹¤."
        )
        strict_prompt = f"{strict_instruction}\n\nìŠ¤í‚¤ë§ˆ:\n{schema_str}\n\nì„¹ì…˜ ì›ë¬¸:\n{context_str}\n\n[OUTPUT]\n"
        inputs2 = _safe_tokenize(strict_prompt, max_len=MAX_INPUT_TOKENS)
        with torch.inference_mode():
            outputs2 = model.generate(
                **inputs2,
                max_new_tokens=min(MAX_NEW_TOKENS, 1000),
                do_sample=False,
                use_cache=True,
                repetition_penalty=1.2,
                no_repeat_ngram_size=4,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        seq2 = outputs2[0]
        inp_len2 = inputs2["input_ids"].shape[1]
        gen_tokens2 = seq2[inp_len2:]
        raw2 = tokenizer.decode(gen_tokens2, skip_special_tokens=True).strip()
        for stop in ("[/OUTPUT]", "[/output]", "<|eot_id|>"):
            p = raw2.find(stop)
            if p != -1:
                raw2 = raw2[:p].strip()
                break
        try:
            data = _coerce_json(raw2)
        except Exception:
            data = data_schema

    # âœ… ìŠ¤í‚¤ë§ˆ ë³´ì •(ëˆ„ë½ í‚¤ ì±„ì›€)
    data = _merge_with_schema(data, data_schema)

    total_time = time.time() - start_total
    data["processing_info"] = {
        "gpu_used": gpu_available,
        "inference_time": inference_time,
        "total_time": total_time,
        "max_new_tokens": MAX_NEW_TOKENS,
    }

    # postprocess easy fields (+ í•„ìš” ì‹œ í•œêµ­ì–´ í´ë°±)
    for k in ["abstract","introduction","methods","results","discussion","conclusion"]:
        try:
            easy_val = (data.get(k, {}) or {}).get("easy", "")
            if isinstance(easy_val, str):
                t = _postprocess_terms(_normalize_bracket_tokens(easy_val))
                if EASY_STRIP_MATH: t = _strip_math_all(t)
                t = _strip_tables_figures_all(t)
                t = _auto_bold_terms(t)
                t = _hilite_sentences(t, max_marks=2)
                t = _sanitize_repeats(t)
                if _need_ko(t):
                    t = _ensure_korean(t)
                    data.setdefault("_meta", {})["easy_was_translated"] = True
                data[k]["easy"] = t
        except Exception:
            pass

    return data
# -------------------- /Endpoints (patched) --------------------

# ====================== BATCH / TRANSPORT / VIZ (Optimized for KO-Easy) ======================

# ENV defaults (if not defined above)
EASY_VIZ_ENABLED = os.getenv("EASY_VIZ_ENABLED", "0").lower() in ("1", "true", "yes")

# ---------- LaTeX íŒŒì„œ: section/subsection ì¶”ì¶œ + í‘œ ë¼ì´íŠ¸ íŒŒì‹± ----------
def _extract_table_data(text: str) -> List[dict]:
    """LaTeX tabularì—ì„œ ê°„ë‹¨ ë©”íŠ¸ë¦­ í…Œì´ë¸”ì„ ì¶”ì¶œ (ì˜µì…˜)"""
    tables = []
    pat = r'\\begin\{tabular\}[^{]*\{([^}]+)\}(.*?)\\end\{tabular\}'
    for m in re.finditer(pat, text, re.DOTALL):
        content = m.group(2)
        lines = [ln.strip() for ln in content.splitlines() if ln.strip() and not ln.strip().startswith('\\hline')]
        if len(lines) < 2:  # header + at least 1 row
            continue
        headers = [h.strip().rstrip('\\') for h in lines[0].split('&')]
        rows = []
        for ln in lines[1:]:
            if '&' in ln:
                row = [c.strip().rstrip('\\') for c in ln.split('&')]
                if len(row) == len(headers):
                    rows.append(row)
        if not rows: 
            continue
        tables.append({
            "type": "metric_table",
            "headers": headers,
            "rows": rows,
        })
    return tables

def _parse_latex_sections(tex_path: Path) -> List[dict]:
    """
    merged_body.tex ê°™ì€ LaTeX ë³¸ë¬¸ì„ ì½ì–´
    - \\section{}, \\subsection{} ë‹¨ìœ„ë¡œ ë¶„í• 
    - section_type: section | subsection
    - ê° ì„¹ì…˜ raw_contentì—ì„œ í‘œ ì •ë³´(ì˜µì…˜) ì¶”ì¶œ
    """
    content = tex_path.read_text(encoding="utf-8", errors="ignore")

    sections: List[dict] = []
    cur_title = None
    cur_buf: List[str] = []
    cur_raw: List[str] = []
    section_type = "section"

    def _flush():
        if cur_title is None:
            return
        raw_txt = "\n".join(cur_raw).strip()
        clean = _clean_latex_text("\n".join(cur_buf))
        sections.append({
            "index": len(sections),
            "title": cur_title,
            "content": clean,
            "raw_content": raw_txt,
            "table_data": _extract_table_data(raw_txt),
            "section_type": section_type,
        })

    # header regex
    sec_pat  = re.compile(r"^\s*\\section\*?\{([^}]+)\}\s*$")
    sub_pat  = re.compile(r"^\s*\\subsection\*?\{([^}]+)\}\s*$")
    abs_beg  = re.compile(r"^\s*\\begin\{abstract\}")
    abs_end  = re.compile(r"^\s*\\end\{abstract\}")

    in_abstract = False
    lines = content.splitlines()
    for ln in lines:
        # abstract
        if abs_beg.match(ln):
            _flush()
            cur_title = "Abstract"
            cur_buf, cur_raw = [], []
            section_type = "section"
            in_abstract = True
            continue
        if abs_end.match(ln):
            _flush()
            cur_title = None
            cur_buf, cur_raw = [], []
            in_abstract = False
            continue

        # section/subsection
        m1 = sec_pat.match(ln)
        m2 = sub_pat.match(ln)
        if m1:
            _flush()
            cur_title = m1.group(1).strip()
            cur_buf, cur_raw = [], []
            section_type = "section"
            continue
        if m2:
            _flush()
            cur_title = m2.group(1).strip()
            cur_buf, cur_raw = [], []
            section_type = "subsection"
            continue

        # collect
        if cur_title is None:
            # ë¬¸ì„œ ì‹œì‘ë¶€(íƒ€ì´í‹€/ì €ì) ìŠ¤í‚µ
            if not any(k in ln.lower() for k in ["\\title", "\\author", "\\date", "\\maketitle"]):
                # ì²« ì„¹ì…˜ ì—†ìœ¼ë©´ ì´ˆë°˜ë¶€ë¥¼ Introductionìœ¼ë¡œ ìŠ¹ê²©
                cur_title = "Introduction"
                section_type = "section"
        if cur_title is not None:
            cur_buf.append(ln)
            cur_raw.append(ln)

    _flush()

    if not sections:
        # fallback: ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì„¹ì…˜ìœ¼ë¡œ
        clean = _clean_latex_text(content)
        sections = [{
            "index": 0,
            "title": "Full Document",
            "content": clean,
            "raw_content": content,
            "table_data": _extract_table_data(content),
            "section_type": "section",
        }]

    logger.info(f"[EASY] Parsed {len(sections)} sections from LaTeX")
    return sections

# ---------- (ì˜µì…˜) ì‹œê°í™” ì—”ì§„ í˜¸ì¶œ ----------

async def _send_to_viz(
    paper_id: str,
    index: int,
    easy_text_ko: str,
    out_dir: Path,
    table_data: List[dict] = None,
    *,
    health_checked: bool = False
) -> VizResult:
    """
    ì‹œê°í™” ì„œë²„ë¡œ í…ìŠ¤íŠ¸ ì „ì†¡. ë°°ì¹˜ ì‹œì‘ ì‹œ 1íšŒ health ì²´í¬ ê²°ê³¼ë¥¼ ë„˜ê²¨ë°›ì•„
    ì„¹ì…˜ë§ˆë‹¤ ë°˜ë³µ health í•‘ì„ ì—†ì•¤ë‹¤.
    """
    if not EASY_VIZ_ENABLED:
        return VizResult(ok=True, index=index, image_path=None, easy_text=easy_text_ko)

    # ë°°ì¹˜ì—ì„œ ì´ë¯¸ health í™•ì¸í–ˆë‹¤ë©´ ìƒëµ
    if not health_checked:
        try:
            async with _httpx_client(EASY_VIZ_HEALTH_TIMEOUT) as client:
                r = await client.get(f"{VIZ_MODEL_URL.rstrip('/')}/health")
                if r.status_code != 200:
                    return VizResult(ok=True, index=index, image_path=None, easy_text=easy_text_ko)
        except Exception:
            return VizResult(ok=True, index=index, image_path=None, easy_text=easy_text_ko)

    payload = {
        "paper_id": paper_id,
        "index": index,
        "rewritten_text": easy_text_ko,
        "target_lang": "ko",
        "bilingual": "missing",
        "text_type": "easy_korean",
    }
    if table_data:
        payload["tables"] = table_data

    try:
        async with _httpx_client(EASY_VIZ_TIMEOUT) as client:
            r = await client.post(f"{VIZ_MODEL_URL.rstrip('/')}/viz", json=payload)
            if r.status_code != 200:
                return VizResult(ok=True, index=index, image_path=None, easy_text=easy_text_ko)

            data = r.json()
            if data.get("image_base64"):
                img_path = out_dir / f"{index:06d}.png"
                img_path.write_bytes(base64.b64decode(data["image_base64"]))
                return VizResult(ok=True, index=index, image_path=str(img_path), easy_text=easy_text_ko)

            if data.get("image_path"):
                return VizResult(ok=True, index=index, image_path=data["image_path"], easy_text=easy_text_ko)

            return VizResult(ok=True, index=index, image_path=None, easy_text=easy_text_ko)

    except Exception as e:
        logger.warning(f"[EASY] viz error: {e}")
        return VizResult(ok=False, index=index, image_path=None, easy_text=easy_text_ko)


@app.post("/batch", response_model=BatchResult)
async def run_batch(request: BatchRequest, assets_metadata: Optional[Dict[str, Any]] = None):
    """
    ì…ë ¥:
      - request.chunks_jsonl: JSONL ë¬¸ìì—´ or íŒŒì¼/ë””ë ‰í† ë¦¬(.jsonl/.jsonl.gz/.tex)
    ì²˜ë¦¬:
      1) ì„¹ì…˜ íŒŒì‹±
      2) ì„¹ì…˜ë³„ ì‰¬ìš´ í•œêµ­ì–´ ë³€í™˜
      3) (ì˜µì…˜) ì‹œê°í™” í˜¸ì¶œ (ë°°ì¹˜ ì‹œì‘ ì‹œ 1íšŒ health í™•ì¸)
      4) ê²°ê³¼ ì €ì¥
    """
    t_batch_start = time.time()

    paper_id = request.paper_id.strip()
    base_out = Path(request.output_dir).expanduser().resolve()
    out_dir  = base_out / paper_id
    out_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"[EASY] output dir: {out_dir}")

    # --- ì…ë ¥ ì†ŒìŠ¤ ë¡œë”© (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---
    src = (request.chunks_jsonl or "").strip()
    src_path: Optional[Path] = None
    if src:
        p = Path(src)
        if p.exists():
            src_path = p.resolve()

    sections: List[dict] = []

    def _append_from_jsonl_lines(lines: List[str]):
        for idx, ln in enumerate(lines):
            try:
                obj = json.loads(ln)
            except Exception as e:
                obj = {"title": f"Section {idx+1}", "text": f"[JSONL parse error] {e}"}
            sections.append({
                "index": idx,
                "title": obj.get("title") or f"Section {idx+1}",
                "content": obj.get("text") or "",
                "raw_content": obj.get("text") or "",
                "table_data": [],
                "section_type": "section",
            })

    def _load_assets_metadata(source_dir: Path) -> Dict[str, Any]:
        """assets.jsonlì—ì„œ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        assets_file = source_dir / "assets.jsonl"
        if not assets_file.exists():
            return {}
        
        assets = {}
        try:
            with open(assets_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        asset = json.loads(line)
                        if asset.get("kind") == "figure" and asset.get("graphics"):
                            assets[asset["id"]] = asset
        except Exception as e:
            logger.warning(f"[EASY] assets.jsonl ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return assets

    def _find_related_images(section_content: str, assets_metadata: Dict[str, Any], source_dir: Path) -> List[Dict[str, Any]]:
        """ì„¹ì…˜ ë‚´ìš©ì—ì„œ ê´€ë ¨ ì´ë¯¸ì§€ ì°¾ê¸°"""
        related_images = []
        
        # ì„¹ì…˜ ë‚´ìš©ì—ì„œ figure ì°¸ì¡° ì°¾ê¸° (ì—¬ëŸ¬ íŒ¨í„´ ì§€ì›)
        fig_refs = []
        
        # 1. LaTeX \ref{...} íŒ¨í„´
        fig_refs.extend(re.findall(r'\\ref\{([^}]+)\}', section_content))
        
        # 2. í”Œë ˆì´ìŠ¤í™€ë” âŸ¨FIG:...âŸ© íŒ¨í„´
        fig_refs.extend(re.findall(r'âŸ¨FIG:([^âŸ©]+)âŸ©', section_content))
        
        # 3. ê¹¨ì§„ í”Œë ˆì´ìŠ¤í™€ë” íŒ¨í„´ (ì¸ì½”ë”© ë¬¸ì œ ëŒ€ì‘)
        fig_refs.extend(re.findall(r'[?ì­²]IG:([^?]+)', section_content))
        
        # 4. ë” êµ¬ì²´ì ì¸ ê¹¨ì§„ íŒ¨í„´ë“¤
        fig_refs.extend(re.findall(r'[?ì­²]IG:fig:([^?]+)', section_content))
        fig_refs.extend(re.findall(r'[?ì­²]IG:([^?]+)??', section_content))
        
        for ref in fig_refs:
            if ref in assets_metadata:
                asset = assets_metadata[ref]
                for graphic_path in asset.get("graphics", []):
                    # ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
                    full_path = source_dir / graphic_path
                    if not full_path.exists():
                        # í™•ì¥ì ì—†ì´ ì°¾ê¸°
                        for ext in ['.pdf', '.jpg', '.jpeg', '.png', '.eps']:
                            test_path = source_dir / f"{graphic_path}{ext}"
                            if test_path.exists():
                                full_path = test_path
                                break
                    
                    if full_path.exists():
                        related_images.append({
                            "id": asset["id"],
                            "path": str(full_path),
                            "caption": asset.get("caption", ""),
                            "label": asset.get("label", ""),
                            "filename": full_path.name
                        })
        
        return related_images

    # assets ë©”íƒ€ë°ì´í„° ë¡œë“œ (ì „ë‹¬ë°›ì€ ê²ƒì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¡œë“œ)
    if assets_metadata is None:
        assets_metadata = {}
        if src_path and src_path.is_dir():
            assets_metadata = _load_assets_metadata(src_path)
            logger.info(f"[EASY] ë¡œë“œëœ ì´ë¯¸ì§€ ìì‚°: {len(assets_metadata)}ê°œ")
        elif src_path and src_path.suffix.lower() == ".tex":
            # .tex íŒŒì¼ì´ ì§ì ‘ ì…ë ¥ëœ ê²½ìš°, ë¶€ëª¨ ë””ë ‰í† ë¦¬ì—ì„œ assets.jsonl ì°¾ê¸°
            assets_metadata = _load_assets_metadata(src_path.parent)
            logger.info(f"[EASY] ë¡œë“œëœ ì´ë¯¸ì§€ ìì‚°: {len(assets_metadata)}ê°œ")
    else:
        logger.info(f"[EASY] ì „ë‹¬ë°›ì€ ì´ë¯¸ì§€ ìì‚°: {len(assets_metadata)}ê°œ")

    try:
        if src_path is None:
            lines = [ln for ln in src.splitlines() if ln.strip()]
            _append_from_jsonl_lines(lines)
        else:
            if src_path.is_dir():
                tex = src_path / "merged_body.tex"
                if tex.exists():
                    sections = _parse_latex_sections(tex)
                else:
                    hits = sorted(src_path.rglob("*.jsonl"))
                    if not hits:
                        raise ValueError("ë””ë ‰í† ë¦¬ì— merged_body.tex ë˜ëŠ” *.jsonl ì—†ìŒ")
                    # ëª…ì‹œì ìœ¼ë¡œ ì •ë ¬ ì²« íŒŒì¼ ì‚¬ìš©
                    lines = hits[0].read_text(encoding="utf-8", errors="ignore").splitlines()
                    _append_from_jsonl_lines([ln for ln in lines if ln.strip()])
            else:
                suf = src_path.suffix.lower()
                if suf in (".jsonl", ".ndjson"):
                    lines = src_path.read_text(encoding="utf-8", errors="ignore").splitlines()
                    _append_from_jsonl_lines([ln for ln in lines if ln.strip()])
                elif suf == ".gz":
                    with gzip.open(src_path, "rt", encoding="utf-8") as f:
                        lines = [ln for ln in f if ln.strip()]
                    _append_from_jsonl_lines(lines)
                elif suf == ".tex":
                    sections = _parse_latex_sections(src_path)
                else:
                    raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ í˜•ì‹ (jsonl/jsonl.gz/tex/dir)")
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"ì…ë ¥ íŒŒì‹± ì‹¤íŒ¨: {e}")

    if not sections:
        raise HTTPException(status_code=400, detail="ì²˜ë¦¬í•  ì„¹ì…˜ ì—†ìŒ")

    logger.info(f"[EASY] batch sections = {len(sections)} (paper_id={paper_id})")

    # --- (ì¤‘ìš”) ì‹œê°í™” health 1íšŒë§Œ ---
    viz_health_ok = False
    if EASY_VIZ_ENABLED:
        try:
            async with _httpx_client(EASY_VIZ_HEALTH_TIMEOUT) as client:
                r = await client.get(f"{VIZ_MODEL_URL.rstrip('/')}/health")
                viz_health_ok = (r.status_code == 200)
        except Exception:
            viz_health_ok = False

    # --- ì„¹ì…˜ë³„ ì²˜ë¦¬ ---
    sem = anyio.Semaphore(EASY_CONCURRENCY)
    results: List[VizResult] = [VizResult(ok=False, index=i) for i in range(len(sections))]

    # ì„¹ì…˜ë³„ ì²˜ë¦¬ ì‹œê°„(ms)ê³¼ ì—ëŸ¬ ìš”ì•½(ì €ì¥ JSONìš©; ì‘ë‹µ ëª¨ë¸ì—ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ)
    section_times_ms: List[float] = [0.0] * len(sections)
    error_briefs: List[str] = []

    async def _work(i: int, section: dict):
        async with sem:
            total = len(sections)
            title = section.get("title") or f"Section {i+1}"
            logger.info(f"[EASY] â–¶ [{i+1}/{total}] section START: {title}")

            sec_t0 = time.time()
            try:
                context_info = f"ì´ ì„¹ì…˜ì€ ì „ì²´ {total}ê°œ ì¤‘ {i+1}ë²ˆì§¸ì…ë‹ˆë‹¤. "
                if i > 0:
                    prev_title = sections[i-1].get("title", "ì´ì „ ì„¹ì…˜")
                    context_info += f"ì´ì „: {prev_title}. "
                if i < total - 1:
                    next_title = sections[i+1].get("title", "ë‹¤ìŒ ì„¹ì…˜")
                    context_info += f"ë‹¤ìŒ: {next_title}. "

                section_type = section.get("section_type", "section")
                if section_type == "subsection":
                    context_info += "ì´ê²ƒì€ ì„œë¸Œì„¹ì…˜ìœ¼ë¡œ, ìƒìœ„ ë‚´ìš©ì„ ì„¸ë¶€ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤. "
                else:
                    context_info += "ì´ê²ƒì€ ë©”ì¸ ì„¹ì…˜ì…ë‹ˆë‹¤. "

                easy_text = await _rewrite_text(
                    section.get("content", ""),
                    title,
                    context_info,
                    style=(request.style or os.getenv("EASY_DEFAULT_STYLE", "three_para_ko"))
                )

                # ê°•ì œë¡œ í•œêµ­ì–´ ë²ˆì—­ ì ìš©
                easy_text = _ensure_korean(easy_text)

                logger.info(f"[EASY] âœ” text rewritten for section {i+1}")
            except Exception as e:
                msg = f"rewrite ì‹¤íŒ¨: {e}"
                logger.error(f"[EASY] âŒ ì„¹ì…˜ ë³€í™˜ ì‹¤íŒ¨ idx={i}: {e}")
                results[i] = VizResult(ok=False, index=i, error=msg, section_title=title)
                error_briefs.append(f"[{i+1}] {title}: {e}")
                section_times_ms[i] = (time.time() - sec_t0) * 1000.0
                return

            # (ì˜µì…˜) í‘œ ë°ì´í„° í¬í•¨
            section_table_data = sections[i].get('table_data', []) if i < total else []

            # ì›ë³¸ ë…¼ë¬¸ ì´ë¯¸ì§€ ì°¾ê¸°
            related_images = []
            if src_path:
                source_dir = src_path if src_path.is_dir() else src_path.parent
                related_images = _find_related_images(section.get("content", ""), assets_metadata, source_dir)
                if related_images:
                    logger.info(f"[EASY] ì„¹ì…˜ {i+1}ì—ì„œ {len(related_images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")

            # VIZ í˜¸ì¶œ (ë¹„í™œì„±í™” ì‹œ ë‚´ë¶€ì—ì„œ no-op ì‘ë‹µ)
            viz_res = await _send_to_viz(
                paper_id, i, easy_text, out_dir, section_table_data,
                health_checked=viz_health_ok
            )
            viz_res.section_title = title
            if viz_res.ok and not viz_res.easy_text:
                viz_res.easy_text = easy_text
            
            # ì›ë³¸ ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
            viz_res.original_images = related_images
            results[i] = viz_res

            sec_elapsed = time.time() - sec_t0
            section_times_ms[i] = sec_elapsed * 1000.0
            has_img = "img" if (viz_res.ok and viz_res.image_path) else "no-img"
            logger.info(f"[EASY] â¹ [{i+1}/{total}] section DONE: {title} elapsed={sec_elapsed:.2f}s {has_img}")

    # --- (ì¤‘ìš”) ì‘ì—…ê·¸ë£¹ì€ 'í•œ ë²ˆë§Œ' + íƒ€ì„ì•„ì›ƒ ê°ì‹¸ê¸° ---
# --- (ì¤‘ìš”) ì‘ì—…ê·¸ë£¹ì€ 'í•œ ë²ˆë§Œ' + íƒ€ì„ì•„ì›ƒ ê°ì‹¸ê¸° ---
    timed_out = False
    try:
        if EASY_BATCH_TIMEOUT and EASY_BATCH_TIMEOUT > 0:
            # â›ï¸ FIX: async with  â†’  with
            with anyio.fail_after(EASY_BATCH_TIMEOUT):
                async with anyio.create_task_group() as tg:
                    for i, sec in enumerate(sections):
                        tg.start_soon(_work, i, sec)
        else:
            async with anyio.create_task_group() as tg:
                for i, sec in enumerate(sections):
                    tg.start_soon(_work, i, sec)
    # â›ï¸ FIX: AnyIO ë²„ì „ì— ë”°ë¼ ì˜ˆì™¸ íƒ€ì…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ë‘˜ ë‹¤ ìºì¹˜
    except (anyio.exceptions.TimeoutError, TimeoutError):
        timed_out = True
        logger.error(f"[EASY] â± batch timed out after {EASY_BATCH_TIMEOUT}s â€” partial results will be saved")

    success = sum(1 for r in results if r.ok)
    failed  = len(sections) - success
    success_rate = (success / max(1, len(sections))) * 100.0
    status_str = "ok" if (not timed_out and failed == 0) else "partial"

    # --- ì €ì¥ ---
    easy_json = {
        "paper_id": paper_id,
        "status": status_str,                      # âœ… ok | partial
        "count": len(sections),
        "success": success,
        "failed": failed,
        "success_rate": round(success_rate, 2),    # %
        "sections": [
            {
                "index": i,
                "title": sections[i].get("title"),
                "original_content": sections[i].get("content"),
                "korean_translation": (results[i].easy_text or ""),
                "image_path": (results[i].image_path or ""),
                "original_images": (results[i].original_images or []),
                "status": "ok" if results[i].ok else f"error: {results[i].error}",
                "section_type": results[i].section_type or sections[i].get("section_type", "section"),
            }
            for i in range(len(sections))
        ],
        "generated_at": _get_current_datetime(),
        "timed_out": timed_out,
        # âœ… ë””ë²„ê¹… ë©”íƒ€ (ì‘ë‹µ ëª¨ë¸ì—” í¬í•¨ë˜ì§€ ì•Šì§€ë§Œ íŒŒì¼ì—ëŠ” ì €ì¥)
        "section_times_ms": section_times_ms,
        "errors": error_briefs,
        "elapsed_ms": round((time.time() - t_batch_start) * 1000.0, 2),
    }

    json_path = out_dir / "easy_results.json"
    json_path.write_text(json.dumps(easy_json, ensure_ascii=False, indent=2), encoding="utf-8")
    logger.info(f"[EASY] saved JSON: {json_path}")

    # HTML ì €ì¥(ì‹¤íŒ¨í•´ë„ ì „ì²´ ë°°ì¹˜ ì„±ê³µì€ ìœ ì§€)
    try:
        html_path = out_dir / "easy_results.html"
        _save_html_results(sections, results, html_path, paper_id)
        (out_dir / "index.html").write_text(html_path.read_text(encoding="utf-8"), encoding="utf-8")
        logger.info(f"[EASY] saved HTML: {html_path}")
    except Exception as e:
        logger.warning(f"[EASY] HTML save skipped (non-fatal): {e}")

    # API ì‘ë‹µ(ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜)
    return BatchResult(
        ok=(status_str == "ok"),
        paper_id=paper_id,
        count=len(sections),
        success=success,
        failed=failed,
        out_dir=str(out_dir),
        images=results,
    )

# ---------- íŒŒì¼ ê²½ìœ  ì‹¤í–‰ (/from-transport) ----------
class TransportRequest(BaseModel):
    paper_id: str
    transport_path: str
    output_dir: Optional[str] = None
    style: Optional[str] = Field(default="three_para_ko")

# í—ˆìš© ë£¨íŠ¸(allowlist) ë° ì‚¬ì´ì¦ˆ ì œí•œ
ALLOWED_DATA_ROOT = Path(os.getenv("EASY_ALLOWED_ROOT", ".")).resolve()
TRANSPORT_MAX_MB  = int(os.getenv("EASY_TRANSPORT_MAX_MB", "50"))

def _is_under_allowed_root(p: Path) -> bool:
    try:
        return str(p.resolve()).startswith(str(ALLOWED_DATA_ROOT))
    except Exception:
        return False

def _read_text_guarded(p: Path, encoding="utf-8") -> str:
    sz = p.stat().st_size
    if sz > TRANSPORT_MAX_MB * 1024 * 1024:
        raise HTTPException(status_code=413, detail=f"íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤(>{TRANSPORT_MAX_MB}MB): {p.name}")
    return p.read_text(encoding=encoding, errors="ignore")

def _json_to_jsonl(s: str) -> str:
    """ë‹¨ì¼ JSON(ê°ì²´/ë°°ì—´)ì„ JSONL ë¬¸ìì—´ë¡œ ë³€í™˜."""
    try:
        obj = json.loads(s)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    lines = []
    if isinstance(obj, list):
        for i, item in enumerate(obj):
            if isinstance(item, dict):
                title = item.get("title") or f"Section {i+1}"
                text  = item.get("text")  or item.get("content") or ""
                lines.append(json.dumps({"title": title, "text": text}, ensure_ascii=False))
            else:
                lines.append(json.dumps({"title": f"Section {i+1}", "text": str(item)}, ensure_ascii=False))
    elif isinstance(obj, dict):
        title = obj.get("title") or "Section 1"
        text  = obj.get("text")  or obj.get("content") or ""
        lines.append(json.dumps({"title": title, "text": text}, ensure_ascii=False))
    else:
        lines.append(json.dumps({"title": "Section 1", "text": str(obj)}, ensure_ascii=False))
    return "\n".join(lines)

@app.post("/from-transport", response_model=BatchResult)
async def from_transport(request: TransportRequest):
    """
    transport_path ê²½ë¡œ(íŒŒì¼/ë””ë ‰í† ë¦¬)ë¥¼ ì½ì–´ /batch ë¡œ ìœ„ì„
    - ì§€ì› íŒŒì¼: .jsonl, .ndjson, .gz(jsonl.gz), .json(ë‹¨ì¼ JSON), .tex
    - ì§€ì› ë””ë ‰í† ë¦¬: merged_body.tex ë˜ëŠ” *.jsonl / *.jsonl.gz
    """
    tpath = Path(request.transport_path).expanduser().resolve()

    logger.info(f"[EASY] from-transport: tpath={tpath}")
    logger.info(f"[EASY] allowed_root={ALLOWED_DATA_ROOT}")

    # Allowlist ë³´ì•ˆ ê°€ë“œ
    if not _is_under_allowed_root(tpath):
        raise HTTPException(status_code=403, detail="í—ˆìš©ë˜ì§€ ì•Šì€ ê²½ë¡œì…ë‹ˆë‹¤")

    if not tpath.exists():
        raise HTTPException(status_code=404, detail=f"transport íŒŒì¼/ë””ë ‰í† ë¦¬ ì—†ìŒ: {tpath}")

    # ì¶œë ¥ ë£¨íŠ¸
    base_out = Path(request.output_dir).expanduser().resolve() if request.output_dir else tpath.parent

    # assets ë©”íƒ€ë°ì´í„° ë¡œë“œ (from_transportìš©)
    assets_metadata = {}
    if tpath.is_dir():
        assets_metadata = _load_assets_metadata(tpath)
        logger.info(f"[EASY] from-transport ë¡œë“œëœ ì´ë¯¸ì§€ ìì‚°: {len(assets_metadata)}ê°œ")
    elif tpath.suffix.lower() == ".tex":
        assets_metadata = _load_assets_metadata(tpath.parent)
        logger.info(f"[EASY] from-transport ë¡œë“œëœ ì´ë¯¸ì§€ ìì‚°: {len(assets_metadata)}ê°œ")

    content: Optional[str] = None

    try:
        if tpath.is_dir():
            # ë””ë ‰í† ë¦¬: merged_body.tex ìš°ì„ 
            tex = tpath / "merged_body.tex"
            if tex.exists():
                secs = _parse_latex_sections(tex)
                lines = [
                    json.dumps({"title": s.get("title") or f"Section {i+1}", "text": s.get("content") or ""}, ensure_ascii=False)
                    for i, s in enumerate(secs)
                ]
                content = "\n".join(lines)
            else:
                # *.jsonl ë˜ëŠ” *.jsonl.gz ì¤‘ ì•ŒíŒŒë²³ ì •ë ¬ ì²« íŒŒì¼
                hits_jsonl = sorted(tpath.rglob("*.jsonl"))
                hits_gz    = sorted(tpath.rglob("*.jsonl.gz"))
                if hits_jsonl:
                    content = _read_text_guarded(hits_jsonl[0])
                elif hits_gz:
                    with gzip.open(hits_gz[0], "rt", encoding="utf-8") as f:
                        content = f.read()
                else:
                    raise HTTPException(status_code=400, detail="ë””ë ‰í† ë¦¬ì— ì²˜ë¦¬ ê°€ëŠ¥í•œ íŒŒì¼ ì—†ìŒ (merged_body.tex ë˜ëŠ” *.jsonl/*.jsonl.gz)")
        else:
            ext = tpath.suffix.lower()
            if ext in (".jsonl", ".ndjson"):
                content = _read_text_guarded(tpath)
            elif ext == ".gz":
                # jsonl.gz ê°€ì •
                with gzip.open(tpath, "rt", encoding="utf-8") as f:
                    content = f.read()
            elif ext == ".tex":
                # .tex â†’ ì„¹ì…˜ íŒŒì‹± í›„ JSONL ë³€í™˜
                secs = _parse_latex_sections(tpath)
                lines = [
                    json.dumps({"title": s.get("title") or f"Section {i+1}", "text": s.get("content") or ""}, ensure_ascii=False)
                    for i, s in enumerate(secs)
                ]
                content = "\n".join(lines)
            elif ext == ".json":
                # ë‹¨ì¼ JSON(ë°°ì—´/ê°ì²´) â†’ JSONL
                content = _json_to_jsonl(_read_text_guarded(tpath))
            else:
                raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {tpath.name}")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"íŒŒì¼/ë””ë ‰í† ë¦¬ ì½ê¸° ì‹¤íŒ¨: {e}")

    # /batch ìœ„ì„
    req = BatchRequest(
        paper_id=request.paper_id,
        chunks_jsonl=content or "",
        output_dir=str(base_out),
        style=request.style or "three_para_ko",
    )
    
    # assets_metadataë¥¼ run_batchì— ì „ë‹¬í•˜ê¸° ìœ„í•´ ì§ì ‘ í˜¸ì¶œ
    return await run_batch(req, assets_metadata=assets_metadata)

# ====================== /END BATCH / TRANSPORT / VIZ ======================

# -------------------- Run --------------------
if __name__ == "__main__":
    host = os.getenv("EASY_HOST", "0.0.0.0")
    port = int(os.getenv("EASY_PORT", "5003"))
    reload_flag = os.getenv("EASY_RELOAD", "false").lower() in ("1", "true", "yes")
    uvicorn.run("app:app", host=host, port=port, reload=reload_flag, timeout_keep_alive=120)
# === PART 4/4 END ===

