"""
POLO Easy Model - Grounded JSON Generator (GPU-forced, stable)
- GPU í•„ìˆ˜: CUDA ì—†ìœ¼ë©´ ê¸°ë™ ì¤‘ë‹¨
- ì–´í…ì…˜ ë°±ì—”ë“œ ìë™ ì„ íƒ: flash_attn > sdpa(ê°€ëŠ¥ ì‹œ) > eager í´ë°±
- ì„¹ì…˜ë³„ originalë§Œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µí•˜ì—¬ 'easy'ë¥¼ ì¬ì„œìˆ (ì¶”ì¸¡ ê¸ˆì§€)
- Greedy ë””ì½”ë”©, ë‚®ì€ í† í° ìˆ˜(ê¸°ë³¸ 600)ë¡œ ì†ë„/ì•ˆì •ì„± í™•ë³´
- JSON ê°•ì œ íŒŒì‹± + ì¬ì‹œë„, ì‹¤íŒ¨ ì‹œ ë¹ˆ ê°’ ìœ ì§€
"""
import os
import uvicorn
import time
import json
import re
import logging
from typing import Optional, Dict, Any

import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, ConfigDict
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from dotenv import load_dotenv

# -------------------- í™˜ê²½/ë¡œê¹… --------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("polo.easy")

# polo-system ë£¨íŠ¸ì˜ .env ë¡œë“œ
_ENV_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".env"))
load_dotenv(_ENV_PATH)

HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
BASE_MODEL = os.getenv("EASY_BASE_MODEL", "meta-llama/Llama-3.2-3B-Instruct")

_DEFAULT_ADAPTER_DIR = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..", "fine-tuning", "outputs", "llama32-3b-qlora", "checkpoint-600")
)
ADAPTER_DIR = os.getenv("EASY_ADAPTER_DIR", _DEFAULT_ADAPTER_DIR)

MAX_NEW_TOKENS = int(os.getenv("EASY_MAX_NEW_TOKENS", "4000"))  # ê°€ì¤‘ì¹˜ 4000ìœ¼ë¡œ ì„¤ì •

# -------------------- FastAPI --------------------
app = FastAPI(title="POLO Easy Model", version="1.2.0")

# -------------------- ì „ì—­ ìƒíƒœ --------------------
model = None
tokenizer = None
device = "cuda"
gpu_available = False
safe_dtype = torch.float16

# -------------------- ëª¨ë¸/ìœ í‹¸ --------------------
def _pick_attn_impl() -> str:
    # 1) flash_attn ê°€ëŠ¥í•˜ë©´ ìµœìš°ì„ 
    try:
        import flash_attn  # noqa: F401
        logger.info("âœ… flash_attn ì‚¬ìš©: flash_attention_2")
        return "flash_attention_2"
    except Exception:
        pass
    # 2) sdpa ê°€ëŠ¥ ì—¬ë¶€(ì—†ìœ¼ë©´ transformersê°€ ImportError ë˜ì§ˆ ìˆ˜ ìˆìŒ)
    try:
        from torch.nn.functional import scaled_dot_product_attention  # noqa: F401
        logger.info("â„¹ï¸ sdpa ì‚¬ìš© ê°€ëŠ¥")
        return "sdpa"
    except Exception:
        logger.info("â„¹ï¸ sdpa ë¶ˆê°€ â†’ eagerë¡œ ì§„í–‰")
        return "eager"

def _coerce_json(text: str) -> Dict[str, Any]:
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1 and end > start:
        text = text[start:end+1]
    return json.loads(text)

def _is_meaningful(d: dict) -> bool:
    try:
        sections = ["abstract","introduction","methods","results","discussion","conclusion"]
        return any(len((d.get(s, {}) or {}).get("easy", "")) > 10 for s in sections)
    except Exception:
        return False

def _extract_sections(src: str) -> dict:
    sections = {
        "abstract": "",
        "introduction": "",
        "methods": "",
        "results": "",
        "discussion": "",
        "conclusion": "",
    }
    headers = [
        ("abstract", r"^\s*abstract\b[:\-]?"),
        ("introduction", r"^\s*introduction\b[:\-]?"),
        ("methods", r"^\s*methods?\b[:\-]?|^\s*materials?\s+and\s+methods\b[:\-]?"),
        ("results", r"^\s*results?\b[:\-]?"),
        ("discussion", r"^\s*discussion\b[:\-]?"),
        ("conclusion", r"^\s*conclusion[s]?\b[:\-]?|^\s*concluding\s+remarks\b[:\-]?"),
    ]
    lines = src.splitlines()
    indices = []
    for idx, line in enumerate(lines):
        for key, pat in headers:
            if re.match(pat, line.strip(), flags=re.IGNORECASE):
                indices.append((idx, key))
                break
    indices.sort()
    for i, (start_idx, key) in enumerate(indices):
        end_idx = indices[i+1][0] if i+1 < len(indices) else len(lines)
        chunk = "\n".join(lines[start_idx+1:end_idx]).strip()
        sections[key] = chunk[:2000]  # ì„¹ì…˜ originalì€ ê¸¸ì´ ì œí•œ
    return sections

# -------------------- ìŠ¤í‚¤ë§ˆ --------------------
GROUND_SCHEMA = {
    "title": "",
    "authors": [],
    "abstract": {"original": "", "easy": ""},
    "introduction": {"original": "", "easy": ""},
    "methods": {"original": "", "easy": ""},
    "results": {"original": "", "easy": ""},
    "discussion": {"original": "", "easy": ""},
    "conclusion": {"original": "", "easy": ""},
    "keywords": [],
    "figures_tables": [],
    "references": [],
    "contributions": [],
    "limitations": [],
    "glossary": [],
    "plain_summary": "",
}

# -------------------- ìš”ì²­/ì‘ë‹µ ëª¨ë¸ --------------------
class TextRequest(BaseModel):
    text: str
    translate: Optional[bool] = False
    model_config = ConfigDict(extra="allow")  # ì¶”ê°€ í•„ë“œ í—ˆìš©(422 ë°©ì§€)

class TextResponse(BaseModel):
    simplified_text: str
    translated_text: Optional[str] = None

# -------------------- ëª¨ë¸ ë¡œë“œ --------------------
def load_model():
    global model, tokenizer, gpu_available, device, safe_dtype

    logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {BASE_MODEL}")

    # âœ… GPU ê°•ì œ
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA(GPU)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU í™˜ê²½ì—ì„œë§Œ ì‹¤í–‰í•˜ë„ë¡ ê°•ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
    gpu_available = True
    device = "cuda"
    safe_dtype = torch.float16

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=HF_TOKEN)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token

    attn_impl = _pick_attn_impl()

    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # 1ì°¨: ì„ íƒëœ ì–´í…ì…˜ìœ¼ë¡œ ë¡œë“œ, ì‹¤íŒ¨ ì‹œ eagerë¡œ í´ë°±
    try:
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            torch_dtype=safe_dtype,
            trust_remote_code=True,
            attn_implementation=attn_impl,
            low_cpu_mem_usage=True,
            token=HF_TOKEN,
            device_map=None,  # ëª…ì‹œì  .to(device) ì‚¬ìš©
        )
    except Exception as e:
        logger.warning(f"attn='{attn_impl}' ë¡œë”© ì‹¤íŒ¨({e}) â†’ eagerë¡œ í´ë°±")
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            torch_dtype=safe_dtype,
            trust_remote_code=True,
            attn_implementation="eager",
            low_cpu_mem_usage=True,
            token=HF_TOKEN,
            device_map=None,
        )

    # LoRA ì–´ëŒ‘í„° (ì‹¤íŒ¨í•´ë„ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ê³„ì† ì§„í–‰)
    m = base  # ê¸°ë³¸ê°’ì€ ë² ì´ìŠ¤ ëª¨ë¸
    
    if ADAPTER_DIR and os.path.exists(ADAPTER_DIR):
        logger.info(f"ğŸ”„ ì–´ëŒ‘í„° ë¡œë”© ì‹œë„: {ADAPTER_DIR}")
        try:
            # Windows ê²½ë¡œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
            adapter_path = os.path.abspath(ADAPTER_DIR)
            # Hugging Faceê°€ ë¡œì»¬ ê²½ë¡œë¥¼ ì¸ì‹í•˜ë„ë¡ ì²˜ë¦¬
            m = PeftModel.from_pretrained(base, adapter_path, is_trainable=False, local_files_only=True)
            logger.info("âœ… ì–´ëŒ‘í„° ë¡œë”© ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.warning("âš ï¸ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ê³„ì† ì§„í–‰")
            m = base
    else:
        logger.warning("âš ï¸ ì–´ëŒ‘í„° ê²½ë¡œ ì—†ìŒ â†’ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ë™ì‘")

    m.eval()
    m = m.to(safe_dtype).to(device)

    p = next(m.parameters())
    logger.info(f"ğŸ§  MODEL DEVICE: {p.device}, DTYPE: {p.dtype}")

    model = m
    logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# -------------------- ìŠ¤íƒ€íŠ¸ì—… --------------------
@app.on_event("startup")
async def startup_event():
    load_model()

# -------------------- ì—”ë“œí¬ì¸íŠ¸ --------------------
@app.get("/")
async def root():
    return {"message": "POLO Easy Model API", "model": BASE_MODEL}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if (model is not None and tokenizer is not None and gpu_available) else "unavailable",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None,
        "gpu_available": gpu_available,
        "gpu_device": torch.cuda.get_device_name(0) if gpu_available else None,
        "model_name": BASE_MODEL,
        "max_new_tokens": MAX_NEW_TOKENS,
    }

@app.get("/healthz")
async def healthz():
    return await health()

@app.post("/simplify", response_model=TextResponse)
async def simplify_text(request: TextRequest):
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    prompt = (
        "ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ í’€ì–´ ì¨ë¼. "
        "ì¶”ê°€ ê°€ì •ì´ë‚˜ ì™¸ë¶€ ì§€ì‹ ì‚¬ìš© ê¸ˆì§€. ì œê³µëœ ë¬¸ì¥ë§Œ ì¬ì„œìˆ í•˜ë©°, ëª¨ë¥´ë©´ ìƒëµ:\n\n"
        f"{request.text}\n\nì‰¬ìš´ ì„¤ëª…:"
    )

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=768)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=4000,          # âœ… ê°€ì¤‘ì¹˜ 4000ìœ¼ë¡œ ì„¤ì •
            do_sample=False,              # âœ… ê·¸ë¦¬ë””(ì¶”ì¸¡ ì–µì œ)
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    simplified_text = generated[len(prompt):].strip()
    return TextResponse(simplified_text=simplified_text, translated_text=None)

@app.post("/generate")
async def generate_json(request: TextRequest):
    """
    ë…¼ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì„¹ì…˜ë³„ originalë§Œ ë°”íƒ•ìœ¼ë¡œ 'easy'ë¥¼ ì¬ì„œìˆ í•˜ì—¬ JSONì„ ìƒì„±.
    - ì™¸ë¶€ì§€ì‹/ì¶”ì¸¡ ê¸ˆì§€
    - originalì´ ë¹„ì–´ ìˆìœ¼ë©´ easyë„ ë¹ˆ ë¬¸ìì—´ ìœ ì§€
    """
    start_time = time.time()
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    logger.info("ğŸš€ JSON ìƒì„± ì‹œì‘")
    logger.info(f"ğŸ“Š ì…ë ¥ ê¸¸ì´: {len(request.text)}")

    # 1) ì„¹ì…˜ ì¶”ì¶œ
    extracted = _extract_sections(request.text)

    # 2) ìŠ¤í‚¤ë§ˆ ì¤€ë¹„ + original ì£¼ì…
    data_schema = json.loads(json.dumps(GROUND_SCHEMA))  # deepcopy
    for k in ["abstract","introduction","methods","results","discussion","conclusion"]:
        data_schema[k]["original"] = extracted.get(k, "")

    # 3) í”„ë¡¬í”„íŠ¸(ê°•í•œ ì œì•½) â€” ì„¹ì…˜ë³„ ì›ë¬¸ë§Œ ì°¸ê³ í•˜ë„ë¡ ëª…ì‹œ
    instruction = (
        "ë„ˆëŠ” ê³¼í•™ ì»¤ë®¤ë‹ˆì¼€ì´í„°ë‹¤. ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆì˜ 'í‚¤ì™€ êµ¬ì¡°'ë¥¼ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ê³ , "
        "'ê°’'ë§Œ ì±„ì›Œë¼. ì¶œë ¥ì€ ì˜¤ì§ 'ìœ íš¨í•œ JSON' í•˜ë‚˜ë§Œ í—ˆìš©ëœë‹¤(ì½”ë“œë¸”ë¡/ì„¤ëª…/ì£¼ì„ ê¸ˆì§€). "
        "ê° ì„¹ì…˜ì˜ 'easy'ëŠ” í•´ë‹¹ ì„¹ì…˜ì˜ 'original'ì— ìˆëŠ” ë¬¸ì¥ë§Œ ì¬ì„œìˆ í•˜ì—¬ 4-6ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ë¼. "
        "originalì´ ë¹„ì–´ ìˆìœ¼ë©´ í•´ë‹¹ 'easy'ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ë‚¨ê²¨ë¼. "
        "ì™¸ë¶€ ì§€ì‹/ì¶”ì¸¡/ì¼ë°˜ ìƒì‹/ê°œë°œì ìƒìƒ ê¸ˆì§€. ì‚¬ì‹¤ì´ ë¶ˆí™•ì‹¤í•˜ë©´ ë¹ˆ ê°’ìœ¼ë¡œ ë‘”ë‹¤. "
        "'title', 'authors', 'keywords', 'references' ë“± ì›ë¬¸ì—ì„œ ëª…í™•íˆ ì•Œ ìˆ˜ ì—†ëŠ” ì •ë³´ëŠ” ë¹ˆ ê°’ìœ¼ë¡œ ìœ ì§€í•œë‹¤. "
        "'figures_tables'ê°€ ë³´ì´ë©´ {label, caption, easy} í˜•ì‹ìœ¼ë¡œë§Œ ì‘ì„±í•˜ë©°, ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´. "
        "'plain_summary'ëŠ” ìœ„ ì„¹ì…˜ 'easy'ì— í¬í•¨ëœ ë‚´ìš©ë§Œ ë°”íƒ•ìœ¼ë¡œ 5-7ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ë¼. "
        "JSONì€ ë°˜ë“œì‹œ '{'ë¡œ ì‹œì‘í•´ '}'ë¡œ ëë‚˜ì•¼ í•˜ë©°, í‚¤ ìˆœì„œ/ì´ë¦„ì„ ë°”ê¾¸ì§€ ë§ˆë¼."
    )

    schema_str = json.dumps(data_schema, ensure_ascii=False, indent=2)
    context_only = {
        "abstract": extracted["abstract"],
        "introduction": extracted["introduction"],
        "methods": extracted["methods"],
        "results": extracted["results"],
        "discussion": extracted["discussion"],
        "conclusion": extracted["conclusion"],
    }
    context_str = json.dumps(context_only, ensure_ascii=False, indent=2)

    prompt = (
        f"{instruction}\n\n"
        "=== ì¶œë ¥ ìŠ¤í‚¤ë§ˆ(ê°’ë§Œ ì±„ì›Œë¼) ===\n"
        f"{schema_str}\n\n"
        "=== ì„¹ì…˜ë³„ original (ì´ í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©) ===\n"
        f"{context_str}\n\n"
        "ìœ„ ì§€ì‹œë¥¼ ë”°ë¼ JSONë§Œ ì¶œë ¥:"
    )

    # 4) í† í¬ë‚˜ì´ì¦ˆ + ë””ë°”ì´ìŠ¤ ì´ë™
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    logger.info(f"INPUT -> device={inputs['input_ids'].device}, shape={tuple(inputs['input_ids'].shape)}")

    # 5) ìƒì„±(ê·¸ë¦¬ë””, ì œí•œëœ ê¸¸ì´)
    t0 = time.time()
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,              # âœ… ê·¸ë¦¬ë””(ì¶”ì¸¡ ì–µì œ)
            use_cache=True,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    inference_time = time.time() - t0
    logger.info(f"âš¡ ì¶”ë¡  ì™„ë£Œ: {inference_time:.2f}s")

    # 6) íŒŒì‹±/ê²€ì¦ (+ì¬ì‹œë„)
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    raw = generated[len(prompt):].strip()

    try:
        data = _coerce_json(raw)
        if not _is_meaningful(data):
            raise ValueError("empty_json")
        logger.info("âœ… 1ì°¨ JSON íŒŒì‹±/ê²€ì¦ ì„±ê³µ")
    except Exception as e:
        logger.warning(f"âš ï¸ 1ì°¨ íŒŒì‹± ì‹¤íŒ¨: {e}. ì¬ì‹œë„")
        strict_instruction = (
            "ìŠ¤í‚¤ë§ˆì˜ í‚¤/êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³  ê°’ë§Œ ì±„ìš´ 'ìœ íš¨í•œ JSON'ë§Œ ì¶œë ¥í•˜ë¼. "
            "ë°˜ë“œì‹œ '{'ë¡œ ì‹œì‘í•´ '}'ë¡œ ëë‚˜ì•¼ í•œë‹¤. ì™¸ë¶€ì§€ì‹/ì¶”ì¸¡ ê¸ˆì§€, ì„¹ì…˜ originalë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©."
        )
        strict_prompt = f"{strict_instruction}\n\nìŠ¤í‚¤ë§ˆ:\n{schema_str}\n\nì„¹ì…˜ original:\n{context_str}\n\nJSONë§Œ ì¶œë ¥:"
        inputs2 = tokenizer(strict_prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs2 = {k: v.to(device) for k, v in inputs2.items()}
        with torch.inference_mode():
            outputs2 = model.generate(
                **inputs2,
                max_new_tokens=min(MAX_NEW_TOKENS, 800),
                do_sample=False,
                use_cache=True,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        gen2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)
        raw2 = gen2[len(strict_prompt):].strip()
        try:
            data = _coerce_json(raw2)
            if not _is_meaningful(data):
                raise ValueError("empty_json_retry")
            logger.info("âœ… 2ì°¨ JSON íŒŒì‹±/ê²€ì¦ ì„±ê³µ")
        except Exception as e2:
            logger.warning(f"âš ï¸ 2ì°¨ íŒŒì‹± ì‹¤íŒ¨: {e2}. ë¹ˆ ê°’ ìœ ì§€í•˜ì—¬ ìŠ¤í‚¤ë§ˆ ë°˜í™˜")
            data = data_schema  # ì›ë³¸ ìŠ¤í‚¤ë§ˆ(ë¹ˆ ê°’)ë¡œ ë°˜í™˜

    total_time = time.time() - start_time
    logger.info(f"ğŸ‰ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {total_time:.2f}s")

    data["processing_info"] = {
        "gpu_used": gpu_available,
        "inference_time": inference_time,
        "total_time": total_time,
        "input_length": len(request.text),
        "output_length": len(str(data)),
    }
    return data

# -------------------- main --------------------
if __name__ == "__main__":
    # GPU ê°•ì œ ëª¨ë“œ: CUDA ì—†ìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ
    if not torch.cuda.is_available():
        raise SystemExit("CUDA(GPU)ê°€ í•„ìš”í•©ë‹ˆë‹¤. GPU í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.")
    uvicorn.run(app, host="0.0.0.0", port=5003)