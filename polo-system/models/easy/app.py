"""
POLO Easy Model - ë…¼ë¬¸ì„ ì‰½ê²Œ í’€ì–´ ì„¤ëª…í•˜ëŠ” LLM ì„œë¹„ìŠ¤
"""
import os
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from googletrans import Translator
from dotenv import load_dotenv
import json
import time
import logging
import re

# --- í™˜ê²½ ë³€ìˆ˜ ---
BASE_MODEL = os.getenv("EASY_BASE_MODEL", "meta-llama/Llama-3.2-3B-Instruct")

# ê¸°ë³¸ ì–´ëŒ‘í„° ê²½ë¡œ: fine-tuning ê²°ê³¼ë¬¼(checkpoint-600)ì„ ì°¸ì¡°
_DEFAULT_ADAPTER_DIR = os.path.abspath(
    os.path.join(
        os.path.dirname(__file__),
        "..", "fine-tuning", "outputs", "llama32-3b-qlora", "checkpoint-600",
    )
)
ADAPTER_DIR = os.getenv("EASY_ADAPTER_DIR", _DEFAULT_ADAPTER_DIR)

# polo-system ë£¨íŠ¸ì˜ .env ë¡œë“œ (ëª¨ë¸ì„ easy ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•´ë„ ì¸ì‹ë˜ë„ë¡)
_ENV_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".env"))
load_dotenv(_ENV_PATH)

# Hugging Face í† í° (ê°€ë“œ ë¦¬í¬ ì ‘ê·¼ìš©)
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

app = FastAPI(title="POLO Easy Model", version="1.0.0")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
translator = Translator()
gpu_available = False

class TextRequest(BaseModel):
    text: str
    translate: Optional[bool] = False

class TextResponse(BaseModel):
    simplified_text: str
    translated_text: Optional[str] = None

def load_model():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    global model, tokenizer, gpu_available
    
    logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {BASE_MODEL}")
    
    # GPU ìƒíƒœ í™•ì¸ - ê°•ì œë¡œ GPU ì‚¬ìš© ì‹œë„
    gpu_available = False
    try:
        # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if torch.cuda.is_available():
            device_name = torch.cuda.get_device_name(0)
            device_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"ğŸš€ GPU ì‚¬ìš© ê°€ëŠ¥: {device_name}")
            logger.info(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {device_memory:.1f}GB")
            logger.info(f"ğŸ¯ GPU ë””ë°”ì´ìŠ¤: cuda:0")
            gpu_available = True
        else:
            # CUDAê°€ ê°ì§€ë˜ì§€ ì•Šì•„ë„ ê°•ì œë¡œ GPU ì‚¬ìš© ì‹œë„
            logger.info("ğŸš€ CUDA ê°ì§€ ì‹¤íŒ¨, GPU ê°•ì œ ì‚¬ìš© ì‹œë„...")
            try:
                # ê°„ë‹¨í•œ í…ì„œë¡œ GPU í…ŒìŠ¤íŠ¸
                test_tensor = torch.tensor([1.0]).cuda()
                logger.info("âœ… GPU ê°•ì œ ì‚¬ìš© ì„±ê³µ!")
                gpu_available = True
            except Exception as e:
                logger.warning(f"âš ï¸ GPU ê°•ì œ ì‚¬ìš© ì‹¤íŒ¨: {e}")
                gpu_available = False
    except Exception as e:
        logger.warning(f"âš ï¸ GPU í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        gpu_available = False
    
    if not gpu_available:
        logger.warning("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=HF_TOKEN)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ëª¨ë¸ ë¡œë“œ: ë¡œì»¬ ì„œë¹™ì€ bitsandbytes ë¯¸ì‚¬ìš© â†’ bfloat16 ê³ ì •(GPU), CPUëŠ” float32
    safe_dtype = torch.bfloat16 if gpu_available else torch.float32
    logger.info(f"ğŸ§  ëª¨ë¸ ë¡œë”© ì¤‘... (dtype: {safe_dtype})")
    
    # GPU ì‚¬ìš© ì‹œ device_map ì„¤ì •
    if gpu_available:
        logger.info("ğŸ¯ GPU device_mapìœ¼ë¡œ ëª¨ë¸ ë¡œë”©...")
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            torch_dtype=safe_dtype,
            trust_remote_code=True,
            attn_implementation="eager",
            device_map="auto",
            token=HF_TOKEN,
        )
        logger.info("âœ… GPUì— ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    else:
        logger.info("ğŸ’» CPUë¡œ ëª¨ë¸ ë¡œë”©...")
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            torch_dtype=safe_dtype,
            trust_remote_code=True,
            attn_implementation="eager",
            token=HF_TOKEN,
        )
        logger.info("âœ… CPUì— ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    # ì–´ëŒ‘í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œ (QLoRA ê°€ì¤‘ì¹˜)
    if ADAPTER_DIR and os.path.exists(ADAPTER_DIR):
        logger.info(f"ğŸ”„ ì–´ëŒ‘í„° ë¡œë”© ì¤‘: {ADAPTER_DIR}")
        model = PeftModel.from_pretrained(base, ADAPTER_DIR, is_trainable=False)
        if gpu_available:
            logger.info("ğŸ¯ ì–´ëŒ‘í„°ë¥¼ GPUë¡œ ì´ë™...")
            model = model.to("cuda")
        logger.info("âœ… ì–´ëŒ‘í„° ë¡œë”© ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ ì–´ëŒ‘í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆœìˆ˜ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
        model = base
    
    model.eval()
    logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    load_model()

@app.get("/")
async def root():
    return {"message": "POLO Easy Model API", "model": BASE_MODEL}

@app.post("/simplify", response_model=TextResponse)
async def simplify_text(request: TextRequest):
    """í…ìŠ¤íŠ¸ë¥¼ ì‰½ê²Œ í’€ì–´ ì„¤ëª…"""
    try:
        if model is None or tokenizer is None:
            raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ ë…¼ë¬¸ ë‚´ìš©ì„ ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”:

{request.text}

ì‰¬ìš´ ì„¤ëª…:"""
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        
        # GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        simplified_text = generated_text[len(prompt):].strip()
        
        # ë²ˆì—­ (ìš”ì²­ëœ ê²½ìš°)
        translated_text = None
        if request.translate:
            try:
                translation = translator.translate(simplified_text, dest='ko')
                translated_text = translation.text
            except Exception as e:
                print(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
        
        return TextResponse(
            simplified_text=simplified_text,
            translated_text=translated_text
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/generate")
async def generate_json(request: TextRequest):
    """ë…¼ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì„¹ì…˜ë³„ë¡œ ì‰½ê²Œ ì¬í•´ì„í•œ JSON ìƒì„±"""
    start_time = time.time()
    try:
        if model is None or tokenizer is None:
            raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        logger.info("ğŸš€ JSON ìƒì„± ì‹œì‘")
        logger.info(f"ğŸ“Š ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(request.text)} ë¬¸ì")
        logger.info(f"ğŸ¯ GPU ì‚¬ìš©: {gpu_available}")

        # 1) ì›ë¬¸ì—ì„œ ì£¼ìš” ì„¹ì…˜ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ ì¶”ì¶œí•˜ì—¬ 'original' ì±„ì›Œë‘ê¸°
        def extract_sections(src: str) -> dict:
            sections = {
                "abstract": "",
                "introduction": "",
                "methods": "",
                "results": "",
                "discussion": "",
                "conclusion": "",
            }
            # ì„¹ì…˜ í—¤ë” íŒ¨í„´ (ëŒ€/ì†Œë¬¸ì, ê³µë°± í¬í•¨, ì½œë¡  í—ˆìš©)
            headers = [
                ("abstract", r"^\s*abstract\b[:\-]?"),
                ("introduction", r"^\s*introduction\b[:\-]?"),
                ("methods", r"^\s*methods?\b[:\-]?|^\s*materials?\s+and\s+methods\b[:\-]?"),
                ("results", r"^\s*results?\b[:\-]?"),
                ("discussion", r"^\s*discussion\b[:\-]?"),
                ("conclusion", r"^\s*conclusion[s]?\b[:\-]?|^\s*concluding\s+remarks\b[:\-]?")
            ]
            lines = src.splitlines()
            indices = []
            for idx, line in enumerate(lines):
                for key, pat in headers:
                    if re.match(pat, line.strip(), flags=re.IGNORECASE):
                        indices.append((idx, key))
                        break
            indices.sort()
            for i, (start_idx, key) in enumerate(indices):
                end_idx = indices[i+1][0] if i+1 < len(indices) else len(lines)
                chunk = "\n".join(lines[start_idx+1:end_idx]).strip()
                sections[key] = chunk[:2000]  # ì›ë¬¸ì€ ê¸¸ì´ ì œí•œ
            return sections

        extracted = extract_sections(request.text)

        json_schema = {
            "title": "",  # ë…¼ë¬¸ ì œëª©(ì›ë¬¸ ì¶”ì¶œ ë¶ˆê°€ ì‹œ ìš”ì•½ ê¸°ë°˜ ìƒì„±)
            "authors": [],  # ì €ì ëª©ë¡(ì•Œ ìˆ˜ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
            "abstract": {"original": extracted["abstract"], "easy": ""},
            "introduction": {"original": extracted["introduction"], "easy": ""},
            "methods": {"original": extracted["methods"], "easy": ""},
            "results": {"original": extracted["results"], "easy": ""},
            "discussion": {"original": extracted["discussion"], "easy": ""},
            "conclusion": {"original": extracted["conclusion"], "easy": ""},
            "keywords": [],
            "figures_tables": [],  # {label, caption, easy}
            "references": [],
            "contributions": [],  # í•µì‹¬ ê¸°ì—¬ í¬ì¸íŠ¸ë¥¼ ì‰¬ìš´ ë¬¸ì¥ìœ¼ë¡œ
            "limitations": [],
            "glossary": [],  # ì¤‘ìš” ìš©ì–´ {term, definition}
            "plain_summary": ""  # ì „ì²´ë¥¼ ì¼ë°˜ì¸ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ 5-7ë¬¸ì¥ ìš”ì•½
        }

        instruction = (
            "ë„ˆëŠ” ê³¼í•™ ì»¤ë®¤ë‹ˆì¼€ì´í„°ë‹¤. ì•„ë˜ ìŠ¤í‚¤ë§ˆì˜ í‚¤ì™€ êµ¬ì¡°ë¥¼ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ê³ , ê°’ë§Œ ì±„ì›Œë¼. "
            "ì¶œë ¥ì€ ì˜¤ì§ JSON í•˜ë‚˜ë§Œ í—ˆìš©ëœë‹¤(ë§ˆí¬ë‹¤ìš´, ì„¤ëª…, ì½”ë“œë¸”ë¡ ê¸ˆì§€). "
            "ê° ì„¹ì…˜ì˜ 'easy'ì—ëŠ” ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ 4-6ë¬¸ì¥ìœ¼ë¡œ í’€ì–´ì“°ê³ , ê³¼ì¥/ì¶”ì¸¡ ê¸ˆì§€. "
            "ëª¨ë¥¼ ì •ë³´ëŠ” ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ë¹ˆ ë°°ì—´ë¡œ ë‘”ë‹¤. 'figures_tables'ëŠ” ìˆìœ¼ë©´ {label, caption, easy}ë¡œ ëª©ë¡í™”. "
            "'plain_summary'ëŠ” ì „ì²´ë¥¼ ì¼ë°˜ì–´ë¡œ 5-7ë¬¸ì¥ ìš”ì•½." 
        )

        schema_str = json.dumps(json_schema, ensure_ascii=False, indent=2)

        prompt = f"""{instruction}

ë‹¤ìŒì€ ì¶œë ¥í•´ì•¼ í•  JSON ìŠ¤í‚¤ë§ˆ(ë¯¸ë¦¬ ì¼ë¶€ originalì´ ì±„ì›Œì§)ì´ë‹¤. í‚¤/êµ¬ì¡°ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³  ê°’ë§Œ ì±„ì›Œë¼. ë°˜ë“œì‹œ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥:
{schema_str}

ì°¸ê³ ìš© ì „ì²´ ì›ë¬¸ í…ìŠ¤íŠ¸(ì„¹ì…˜ ì¶”ì¶œì´ ë¶€ì •í™•í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³´ì¡°ë¡œë§Œ ì‚¬ìš©):\n\n"""

        logger.info("ğŸ“ í† í¬ë‚˜ì´ì§• ì‹œì‘...")
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            prompt + request.text,
            return_tensors="pt",
            truncation=True,
            max_length=2048,
        )

        if gpu_available:
            inputs = {k: v.cuda() for k, v in inputs.items()}
            logger.info("ğŸ¯ ì…ë ¥ì„ GPUë¡œ ì´ë™ ì™„ë£Œ")

        logger.info("ğŸ§  ëª¨ë¸ ì¶”ë¡  ì‹œì‘...")
        inference_start = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=1600,
                do_sample=False,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

        inference_time = time.time() - inference_start
        logger.info(f"âš¡ ì¶”ë¡  ì™„ë£Œ: {inference_time:.2f}ì´ˆ")

        logger.info("ğŸ“„ ë””ì½”ë”© ì‹œì‘...")
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        raw = generated[len(prompt):].strip()

        # JSON ê°•ì œ íŒŒì‹±: ì²« { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€ ìë¥´ê³  íŒŒì‹± ì‹œë„
        def coerce_json(text: str):
            start = text.find("{")
            end = text.rfind("}")
            if start != -1 and end != -1 and end > start:
                text = text[start:end+1]
            return json.loads(text)

        def is_meaningful(d: dict) -> bool:
            try:
                # ì„¹ì…˜ easy ì¤‘ í•˜ë‚˜ë¼ë„ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì˜ë¯¸ ìˆë‹¤ê³  ê°„ì£¼
                sections = ["abstract","introduction","methods","results","discussion","conclusion"]
                return any(len((d.get(s,{}) or {}).get("easy","")) > 10 for s in sections)
            except Exception:
                return False

        try:
            data = coerce_json(raw)
            if not is_meaningful(data):
                raise ValueError("empty_json")
            logger.info("âœ… JSON íŒŒì‹±/ê²€ì¦ ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸ 1ì°¨ íŒŒì‹± ì‹¤íŒ¨: {e}. ì¬ìƒì„± ì‹œë„")
            # 2ì°¨ ì‹œë„: ë” ì—„ê²©í•œ ì§€ì‹œë¬¸ê³¼ ìƒ˜í”Œë§/ê¸¸ì´ ì¡°ì •
            strict_instruction = (
                "ìœ„ ìŠ¤í‚¤ë§ˆë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°’ì„ ì±„ì›Œ 'ìœ íš¨í•œ JSON'ë§Œ ì¶œë ¥í•˜ë¼. ë°˜ë“œì‹œ '{' ë¡œ ì‹œì‘í•˜ê³  '}' ë¡œ ëë‚´ë¼. "
                "ì½”ë“œë¸”ë¡, ì£¼ì„, ì„¤ëª…, í‚¤ ë³€ê²½ ì¼ì ˆ ê¸ˆì§€. ì‘ë‹µì€ ìˆœìˆ˜ JSON ë¬¸ìì—´ í•˜ë‚˜ë§Œ í—ˆìš©."
            )
            strict_prompt = f"{strict_instruction}\n\nìŠ¤í‚¤ë§ˆ:\n{schema_str}\n\nì°¸ê³  ì›ë¬¸(ìš”ì•½/ì¬í•´ì„ì—ë§Œ ì‚¬ìš©):\n\n"
            inputs2 = tokenizer(
                strict_prompt + request.text,
                return_tensors="pt",
                truncation=True,
                max_length=2048,
            )
            if gpu_available:
                inputs2 = {k: v.cuda() for k, v in inputs2.items()}
            with torch.no_grad():
                outputs2 = model.generate(
                    **inputs2,
                    max_new_tokens=1400,
                    do_sample=False,
                    repetition_penalty=1.1,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            gen2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)
            raw2 = gen2[len(strict_prompt):].strip()
            try:
                data = coerce_json(raw2)
                if not is_meaningful(data):
                    raise ValueError("empty_json_retry")
                logger.info("âœ… 2ì°¨ JSON íŒŒì‹±/ê²€ì¦ ì„±ê³µ")
            except Exception as e2:
                logger.warning(f"âš ï¸ 2ì°¨ íŒŒì‹± ì‹¤íŒ¨: {e2}. ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ë°˜í™˜")
                data = json_schema
                data["plain_summary"] = ""

        total_time = time.time() - start_time
        logger.info(f"ğŸ‰ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        data["processing_info"] = {
            "gpu_used": gpu_available,
            "inference_time": inference_time,
            "total_time": total_time,
            "input_length": len(request.text),
            "output_length": len(str(data))
        }

        return data
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ JSON ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"JSON ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None,
        "gpu_available": gpu_available,
        "gpu_device": torch.cuda.get_device_name(0) if gpu_available else None,
        "model_name": BASE_MODEL
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=5003)
