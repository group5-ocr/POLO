# === PART 1/4 START ===
# -*- coding: utf-8 -*-
"""
POLO Easy Model - Grounded JSON Generator (Patched)
- Fix(1): token-based slicing (ì²« ê¸€ì ì˜ë¦¼ ë°©ì§€)
- Fix(2): decoding for stability (do_sample=False, no_repeat_ngram_size=4, repetition_penalty=1.2)
- Fix(3): explicit stop markers + safe cut
- Fix(4): LaTeX/table strip keeps sentence boundaries (" . ")
- Fix(5): sanitize repeats (e.g., *flops spam*)
- Fix(6): prompt policy (no invention; 'not stated in the original text'), + style=three_para_ko
"""

from __future__ import annotations
import os, re, json, time, base64, gzip, logging
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple

import anyio, httpx, torch, uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, ConfigDict, Field
from dotenv import load_dotenv

# -------------------- .env --------------------
ROOT_ENV = Path(__file__).resolve().parents[2] / ".env"
if ROOT_ENV.exists():
    load_dotenv(dotenv_path=str(ROOT_ENV), override=True)

# -------------------- ENV / ê¸°ë³¸ê°’ --------------------
HF_TOKEN           = os.getenv("HUGGINGFACE_TOKEN", "")
BASE_MODEL         = os.getenv("EASY_BASE_MODEL", "meta-llama/Llama-3.2-3B-Instruct")
ADAPTER_DIR        = os.getenv("EASY_ADAPTER_DIR", str(Path(__file__).resolve().parent.parent / "fine-tuning" / "outputs" / "llama32-3b-qlora" / "checkpoint-4000"))
MAX_NEW_TOKENS     = int(os.getenv("EASY_MAX_NEW_TOKENS", "1200"))
VIZ_MODEL_URL      = os.getenv("VIZ_MODEL_URL", "http://localhost:5005")
EASY_CONCURRENCY   = int(os.getenv("EASY_CONCURRENCY", "8"))
EASY_BATCH_TIMEOUT = int(os.getenv("EASY_BATCH_TIMEOUT", "600"))

EASY_STRIP_MATH = os.getenv("EASY_STRIP_MATH", "1").lower() in ("1", "true", "yes")
EASY_FORCE_KO   = os.getenv("EASY_FORCE_KO", "1").lower() in ("1", "true", "yes")
EASY_AUTO_BOLD  = os.getenv("EASY_AUTO_BOLD", "1").lower() in ("1", "true", "yes")
EASY_HILITE     = os.getenv("EASY_HILITE", "1").lower() in ("1", "true", "yes")

# -------------------- HF cache pin --------------------
SAFE_CACHE_DIR = Path(__file__).resolve().parent / "hf_cache"
SAFE_CACHE_DIR.mkdir(parents=True, exist_ok=True)
for k in ("HF_HOME","TRANSFORMERS_CACHE","HF_DATASETS_CACHE","HF_HUB_CACHE"):
    os.environ[k] = str(SAFE_CACHE_DIR)
CACHE_DIR = os.environ["HF_HOME"]

from transformers import AutoModelForCausalLM, AutoTokenizer  # noqa: E402
from peft import PeftModel  # noqa: E402

# -------------------- Logger --------------------
logger = logging.getLogger("polo.easy")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# -------------------- FastAPI --------------------
app = FastAPI(title="POLO Easy Model", version="1.5.0-patched")

# -------------------- Global state --------------------
model = None
tokenizer = None
device = "cuda" if torch.cuda.is_available() else "cpu"
gpu_available = torch.cuda.is_available()
safe_dtype = torch.float16 if gpu_available else torch.float32

# -------------------- Utils --------------------
def _pick_attn_impl() -> str:
    try:
        import flash_attn  # noqa
        logger.info("âœ… flash_attn ì‚¬ìš©: flash_attention_2")
        return "flash_attention_2"
    except Exception:
        pass
    try:
        from torch.nn.functional import scaled_dot_product_attention  # noqa
        logger.info("â„¹ï¸ sdpa ì‚¬ìš© ê°€ëŠ¥")
        return "sdpa"
    except Exception:
        logger.info("â„¹ï¸ sdpa ë¶ˆê°€ â†’ eager")
        return "eager"

def _coerce_json(text: str) -> Dict[str, Any]:
    s = text.find("{"); e = text.rfind("}")
    if s != -1 and e != -1 and e > s:
        text = text[s:e+1]
    return json.loads(text)

def _contains_hangul(text: str) -> bool:
    return bool(re.search(r"[ê°€-í£]", text or ""))

def _detect_lang_safe(text: str) -> str:
    if not text or not text.strip():
        return "en"
    if _contains_hangul(text): return "ko"
    latin_chars = len(re.findall(r"[A-Za-z]", text))
    total_chars = len(re.findall(r"[A-Za-zê°€-í£]", text))
    if total_chars == 0: return "en"
    return "ko" if (latin_chars/total_chars) < 0.5 else "en"

# === Fix(4): Keep sentence boundaries when stripping ===
def _strip_math_all(s: str) -> str:
    if not s: return s
    rep = " . "
    s = re.sub(r"\$\$[\s\S]*?\$\$", rep, s)
    s = re.sub(r"\\\[[\s\S]*?\\\]", rep, s)
    s = re.sub(r"\\\([\s\S]*?\\\)", rep, s)
    s = re.sub(r"\$(?!\$)(?:[^$\\]|\\.)+\$", rep, s)
    s = re.sub(r"\\begin\{(equation\*?|align\*?|gather\*?|multline\*?|eqnarray\*?)\}[\s\S]*?\\end\{\1\}", rep, s)
    return s

def _strip_tables_figures_all(s: str) -> str:
    if not s: return s
    rep = " . "
    s = re.sub(r"\\begin\{table\*?\}[\s\S]*?\\end\{table\*?\}", rep, s)
    s = re.sub(r"\\begin\{tabularx?\*?\}[\s\S]*?\\end\{tabularx?\*?\}", rep, s)
    s = re.sub(r"\\begin\{figure\*?\}[\s\S]*?\\end\{figure\*?\}", rep, s)
    s = re.sub(r"\\includegraphics(?:\[[^\]]*\])?\{[^}]*\}", rep, s)
    s = re.sub(r"\\caption\{[^}]*\}", rep, s)
    s = re.sub(r"(?i)\b(?:figure|table)\s*\d+\s*[:.]\s*", rep, s)
    s = re.sub(r"(?:í‘œ|ê·¸ë¦¼)\s*\d+\s*[:.]\s*", rep, s)
    return s

def _normalize_bracket_tokens(text: str) -> str:
    if not text: return text
    text = re.sub(r"(?i)\bL\s*R\s*B\b", "(", text)
    text = re.sub(r"(?i)\bL\s*L\s*B\b", "(", text)
    text = re.sub(r"(?i)\bR\s*R\s*B\b", ")", text)
    return text

def _postprocess_terms(text: str) -> str:
    if not text: return text
    text = re.sub(r"(?i)\bY\s*O\s*L\s*O\b", "YOLO", text)
    text = re.sub(r"(?i)\bO\s*L\s*O\b", "YOLO", text)
    text = re.sub(r"(?i)\bm\s*A\s*P\b", "mAP", text)
    text = re.sub(r"(?i)\bR\s*-?\s*CNN\b", "R-CNN", text)
    text = re.sub(r"(?i)\bFast\s*-?\s*R\s*-?\s*CNN\b", "Fast R-CNN", text)
    text = re.sub(r"(?i)\bFaster\s*-?\s*R\s*-?\s*CNN\b", "Faster R-CNN", text)
    return text

def _auto_bold_terms(text: str) -> str:
    if not text or not EASY_AUTO_BOLD: return text
    patterns = [
        r"\b(?:YOLOv?\d+|YOLO|RetinaNet|Faster\s*R-?CNN|Fast\s*R-?CNN|R-?CNN|ResNet-?\d+|EfficientNet[-A-Z0-9]*)\b",
        r"\b(?:Transformer|ViT|CLIP|BERT|GPT(?:-\d+)?)\b",
        r"\b(?:mAP|IoU|F1|AP50|AP75|AUC|ROC|BLEU|ROUGE|PSNR|SSIM|CER|WER)\b",
        r"\b\d+\s?[xÃ—]\s?\d+\b",
        r"\b(?:Top-1|Top-5|SOTA|SotA|FPS|GFLOPs?|Params?)\b",
        r"\b(?:Llama|LLaMA|Mistral|Phi|Qwen|Gemma|Whisper)\b",
    ]
    def wrap(m: re.Match) -> str:
        g = m.group(0)
        if g.startswith("**") and g.endswith("**"): return g
        return f"**{g}**"
    for pat in patterns:
        text = re.sub(pat, wrap, text)
    return text

def _hilite_sentences(text: str, max_marks: int = 2) -> str:
    if not text or not EASY_HILITE: return text
    parts = re.split(r"(?<=[.!?])\s+|(?<=ë‹¤\.)\s+", text.strip())
    marks = 0; out = []
    key = re.compile(r"(?:ê¸°ì—¬|í•µì‹¬|ìš”ì•½|ê²°ë¡ |ì„±ëŠ¥|ê°œì„ |í–¥ìƒ|ì •í™•ë„|ìš°ìˆ˜|ë‹¬ì„±|ì¦ê°€|ê°ì†Œ|í•œê³„|ì œì•ˆ|ìš°ë¦¬\s*ëª¨ë¸|ë³¸\s*ì—°êµ¬|SOTA|improv|achiev|propos|contribut)", re.IGNORECASE)
    has_number = re.compile(r"\d+(?:\.\d+)?\s*(?:%|ì |ë°°|ms|s|fps|GFLOPs?|M|K)?", re.IGNORECASE)
    for sent in parts:
        s = sent.strip()
        if not s: continue
        if marks < max_marks and (key.search(s) or has_number.search(s)):
            out.append(f"=={s}=="); marks += 1
        else:
            out.append(s)
    return " ".join(out)

# === Fix(5): sanitize pathological repeats (e.g., flops spam) ===
def _sanitize_repeats(text: str) -> str:
    if not text: return text
    # ê°™ì€ í† í° 4íšŒ ì´ìƒ ë°˜ë³µ â†’ 3íšŒë¡œ ì¶•ì†Œ
    text = re.sub(r"(?:\b[\w\-ê°€-í£]{2,}\b)(?:\s+\1){3,}", r"\1 \1 \1", text, flags=re.IGNORECASE)
    # flops ë„ë°° ì œê±° (10íšŒ ì´ìƒ ì—°ì†)
    text = re.sub(r"(?:\b[\w\-]*flops\b[\s,.;:]*){10,}", "", text, flags=re.IGNORECASE)
    return text
# === PART 1/4 END ===

# === PART 2/4 START ===
# -------------------- Translation (LLM) --------------------
def _ensure_korean(text: str) -> str:
    if not text or not text.strip(): return text
    lang = _detect_lang_safe(text)
    latin = len(re.findall(r"[A-Za-z]", text))
    hangul = len(re.findall(r"[ê°€-í£]", text))
    high_latin_ratio = latin > 0 and (hangul == 0 or latin / max(1, latin + hangul) >= 0.4)
    if lang == "ko" and not (EASY_FORCE_KO and high_latin_ratio): return text
    if lang == "en" or (EASY_FORCE_KO and high_latin_ratio): return _translate_to_korean(text)
    if EASY_FORCE_KO and hangul < latin: return _translate_to_korean(text)
    return text

# Translation prompt uses the model itself
def _translate_to_korean(text: str) -> str:
    try:
        if not text or not text.strip(): return ""
        if model is None or tokenizer is None:
            logger.warning("ëª¨ë¸ ë¯¸ë¡œë“œ â†’ ë²ˆì—­ ìŠ¤í‚µ")
            return text
        translate_prompt = (
            "<|begin_of_text|>\n"
            "<|start_header_id|>system<|end_header_id|>\n"
            "ë„ˆëŠ” ì „ë¬¸ ë²ˆì—­ê°€ë‹¤. ì˜ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ë˜, "
            "ì „ë¬¸ ìš©ì–´ëŠ” ì›ì–´ ìœ ì§€ + ê´„í˜¸ í•´ì„¤ì„ ë§ë¶™ì¸ë‹¤. ìˆ˜ì‹/í‘œ/ê·¸ë¦¼ ì–¸ê¸‰ì€ ì œê±°.\n"
            "<|eot_id|>\n"
            "<|start_header_id|>user<|end_header_id|>\n"
            f"[INPUT]\n{text}\n\n[OUTPUT]\n"
            "<|eot_id|>\n"
        )
        inputs = tokenizer(translate_prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=min(MAX_NEW_TOKENS, 700),
                do_sample=False,                    # ì•ˆì • ìš°ì„ 
                use_cache=True,
                repetition_penalty=1.2,
                no_repeat_ngram_size=4,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        # === Fix(1): token-based slicing ===
        seq = outputs[0]
        inp_len = inputs["input_ids"].shape[1]
        gen_tokens = seq[inp_len:]
        result = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()
        # === Fix(3): Stop cut ===
        for stop in ("[/OUTPUT]", "[/output]", "<|eot_id|>"):
            p = result.find(stop)
            if p != -1:
                result = result[:p].strip()
                break
        result = _postprocess_terms(result)
        result = _strip_math_all(result); result = _strip_tables_figures_all(result)
        result = _sanitize_repeats(result)
        return result or text
    except Exception as e:
        logger.warning(f"LLM ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return text

# -------------------- I/O Models --------------------
class TextRequest(BaseModel):
    text: str
    translate: Optional[bool] = False
    style: Optional[str] = Field(default="default", description="easy ìŠ¤íƒ€ì¼: 'default' | 'three_para_ko'")
    model_config = ConfigDict(extra="allow")

class TextResponse(BaseModel):
    simplified_text: str
    translated_text: Optional[str] = None

class BatchRequest(BaseModel):
    paper_id: str = Field(..., description="ê²°ê³¼ íŒŒì¼/ê²½ë¡œ ì‹ë³„ì")
    chunks_jsonl: str = Field(..., description="ê° ë¼ì¸ì— {'text': ...} í˜•íƒœì˜ JSONL ë˜ëŠ” ê²½ë¡œ/í´ë”")
    output_dir: str = Field(..., description="ì´ë¯¸ì§€/ê²°ê³¼ ì €ì¥ ë£¨íŠ¸")

class VizResult(BaseModel):
    ok: bool = True
    index: int
    image_path: Optional[str] = None
    error: Optional[str] = None
    easy_text: Optional[str] = None
    section_title: Optional[str] = None

class BatchResult(BaseModel):
    ok: bool
    paper_id: str
    count: int
    success: int
    failed: int
    out_dir: str
    images: List[VizResult]

class TransportRequest(BaseModel):
    paper_id: str
    transport_path: str
    output_dir: Optional[str] = None

# -------------------- Model Load --------------------
def load_model():
    global model, tokenizer, gpu_available, device, safe_dtype
    logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”©: {BASE_MODEL}")
    logger.info(f"EASY_ADAPTER_DIR={ADAPTER_DIR}")
    logger.info(f"HF_HOME={os.getenv('HF_HOME')}")
    if torch.cuda.is_available():
        gpu_available = True; device = "cuda"; safe_dtype = torch.float16
        logger.info(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
    else:
        gpu_available = False; device = "cpu"; safe_dtype = torch.float32
        logger.info("âš ï¸ CPU ëª¨ë“œ")

    tokenizer_local = AutoTokenizer.from_pretrained(BASE_MODEL, token=HF_TOKEN, trust_remote_code=True, cache_dir=CACHE_DIR)
    if tokenizer_local.pad_token_id is None:
        tokenizer_local.pad_token = tokenizer_local.eos_token

    attn_impl = _pick_attn_impl()
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    try:
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL, torch_dtype=safe_dtype, trust_remote_code=True,
            attn_implementation=attn_impl, low_cpu_mem_usage=True, token=HF_TOKEN, cache_dir=CACHE_DIR,
        )
    except Exception as e:
        logger.warning(f"attn='{attn_impl}' ì‹¤íŒ¨({e}) â†’ eager")
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL, torch_dtype=safe_dtype, trust_remote_code=True,
            attn_implementation="eager", low_cpu_mem_usage=True, token=HF_TOKEN, cache_dir=CACHE_DIR,
        )

    m = base
    if ADAPTER_DIR and os.path.exists(ADAPTER_DIR):
        try:
            m = PeftModel.from_pretrained(base, os.path.abspath(ADAPTER_DIR), is_trainable=False, local_files_only=True)
            logger.info("âœ… ì–´ëŒ‘í„° ë¡œë”© OK")
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° ë¡œë”© ì‹¤íŒ¨: {e} â†’ ë² ì´ìŠ¤ ì‚¬ìš©")
            m = base

    m.eval(); m = m.to(safe_dtype).to(device)
    globals()["model"] = m; globals()["tokenizer"] = tokenizer_local
    logger.info("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")

@app.on_event("startup")
async def startup_event():
    load_model()

# -------------------- Prompts --------------------
def _clean_latex_text(text: str) -> str:
    if not text: return ""
    s = text
    if EASY_STRIP_MATH: s = _strip_math_all(s)
    s = _strip_tables_figures_all(s)
    s = re.sub(r"\\cite\{[^}]*\}|\\label\{[^}]*\}|\\ref\{[^}]*\}", " ", s)
    s = re.sub(r"\\url\{([^}]*)\}", r"(\1)", s)
    s = re.sub(r"\\(textbf|textit|emph)\{([^}]*)\}", r"\2", s)
    s = re.sub(r"^\\(sub)*section\{[^}]*\}\s*", "", s, flags=re.MULTILINE)
    s = re.sub(r"\s+", " ", s); s = re.sub(r"\s*\n\s*", "\n", s)
    return s.strip()

# === Fix(6): strict policy to avoid invention; style support ===
def _build_easy_prompt(text: str, section_title: str | None = None) -> str:
    title_line = f"[Section] {section_title}\n\n" if section_title else ""
    system_block = (
        "<|begin_of_text|>\n"
        "<|start_header_id|>system<|end_header_id|>\n"
        "You are a professional science communicator.\n"
        "Rewrite to simple, readable English for beginners.\n\n"
        "Policy (DO NOT VIOLATE):\n"
        "- Use ONLY information from [Original Text].\n"
        "- Never invent numbers, tables, versions, or comparisons not present.\n"
        "- If something is not in [Original Text], write: 'not stated in the original text'.\n"
        "- Exclude equations/tables/figures.\n"
        "<|eot_id|>\n"
    )
    user_block = (
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{title_line}[Original Text]\n{text}\n\n[OUTPUT]\n"
        "<|eot_id|>\n"
    )
    return system_block + user_block

def _build_easy_prompt_three_para_ko(text: str, section_title: str | None = None) -> str:
    title_line = f"[ì„¹ì…˜] {section_title}\n\n" if section_title else ""
    system = (
        "<|begin_of_text|>\n"
        "<|start_header_id|>system<|end_header_id|>\n"
        "ë„ˆëŠ” ê³¼í•™/AI ë‚´ìš©ì„ ì¤‘í•™ìƒë„ ì´í•´í•˜ê²Œ í•œêµ­ì–´ë¡œ ì•„ì£¼ ì‰½ê²Œ ì„¤ëª…í•œë‹¤.\n"
        "ì •í™•íˆ 3ë‹¨ë½(ê° 1~2ë¬¸ì¥). 1)ë¬´ì—‡ 2)ì™œ(ì¥ì /ë¹ ë¦„) 3)ì–´ë””ì— ì“°ë‚˜.\n"
        "ì›ë¬¸ ë°– ì •ë³´ ê¸ˆì§€. ì›ë¬¸ì— ì—†ìœ¼ë©´ 'ì›ë¬¸ì— ì—†ìŒ'ì´ë¼ê³  ì ëŠ”ë‹¤.\n"
        "<|eot_id|>\n"
    )
    user = (
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{title_line}[ì›ë¬¸]\n{text}\n\n[OUTPUT]\n"
        "<|eot_id|>\n"
    )
    return system + user
# === PART 2/4 END ===

# === PART 3/4 START ===
# -------------------- Core Rewriter --------------------
async def _rewrite_text(text: str, section_title: str = None, context_info: str = None, style: str = "default") -> str:
    if model is None or tokenizer is None:
        raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    cleaned_text = _clean_latex_text(text)
    if context_info:
        cleaned_text = f"[ë¬¸ë§¥] {context_info}\n\n" + cleaned_text

    if style == "three_para_ko":
        prompt = _build_easy_prompt_three_para_ko(cleaned_text, section_title)
    else:
        prompt = _build_easy_prompt(cleaned_text, section_title)

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=min(MAX_NEW_TOKENS, 700),
            do_sample=False,                 # === Fix(2): ì•ˆì • ëª¨ë“œ
            use_cache=True,
            repetition_penalty=1.2,
            no_repeat_ngram_size=4,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    # === Fix(1): token-based slicing ===
    seq = outputs[0]
    inp_len = inputs["input_ids"].shape[1]
    gen_tokens = seq[inp_len:]
    result = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()

    # === Fix(3): stop marker cut ===
    for stop in ("[/OUTPUT]", "[/output]", "<|eot_id|>"):
        p = result.find(stop)
        if p != -1:
            result = result[:p].strip()
            break

    # post
    result = _normalize_bracket_tokens(result)
    result = _postprocess_terms(result)
    if EASY_STRIP_MATH: result = _strip_math_all(result)
    result = _strip_tables_figures_all(result)
    result = _auto_bold_terms(result)
    result = _hilite_sentences(result, max_marks=2)
    result = _sanitize_repeats(result)
    return result

# -------------------- HTML helpers (unchanged behavior) --------------------
def _get_current_datetime() -> str:
    from datetime import datetime
    return datetime.now().strftime("%Yë…„ %mì›” %dì¼ %H:%M:%S")

def _slugify(s: str, fallback: str) -> str:
    s = re.sub(r"[^0-9A-Za-zê°€-í£\- ]", "", s or "")
    s = s.strip().replace(" ", "-")
    return s if s else fallback

def _md_to_html(md: str) -> str:
    if not md: return ""
    def _codeblock_repl(m): return f"<pre><code>{m.group(1)}</code></pre>"
    md = re.sub(r"```([\s\S]*?)```", _codeblock_repl, md)
    md = re.sub(r"^###\s*(.+)$", r"<h3>\1</h3>", md, flags=re.MULTILINE)
    md = re.sub(r"^##\s*(.+)$",  r"<h2>\1</h2>", md, flags=re.MULTILINE)
    md = re.sub(r"^#\s*(.+)$",   r"<h1>\1</h1>", md, flags=re.MULTILINE)
    md = re.sub(r"(?<!\$)\*\*([^*$]+?)\*\*(?!\$)", r"<strong>\1</strong>", md)
    md = re.sub(r"(?<!\$)\*([^*$]+?)\*(?!\$)",     r"<em>\1</em>", md)
    md = re.sub(r"`([^`]+?)`",                    r"<code>\1</code>", md)
    md = re.sub(r"==([^=]+?)==",                  r"<mark>\1</mark>", md)
    md = re.sub(r"\[([^\]]+)\]\((https?://[^\s)]+)\)", r'<a href="\2" target="_blank">\1</a>', md)
    md = re.sub(r"(https?://[^\s<>\"']+)", r'<a href="\1" target="_blank">\1</a>', md)
    lines = md.splitlines(); out, in_ul = [], False
    for ln in lines:
        if re.match(r"^\s*[-â€¢]\s+.+", ln):
            if not in_ul: out.append("<ul>")
            in_ul = True; out.append("<li>"+re.sub(r"^\s*[-â€¢]\s+", "", ln).strip()+"</li>")
        else:
            if in_ul: out.append("</ul>"); in_ul = False
            out.append(ln)
    if in_ul: out.append("</ul>")
    html = "\n".join(out)
    blocks = [b.strip() for b in re.split(r"\n\s*\n", html) if b.strip()]
    wrapped = []
    for b in blocks:
        if b.startswith(("<h1","<h2","<h3","<ul","<pre","<table","<blockquote")):
            wrapped.append(b)
        else:
            wrapped.append(f"<p>{b}</p>")
    return "\n".join(wrapped)

def _render_rich_html(text: str) -> str:
    if not text: return ""
    html_text = _md_to_html(text)
    html_text = re.sub(r'\\textbf\{([^}]+)\}', r'<strong>\1</strong>', html_text)
    html_text = re.sub(r'\\textit\{([^}]+)\}', r'<em>\1</em>', html_text)
    html_text = re.sub(r'\\(sub)*section\{([^}]+)\}', r'<h2>\2</h2>', html_text)
    html_text = re.sub(r' +', ' ', html_text)
    return html_text

def _starts_with_same_heading(html: str, title: str) -> bool:
    if not title or not html: return False
    plain = re.sub(r"<[^>]+>", "", html).strip().lower()
    t = (title or "").strip().lower()
    return plain.startswith(t) or plain[:120].startswith(t + ":")

def _save_html_results(sections: List[dict], results: List[VizResult], output_path: Path, paper_id: str):
    # TOC ì¤€ë¹„
    toc_items: List[Tuple[str, str]] = []  # (id, title)

    def _split_paragraphs_ko(text: str, min_s: int = 3, max_s: int = 5, max_chars: int = 700) -> str:
        import re as _re
        if not text:
            return ""
        s = _re.sub(r"\s+", " ", str(text).strip())
        parts = _re.split(r"(?<=[.!?])\s+|(?<=ë‹¤\.)\s+", s)
        parts = [p.strip() for p in parts if p and p.strip()]
        paras: List[str] = []
        cur: List[str] = []
        cur_len = 0
        for sent in parts:
            start_new = (len(cur) >= min_s and (len(cur) >= max_s or cur_len + len(sent) > max_chars))
            if start_new and cur:
                paras.append(" ".join(cur))
                cur = [sent]
                cur_len = len(sent)
            else:
                cur.append(sent)
                cur_len += len(sent)
        if cur:
            paras.append(" ".join(cur))
        out: List[str] = []
        for p in paras:
            if out and len(p) < 80:
                out[-1] = (out[-1] + " " + p).strip()
            else:
                out.append(p)
        return "\n\n".join(out)

    # -------- HTML í—¤ë”(ì¼ë°˜ ë¬¸ìì—´ + í† í°ì¹˜í™˜) --------
    gen_at = _get_current_datetime()
    html_header = """
<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>POLO - ì‰¬ìš´ ë…¼ë¬¸ ì„¤ëª…</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
<style>
:root {
  --bg:#f7fafc; --card:#ffffff; --text:#111827; --muted:#6b7280; --brand:#2563eb; --hi:#fff7a1;
  --code-bg:#0f172a; --code-fg:#e5e7eb;
  --sidebar-w:260px;
}
* { box-sizing:border-box; }
html { scroll-behavior:smooth; }
body {
  font-family:'Noto Sans KR','Malgun Gothic','Apple SD Gothic Neo','Segoe UI',Roboto,Arial,sans-serif;
  background:var(--bg); color:var(--text); line-height:1.85; margin:0; padding:24px;
}
.wrapper { max-width:1100px; margin:0 auto; }
.header {
  background:var(--card); border:1px solid #e5e7eb; border-radius:12px;
  padding:22px 24px; margin-bottom:16px;
}
.title { font-weight:700; font-size:22pt; margin:0 0 6px; }
.subtitle { color:var(--muted); font-size:12.5pt; }
.meta { color:var(--muted); font-size:11pt; margin-top:6px; }

/* ë ˆì´ì•„ì›ƒ: ì¢Œì¸¡ ê³ ì • TOC + ìš°ì¸¡ ë³¸ë¬¸ */
.layout { display:grid; grid-template-columns:minmax(200px,var(--sidebar-w)) 1fr; gap:24px; align-items:start; }
.toc-sidebar {
  position:sticky; top:20px; max-height:calc(100vh - 40px); overflow:auto;
  background:var(--card); border:1px solid #e5e7eb; border-radius:12px; padding:16px 14px;
}
.toc-title { font-weight:700; font-size:13.5pt; margin:2px 0 10px; }
.toc-list, .toc-sublist { list-style:none; margin:0; padding:0; }
.toc-item { margin:6px 0; }
.toc-link {
  display:block; text-decoration:none; color:#1f2937; padding:2px 4px; border-radius:6px;
  font-weight:600; font-size:12.5pt;
}
.toc-link .num { color:var(--muted); margin-right:6px; font-weight:700; }
.toc-sublist .toc-link { font-weight:500; font-size:11.5pt; color:#374151; padding-left:6px; }
.toc-link:hover { background:#f3f4f6; }
.toc-link.active { background:#eef2ff; color:#111827; box-shadow:0 0 0 2px rgba(37,99,235,.15) inset; }

.content-area { min-width:0; }

.section-card {
  background:var(--card); border:1px solid #e5e7eb; border-radius:12px;
  padding:22px 22px; margin-bottom:18px;
}
.section-card.section { border-left:4px solid var(--brand); box-shadow:0 2px 8px rgba(37,99,235,.08); }
.section-card.subsection { border-left:4px solid #10b981; box-shadow:0 2px 8px rgba(16,185,129,.08); margin-left:0; margin-right:0; }
.section-card h2 { font-size:19pt; margin:2px 0 12px; font-weight:700; letter-spacing:.2px; }
.section-card.subsection h2 { font-size:16pt; color:#059669; position:relative; padding-left:0; }
.content p { margin:10px 0 12px; }
.content ul { margin:8px 0 12px 18px; }
.content li { margin:4px 0; }
.content code {
  background:#f6f8fa; padding:2px 6px; border-radius:4px;
  font-family:ui-monospace,SFMono-Regular,Menlo,Consolas,'Liberation Mono',monospace; font-size:12pt;
}
.content pre { background:var(--code-bg); color:var(--code-fg); padding:14px; border-radius:8px; overflow:auto; }
.content strong { font-weight:700; }
.content mark { background:var(--hi); padding:0 .2em; border-radius:3px; box-shadow:0 0 0 2px rgba(255,247,161,.4) inset; }

.image-container { text-align:center; margin:18px 0 6px; padding:12px; border:1px dashed #e5e7eb; border-radius:10px; }
.image-caption { margin-top:6px; color:var(--muted); font-size:10.5pt; font-style:italic; }

.footer-actions { position:fixed; bottom:16px; right:16px; display:flex; gap:8px; z-index:5; }
.btn { appearance:none; border:none; padding:9px 14px; border-radius:10px; font-weight:700; color:white; background:#2563eb; cursor:pointer; box-shadow:0 6px 18px rgba(37,99,235,.24); }
.btn.secondary { background:#111827; }

@media print {
  .footer-actions, .toc-sidebar { display:none; }
  body { background:white; padding:0; }
  .header, .section-card { border:none; box-shadow:none; }
}
@media (max-width:980px){
  .layout { grid-template-columns:1fr; }
  .toc-sidebar { position:relative; top:auto; max-height:none; }
}
/* í˜•ê´‘íœ í† ê¸€ */
.hide-hilite mark { background:transparent; box-shadow:none; }
</style>
<script>
function downloadHTML(){
  const b=new Blob([document.documentElement.outerHTML],{type:'text/html'});
  const u=URL.createObjectURL(b); const a=document.createElement('a');
  a.href=u; a.download='polo_easy_explanation___PAPER_ID__.html';
  document.body.appendChild(a); a.click(); a.remove(); URL.revokeObjectURL(u);
}
function toggleHighlights(){
  document.body.classList.toggle('hide-hilite');
  const on=!document.body.classList.contains('hide-hilite');
  document.getElementById('toggleHi').innerText=on?'í˜•ê´‘íœ ë„ê¸°':'í˜•ê´‘íœ ì¼œê¸°';
}
document.addEventListener('DOMContentLoaded',()=>{
  const links=[...document.querySelectorAll('.toc-link')];
  const map=new Map(links.map(a=>[a.getAttribute('href').slice(1),a]));
  const obs=new IntersectionObserver(entries=>{
    entries.forEach(e=>{
      const id=e.target.id, a=map.get(id);
      if(!a) return;
      if(e.isIntersecting){
        links.forEach(x=>x.classList.remove('active'));
        a.classList.add('active');
      }
    });
  },{rootMargin:'0px 0px -70% 0px',threshold:0.1});
  window.__attachObserver=()=>document.querySelectorAll('.section-card[id]').forEach(sec=>obs.observe(sec));
});
</script>
</head>
<body>
<div class="wrapper">
  <div class="header">
    <div class="title">POLO ë…¼ë¬¸ ì´í•´ ë„ìš°ë¯¸ ë³€í™˜ ê²°ê³¼</div>
    <div class="subtitle">ë³µì¡í•œ ë…¼ë¬¸ì„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì¬êµ¬ì„±í•œ ê²°ê³¼</div>
    <div class="meta">ë…¼ë¬¸ ID: __PAPER_ID__ | ìƒì„±: __GEN_AT__</div>
  </div>
"""

    html: List[str] = []
    html.append(html_header.replace("__PAPER_ID__", paper_id).replace("__GEN_AT__", gen_at))

    # ë ˆì´ì•„ì›ƒ ì‹œì‘: ì¢Œì¸¡ TOC + ìš°ì¸¡ ë³¸ë¬¸
    html.append('<div class="layout">')

    # ===== ì¢Œì¸¡ ê³ ì • TOC =====
    html.append('<aside class="toc-sidebar">')
    html.append('<div class="toc-title">ëª©ì°¨</div>')
    html.append('<ol class="toc-list">')

    section_num = 0
    open_sub = False
    subsection_num = 0

    for i, sec in enumerate(sections):
        title = (sec.get("title") or f"Section {i+1}").strip()
        sid = _slugify(title, f"sec-{i+1}")
        section_type = sec.get("section_type", "section")
        toc_items.append((sid, title))

        if section_type == "section":
            if open_sub:
                html.append('</ol></li>')  # ì´ì „ ì„¹ì…˜ ì„œë¸Œë¦¬ìŠ¤íŠ¸ ë‹«ê¸°
                open_sub = False
            section_num += 1
            subsection_num = 0
            html.append(
                f'<li class="toc-item"><a class="toc-link" href="#{sid}">'
                f'<span class="num">{section_num}.</span>{title}</a>'
            )
            html.append('<ol class="toc-sublist">')  # ì„œë¸Œì„¹ì…˜ ë¦¬ìŠ¤íŠ¸ ì‹œì‘
            open_sub = True
        else:
            subsection_num += 1
            html.append(
                f'<li class="toc-item"><a class="toc-link" href="#{sid}">'
                f'<span class="num">{section_num}.{subsection_num}</span>{title}</a></li>'
            )

    if open_sub:
        html.append('</ol></li>')  # ë§ˆì§€ë§‰ ì„¹ì…˜ ì„œë¸Œë¦¬ìŠ¤íŠ¸ ë‹«ê¸°
    html.append('</ol>')
    html.append('</aside>')  # /toc-sidebar

    # ===== ìš°ì¸¡ ë³¸ë¬¸ ì‹œì‘ =====
    html.append('<main class="content-area">')

    # ì„¹ì…˜ë“¤
    for i, (sec, res) in enumerate(zip(sections, results)):
        title = (sec.get("title") or f"Section {i+1}").strip()
        sid = toc_items[i][0]
        section_type = sec.get("section_type", "section")

        # ìš°ì„ ìˆœìœ„: viz.easy_text -> sec.content
        raw_text = (getattr(res, "easy_text", None) or sec.get("content") or "").strip()

        def _split_ko_local(tx: str) -> str:
            return _split_paragraphs_ko(tx)

        processed_text = _split_ko_local(raw_text)
        content_html = _render_rich_html(processed_text)

        header_html = "" if _starts_with_same_heading(content_html, title) else f"<h2>{title}</h2>"

        html.append(
            f'<div class="section-card {section_type}" id="{sid}">{header_html}'
            f'<div class="content">{content_html}</div>'
        )

        # (ì˜µì…˜) ì´ë¯¸ì§€
        if res.ok and res.image_path and Path(res.image_path).exists():
            src_path = Path(res.image_path)
            dst_path = output_path.parent / src_path.name
            try:
                import shutil
                shutil.copy2(src_path, dst_path)
                logger.info(f"ğŸ“Š [EASY] ì´ë¯¸ì§€ ë³µì‚¬ ì™„ë£Œ: {src_path.name}")
            except Exception as e:
                logger.warning(f"ğŸ“Š [EASY] ì´ë¯¸ì§€ ë³µì‚¬ ì‹¤íŒ¨: {e}")
                dst_path = Path("../../viz") / paper_id / src_path.name

            html.append(f"""
<div class="image-container">
  <img src="{dst_path.name}" alt="{title} ê´€ë ¨ ì‹œê°í™”" style="max-width:100%; height:auto; border-radius:8px;" />
  <div class="image-caption">ê·¸ë¦¼ {i+1}: {title} ê´€ë ¨ ì‹œê°í™”</div>
</div>""")

        html.append("</div>")  # /section-card

    # í”Œë¡œíŒ… ì•¡ì…˜
    html.append("""
<div class="footer-actions">
  <button class="btn" onclick="downloadHTML()">HTML ì €ì¥</button>
  <button class="btn secondary" id="toggleHi" onclick="toggleHighlights()">í˜•ê´‘íœ ë„ê¸°</button>
</div>
""")

    # ë ˆì´ì•„ì›ƒ/ë³¸ë¬¸ ë‹«ê¸° + Observer attach
    html.append('</main></div>')  # </main></div.layout>
    html.append("<script>if(window.__attachObserver) window.__attachObserver();</script>")
    html.append("</div></body></html>")

    output_path.write_text("".join(html), encoding="utf-8")


# === PART 3/4 END ===

# === PART 4/4 START ===
# -------------------- Endpoints --------------------
@app.get("/")
async def root():
    return {"message": "POLO Easy Model API", "model": BASE_MODEL}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if (model is not None and tokenizer is not None) else "unavailable",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None,
        "gpu_available": gpu_available,
        "gpu_device": torch.cuda.get_device_name(0) if gpu_available else None,
        "model_name": BASE_MODEL,
        "max_new_tokens": MAX_NEW_TOKENS,
        "cache_dir": str(CACHE_DIR),
    }

@app.get("/healthz")
async def healthz():
    return await health()

@app.post("/easy", response_model=TextResponse)
async def simplify_text(request: TextRequest):
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    simplified_text = await _rewrite_text(request.text, style=request.style or "default")

    # style==three_para_koë©´ ì´ë¯¸ í•œêµ­ì–´ â†’ ë²ˆì—­ ìƒëµ
    if (request.style or "default") == "three_para_ko":
        out_text = simplified_text
    else:
        out_text = _ensure_korean(simplified_text) if request.translate or EASY_FORCE_KO else simplified_text
    return TextResponse(simplified_text=out_text, translated_text=None)

@app.post("/generate")
async def generate_json(request: TextRequest):
    start_total = time.time()
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    # ê°„ì´ ì„¹ì…˜ ì¶”ì¶œ(ì›ë³¸ ì½”ë“œ ìœ ì§€)
    def _extract_sections(src: str) -> dict:
        sections = {k: "" for k in ["abstract","introduction","methods","results","discussion","conclusion"]}
        headers = [
            ("abstract", r"^\s*abstract\b[:\-]?"),
            ("introduction", r"^\s*introduction\b[:\-]?"),
            ("methods", r"^\s*methods?\b[:\-]?|^\s*materials?\s+and\s+methods\b[:\-]?"),
            ("results", r"^\s*results?\b[:\-]?"),
            ("discussion", r"^\s*discussion\b[:\-]?"),
            ("conclusion", r"^\s*conclusion[s]?\b[:\-]?|^\s*concluding\s+remarks\b[:\-]?"),
        ]
        lines = src.splitlines(); idxs = []
        for i, line in enumerate(lines):
            for key, pat in headers:
                if re.match(pat, line.strip(), flags=re.IGNORECASE):
                    idxs.append((i, key)); break
        idxs.sort()
        for j, (start_i, key) in enumerate(idxs):
            end_i = idxs[j+1][0] if j+1 < len(idxs) else len(lines)
            chunk = "\n".join(lines[start_i+1:end_i]).strip()[:2000]
            if EASY_STRIP_MATH: chunk = _strip_math_all(chunk)
            chunk = _strip_tables_figures_all(chunk)
            sections[key] = chunk
        return sections

    extracted = _extract_sections(request.text)
    GROUND_SCHEMA = {
        "title": "",
        "authors": [],
        "abstract": {"original": "", "easy": ""},
        "introduction": {"original": "", "easy": ""},
        "methods": {"original": "", "easy": ""},
        "results": {"original": "", "easy": ""},
        "discussion": {"original": "", "easy": ""},
        "conclusion": {"original": "", "easy": ""},
        "keywords": [],
        "figures_tables": [],
        "references": [],
        "contributions": [],
        "limitations": [],
        "glossary": [],
        "plain_summary": "",
    }

    data_schema = json.loads(json.dumps(GROUND_SCHEMA))
    for k in ["abstract","introduction","methods","results","discussion","conclusion"]:
        data_schema[k]["original"] = extracted.get(k, "")

    instruction = (
        "ë„ˆëŠ” ì¹œê·¼í•œ ê³¼í•™ ì„ ìƒë‹˜ì´ë‹¤. ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆì˜ 'í‚¤/êµ¬ì¡°'ë¥¼ ê·¸ëŒ€ë¡œ ë‘ê³  'ê°’'ë§Œ ì±„ì›Œë¼. "
        "ì™¸ë¶€ ì§€ì‹/ì¶”ì¸¡ ê¸ˆì§€. ì›ë¬¸ì— ì—†ìœ¼ë©´ 'ì›ë¬¸ì— ì—†ìŒ'ì´ë¼ê³  ì ì–´ë¼. "
        "ì¶œë ¥ì€ ì˜¤ì§ 'ìœ íš¨í•œ JSON' í•˜ë‚˜ë§Œ í—ˆìš©ëœë‹¤."
    )
    schema_str = json.dumps(data_schema, ensure_ascii=False, indent=2)
    context_only = {k: extracted[k] for k in ["abstract","introduction","methods","results","discussion","conclusion"]}
    context_str = json.dumps(context_only, ensure_ascii=False, indent=2)

    prompt = f"{instruction}\n\n=== ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ===\n{schema_str}\n\n=== ì„¹ì…˜ ì›ë¬¸ ===\n{context_str}\n\n[OUTPUT]\n"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    t0 = time.time()
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            use_cache=True,
            repetition_penalty=1.2,         # === Fix(2)
            no_repeat_ngram_size=4,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    inference_time = time.time() - t0

    # === Fix(1): token-based slicing ===
    seq = outputs[0]
    inp_len = inputs["input_ids"].shape[1]
    gen_tokens = seq[inp_len:]
    raw = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()

    # === Fix(3): stop cut ===
    for stop in ("[/OUTPUT]", "[/output]", "<|eot_id|>"):
        p = raw.find(stop)
        if p != -1:
            raw = raw[:p].strip()
            break

    def _is_meaningful(d: dict) -> bool:
        try:
            sections = ["abstract","introduction","methods","results","discussion","conclusion"]
            return any(len((d.get(s, {}) or {}).get("easy", "")) > 10 for s in sections)
        except Exception:
            return False

    try:
        data = _coerce_json(raw)
        if not _is_meaningful(data):
            raise ValueError("empty_json")
    except Exception:
        strict_instruction = (
            "ìŠ¤í‚¤ë§ˆì˜ í‚¤/êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³  ê°’ë§Œ ì±„ìš´ 'ìœ íš¨í•œ JSON'ë§Œ ì¶œë ¥í•˜ë¼. ë°˜ë“œì‹œ '{'ë¡œ ì‹œì‘í•´ '}'ë¡œ ëë‚˜ì•¼ í•œë‹¤."
        )
        strict_prompt = f"{strict_instruction}\n\nìŠ¤í‚¤ë§ˆ:\n{schema_str}\n\nì„¹ì…˜ ì›ë¬¸:\n{context_str}\n\n[OUTPUT]\n"
        inputs2 = tokenizer(strict_prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs2 = {k: v.to(device) for k, v in inputs2.items()}
        with torch.inference_mode():
            outputs2 = model.generate(
                **inputs2,
                max_new_tokens=min(MAX_NEW_TOKENS, 800),
                do_sample=False,
                use_cache=True,
                repetition_penalty=1.2,
                no_repeat_ngram_size=4,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        seq2 = outputs2[0]
        inp_len2 = inputs2["input_ids"].shape[1]
        gen_tokens2 = seq2[inp_len2:]
        raw2 = tokenizer.decode(gen_tokens2, skip_special_tokens=True).strip()
        for stop in ("[/OUTPUT]", "[/output]", "<|eot_id|>"):
            p = raw2.find(stop)
            if p != -1:
                raw2 = raw2[:p].strip()
                break
        try:
            data = _coerce_json(raw2)
        except Exception:
            data = data_schema

    total_time = time.time() - start_total   # â† ê²½ê³¼ì‹œê°„ìœ¼ë¡œ ìˆ˜ì •
    data["processing_info"] = {
        "gpu_used": gpu_available,
        "inference_time": inference_time,
        "total_time": total_time,
        "max_new_tokens": MAX_NEW_TOKENS,
    }
    # postprocess easy fields
    for k in ["abstract","introduction","methods","results","discussion","conclusion"]:
        try:
            easy_val = (data.get(k, {}) or {}).get("easy", "")
            if isinstance(easy_val, str):
                t = _postprocess_terms(_normalize_bracket_tokens(easy_val))
                if EASY_STRIP_MATH: t = _strip_math_all(t)
                t = _strip_tables_figures_all(t)
                t = _auto_bold_terms(t)
                t = _hilite_sentences(t, max_marks=2)
                t = _sanitize_repeats(t)
                data[k]["easy"] = t
        except Exception:
            pass

    return data

# -------------------- (ì˜µì…˜) batch/transport/viz íŒŒíŠ¸ --------------------
# ====================== BATCH / TRANSPORT / VIZ (Optimized for KO-Easy) ======================

# ENV defaults (if not defined above)
EASY_VIZ_ENABLED = os.getenv("EASY_VIZ_ENABLED", "0").lower() in ("1", "true", "yes")

# ---------- LaTeX íŒŒì„œ: section/subsection ì¶”ì¶œ + í‘œ ë¼ì´íŠ¸ íŒŒì‹± ----------
def _extract_table_data(text: str) -> List[dict]:
    """LaTeX tabularì—ì„œ ê°„ë‹¨ ë©”íŠ¸ë¦­ í…Œì´ë¸”ì„ ì¶”ì¶œ (ì˜µì…˜)"""
    tables = []
    pat = r'\\begin\{tabular\}[^{]*\{([^}]+)\}(.*?)\\end\{tabular\}'
    for m in re.finditer(pat, text, re.DOTALL):
        content = m.group(2)
        lines = [ln.strip() for ln in content.splitlines() if ln.strip() and not ln.strip().startswith('\\hline')]
        if len(lines) < 2:  # header + at least 1 row
            continue
        headers = [h.strip().rstrip('\\') for h in lines[0].split('&')]
        rows = []
        for ln in lines[1:]:
            if '&' in ln:
                row = [c.strip().rstrip('\\') for c in ln.split('&')]
                if len(row) == len(headers):
                    rows.append(row)
        if not rows: 
            continue
        tables.append({
            "type": "metric_table",
            "headers": headers,
            "rows": rows,
        })
    return tables

def _parse_latex_sections(tex_path: Path) -> List[dict]:
    """
    merged_body.tex ê°™ì€ LaTeX ë³¸ë¬¸ì„ ì½ì–´
    - \\section{}, \\subsection{} ë‹¨ìœ„ë¡œ ë¶„í• 
    - section_type: section | subsection
    - ê° ì„¹ì…˜ raw_contentì—ì„œ í‘œ ì •ë³´(ì˜µì…˜) ì¶”ì¶œ
    """
    content = tex_path.read_text(encoding="utf-8", errors="ignore")

    sections: List[dict] = []
    cur_title = None
    cur_buf: List[str] = []
    cur_raw: List[str] = []
    section_type = "section"

    def _flush():
        if cur_title is None:
            return
        raw_txt = "\n".join(cur_raw).strip()
        clean = _clean_latex_text("\n".join(cur_buf))
        sections.append({
            "index": len(sections),
            "title": cur_title,
            "content": clean,
            "raw_content": raw_txt,
            "table_data": _extract_table_data(raw_txt),
            "section_type": section_type,
        })

    # header regex
    sec_pat  = re.compile(r"^\s*\\section\*?\{([^}]+)\}\s*$")
    sub_pat  = re.compile(r"^\s*\\subsection\*?\{([^}]+)\}\s*$")
    abs_beg  = re.compile(r"^\s*\\begin\{abstract\}")
    abs_end  = re.compile(r"^\s*\\end\{abstract\}")

    in_abstract = False
    lines = content.splitlines()
    for ln in lines:
        # abstract
        if abs_beg.match(ln):
            _flush()
            cur_title = "Abstract"
            cur_buf, cur_raw = [], []
            section_type = "section"
            in_abstract = True
            continue
        if abs_end.match(ln):
            _flush()
            cur_title = None
            cur_buf, cur_raw = [], []
            in_abstract = False
            continue

        # section/subsection
        m1 = sec_pat.match(ln)
        m2 = sub_pat.match(ln)
        if m1:
            _flush()
            cur_title = m1.group(1).strip()
            cur_buf, cur_raw = [], []
            section_type = "section"
            continue
        if m2:
            _flush()
            cur_title = m2.group(1).strip()
            cur_buf, cur_raw = [], []
            section_type = "subsection"
            continue

        # collect
        if cur_title is None:
            # ë¬¸ì„œ ì‹œì‘ë¶€(íƒ€ì´í‹€/ì €ì) ìŠ¤í‚µ
            if not any(k in ln.lower() for k in ["\\title", "\\author", "\\date", "\\maketitle"]):
                # ì²« ì„¹ì…˜ ì—†ìœ¼ë©´ ì´ˆë°˜ë¶€ë¥¼ Introductionìœ¼ë¡œ ìŠ¹ê²©
                cur_title = "Introduction"
                section_type = "section"
        if cur_title is not None:
            cur_buf.append(ln)
            cur_raw.append(ln)

    _flush()

    if not sections:
        # fallback: ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì„¹ì…˜ìœ¼ë¡œ
        clean = _clean_latex_text(content)
        sections = [{
            "index": 0,
            "title": "Full Document",
            "content": clean,
            "raw_content": content,
            "table_data": _extract_table_data(content),
            "section_type": "section",
        }]

    logger.info(f"[EASY] Parsed {len(sections)} sections from LaTeX")
    return sections

# ---------- (ì˜µì…˜) ì‹œê°í™” ì—”ì§„ í˜¸ì¶œ ----------
async def _send_to_viz(paper_id: str, index: int, easy_text_ko: str, out_dir: Path, table_data: List[dict] = None) -> Tuple[bool, Optional[str]]:
    """
    ì‹œê°í™” ì„œë²„(VIZ_MODEL_URL)ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë³´ë‚´ ì´ë¯¸ì§€ ìƒì„± (ì˜µì…˜).
    - EASY_VIZ_ENABLED=0ì´ë©´ í…ìŠ¤íŠ¸ë§Œ.
    - ì‹¤íŒ¨í•´ë„ ì‹¤íŒ¨ë¡œ ë§‰ì§€ ì•Šê³  (ok=False, image=None) ë¡œ ë¦¬í„´.
    """
    if not EASY_VIZ_ENABLED:
        return True, None

    # ê°„ë‹¨ health probe (ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ë§Œ)
    try:
        async with httpx.AsyncClient(timeout=httpx.Timeout(5.0)) as client:
            r = await client.get(f"{VIZ_MODEL_URL.rstrip('/')}/health")
            if r.status_code != 200:
                return True, None
    except Exception:
        return True, None

    # payload êµ¬ì„±
    payload = {
        "paper_id": paper_id,
        "index": index,
        "rewritten_text": easy_text_ko,
        "target_lang": "ko",
        "bilingual": "missing",
        "text_type": "easy_korean",
    }
    # í‘œ ì •ë³´ê°€ ìˆìœ¼ë©´ ê°„ë‹¨íˆ ì‹¤ì–´ë³´ë‚´ íŒíŠ¸ ì œê³µ
    if table_data:
        payload["tables"] = table_data

    try:
        timeout = httpx.Timeout(60.0, connect=10.0)
        limits  = httpx.Limits(max_keepalive_connections=2, max_connections=5)
        async with httpx.AsyncClient(timeout=timeout, limits=limits) as client:
            r = await client.post(f"{VIZ_MODEL_URL.rstrip('/')}/viz", json=payload)
            if r.status_code != 200:
                return True, None
            data = r.json()
            # image ì €ì¥
            img_path = None
            if data.get("image_base64"):
                img_path = out_dir / f"{index:06d}.png"
                img_path.write_bytes(base64.b64decode(data["image_base64"]))
                return True, str(img_path)
            if data.get("image_path"):
                # ì›ê²½ë¡œ ë³µì‚¬ ëŒ€ì‹  HTML ì˜† í´ë”ë¡œ ë§í¬í•´ë„ ë¨
                return True, data["image_path"]
            return True, None
    except Exception as e:
        logger.warning(f"[EASY] viz error: {e}")
        return False, None

# ---------- ë°°ì¹˜ ì‹¤í–‰ (/batch) ----------
class BatchRequest(BaseModel):
    paper_id: str = Field(..., description="ê²°ê³¼ íŒŒì¼/ê²½ë¡œ ì‹ë³„ì")
    chunks_jsonl: str = Field(..., description="JSONL ë‚´ìš© ë¬¸ìì—´ ë˜ëŠ” ê²½ë¡œ(íŒŒì¼/ë””ë ‰í† ë¦¬/tex)")
    output_dir: str = Field(..., description="ê²°ê³¼ ì €ì¥ ë£¨íŠ¸")
    style: Optional[str] = Field(default="three_para_ko", description="easy ìŠ¤íƒ€ì¼ (default|three_para_ko)")

class VizResult(BaseModel):
    ok: bool = True
    index: int
    image_path: Optional[str] = None
    error: Optional[str] = None
    easy_text: Optional[str] = None
    section_title: Optional[str] = None
    section_type: Optional[str] = None

class BatchResult(BaseModel):
    ok: bool
    paper_id: str
    count: int
    success: int
    failed: int
    out_dir: str
    images: List[VizResult]

@app.post("/batch", response_model=BatchResult)
async def run_batch(request: BatchRequest):
    """
    ì…ë ¥:
      - request.chunks_jsonl:
          * JSONL ë¬¸ìì—´(ê° ë¼ì¸ {"title":..., "text":...})
          * íŒŒì¼ ê²½ë¡œ(.jsonl/.jsonl.gz/.tex)
          * ë””ë ‰í† ë¦¬(ë‚´ë¶€ì— merged_body.tex ë˜ëŠ” *.jsonl íƒìƒ‰)
    ì²˜ë¦¬:
      1) ì„¹ì…˜ íŒŒì‹±(ë¬¸ë‹¨ í´ë Œì§•)
      2) ì„¹ì…˜ë³„ ì‰¬ìš´ í•œêµ­ì–´ ë³€í™˜(_rewrite_text, style ê¸°ë³¸ three_para_ko)
      3) (ì˜µì…˜) ì‹œê°í™” í˜¸ì¶œ
      4) easy_results.json / easy_results.html ì €ì¥
    """
    paper_id = request.paper_id.strip()
    base_out = Path(request.output_dir).expanduser().resolve()
    out_dir  = base_out / paper_id
    out_dir.mkdir(parents=True, exist_ok=True)

    # 1) ì…ë ¥ ì†ŒìŠ¤ ë¡œë”©
    src = (request.chunks_jsonl or "").strip()
    src_path: Optional[Path] = None
    if src:
        p = Path(src)
        if p.exists():
            src_path = p.resolve()

    sections: List[dict] = []

    def _append_from_jsonl_lines(lines: List[str]):
        for idx, ln in enumerate(lines):
            try:
                obj = json.loads(ln)
            except Exception as e:
                obj = {"title": f"Section {idx+1}", "text": f"[JSONL parse error] {e}"}
            sections.append({
                "index": idx,
                "title": obj.get("title") or f"Section {idx+1}",
                "content": obj.get("text") or "",
                "raw_content": obj.get("text") or "",
                "table_data": [],
                "section_type": "section",
            })

    try:
        if src_path is None:
            # JSONL ë‚´ìš©ì„ ì§ì ‘ ë„˜ê¸´ ì¼€ì´ìŠ¤
            lines = [ln for ln in src.splitlines() if ln.strip()]
            _append_from_jsonl_lines(lines)
        else:
            if src_path.is_dir():
                tex = src_path / "merged_body.tex"
                if tex.exists():
                    sections = _parse_latex_sections(tex)
                else:
                    hits = sorted(src_path.rglob("*.jsonl"))
                    if not hits:
                        raise ValueError("ë””ë ‰í† ë¦¬ì— merged_body.tex ë˜ëŠ” *.jsonl ì—†ìŒ")
                    lines = hits[0].read_text(encoding="utf-8", errors="ignore").splitlines()
                    _append_from_jsonl_lines([ln for ln in lines if ln.strip()])
            else:
                if src_path.suffix.lower() in (".jsonl", ".ndjson"):
                    lines = src_path.read_text(encoding="utf-8", errors="ignore").splitlines()
                    _append_from_jsonl_lines([ln for ln in lines if ln.strip()])
                elif src_path.suffix.lower() == ".gz":
                    with gzip.open(src_path, "rt", encoding="utf-8") as f:
                        lines = [ln for ln in f if ln.strip()]
                    _append_from_jsonl_lines(lines)
                elif src_path.suffix.lower() == ".tex":
                    sections = _parse_latex_sections(src_path)
                else:
                    raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ í˜•ì‹ (jsonl/jsonl.gz/tex/dir)")
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"ì…ë ¥ íŒŒì‹± ì‹¤íŒ¨: {e}")

    if not sections:
        raise HTTPException(status_code=400, detail="ì²˜ë¦¬í•  ì„¹ì…˜ ì—†ìŒ")

    logger.info(f"[EASY] batch sections = {len(sections)} (paper_id={paper_id})")

    # 2) ì„¹ì…˜ë³„ ë³€í™˜ + (3) ì‹œê°í™”
    sem = anyio.Semaphore(EASY_CONCURRENCY)
    results: List[VizResult] = [VizResult(ok=False, index=i) for i in range(len(sections))]

    async def _work(i: int, sec: dict):
        async with sem:
            title = sec.get("title") or f"Section {i+1}"
            stype = sec.get("section_type", "section")
            try:
                context_info = f"{len(sections)}ê°œ ì¤‘ {i+1}ë²ˆì§¸ ì„¹ì…˜. ìœ í˜•: {stype}."
                easy_text = await _rewrite_text(sec.get("content", ""), title, context_info, style=request.style or "three_para_ko")
            except Exception as e:
                logger.exception(f"[EASY] ì„¹ì…˜ ë³€í™˜ ì‹¤íŒ¨ idx={i}: {e}")
                results[i] = VizResult(ok=False, index=i, error=str(e), section_title=title, section_type=stype)
                return

            # viz
            ok_viz, img_path = await _send_to_viz(paper_id, i, easy_text, out_dir, sec.get("table_data", []))
            results[i] = VizResult(
                ok=True, index=i, image_path=img_path or None, error=None,
                easy_text=easy_text, section_title=title, section_type=stype
            )

    async with anyio.create_task_group() as tg:
        for i, sec in enumerate(sections):
            tg.start_soon(_work, i, sec)

    success = sum(1 for r in results if r.ok)
    failed  = len(sections) - success

    # 4) JSON ì €ì¥
    easy_json = {
        "paper_id": paper_id,
        "count": len(sections),
        "sections": [
            {
                "index": i,
                "title": sections[i].get("title"),
                "original_content": sections[i].get("content"),
                "korean_translation": (results[i].easy_text or ""),
                "image_path": (results[i].image_path or ""),
                "status": "ok" if results[i].ok else f"error: {results[i].error}",
                "section_type": results[i].section_type or sections[i].get("section_type", "section"),
            }
            for i in range(len(sections))
        ],
        "generated_at": _get_current_datetime(),
    }
    json_path = out_dir / "easy_results.json"
    json_path.write_text(json.dumps(easy_json, ensure_ascii=False, indent=2), encoding="utf-8")
    logger.info(f"[EASY] saved: {json_path}")

    # 5) HTML ì €ì¥ (ì‚¬ì´ë“œë°” TOC ë²„ì „ - ë„¤ê°€ ì´ë¯¸ ì •ì˜í•œ _save_html_results ì‚¬ìš©)
    html_path = out_dir / "easy_results.html"
    _save_html_results(sections, results, html_path, paper_id)
    (out_dir / "index.html").write_text(html_path.read_text(encoding="utf-8"), encoding="utf-8")  # index.html ë³µì œ

    return BatchResult(
        ok=True,
        paper_id=paper_id,
        count=len(sections),
        success=success,
        failed=failed,
        out_dir=str(out_dir),
        images=results,
    )

# ---------- íŒŒì¼ ê²½ìœ  ì‹¤í–‰ (/from-transport) ----------
class TransportRequest(BaseModel):
    paper_id: str
    transport_path: str
    output_dir: Optional[str] = None
    style: Optional[str] = Field(default="three_para_ko")

@app.post("/from-transport", response_model=BatchResult)
async def from_transport(request: TransportRequest):
    """
    transport_path ê²½ë¡œ(íŒŒì¼: .jsonl/.jsonl.gz/.tex)ë¥¼ ì½ì–´ /batch ë¡œ ìœ„ì„
    """
    tpath = Path(request.transport_path).expanduser().resolve()
    if not tpath.exists():
        raise HTTPException(status_code=404, detail=f"transport íŒŒì¼ ì—†ìŒ: {tpath}")

    base_out = Path(request.output_dir).expanduser().resolve() if request.output_dir else tpath.parent
    content = None

    try:
        ext = tpath.suffix.lower()
        if ext in (".jsonl", ".ndjson"):
            content = tpath.read_text(encoding="utf-8", errors="ignore")
        elif ext == ".gz":
            with gzip.open(tpath, "rt", encoding="utf-8") as f:
                content = f.read()
        elif ext == ".tex":
            # .tex â†’ ì„¹ì…˜ íŒŒì‹± í›„ JSONLë¡œ ë³€í™˜í•´ ì „ë‹¬
            secs = _parse_latex_sections(tpath)
            lines = [json.dumps({"title": s.get("title") or f"Section {i+1}", "text": s.get("content") or ""}, ensure_ascii=False)
                     for i, s in enumerate(secs)]
            content = "\n".join(lines)
        else:
            raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {tpath.name}")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"íŒŒì¼ ì½ê¸°/íŒŒì‹± ì‹¤íŒ¨: {e}")

    # /batch ìœ„ì„
    req = BatchRequest(
        paper_id=request.paper_id,
        chunks_jsonl=content,
        output_dir=str(base_out),
        style=request.style or "three_para_ko",
    )
    return await run_batch(req)
# ====================== /END BATCH / TRANSPORT / VIZ ======================

# -------------------- Run --------------------
if __name__ == "__main__":
    host = os.getenv("EASY_HOST", "0.0.0.0")
    port = int(os.getenv("EASY_PORT", "5003"))
    reload_flag = os.getenv("EASY_RELOAD", "false").lower() in ("1", "true", "yes")
    uvicorn.run("app:app", host=host, port=port, reload=reload_flag)
# === PART 4/4 END ===

