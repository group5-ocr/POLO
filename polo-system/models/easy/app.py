# -*- coding: utf-8 -*-
"""
POLO Easy Model - Grounded JSON Generator

- CUDA ìš°ì„ (ì—†ìœ¼ë©´ CPU í´ë°±)
- ì–´í…ì…˜ ë°±ì—”ë“œ: flash_attn > sdpa > eager
- 'ì‰¬ìš´ í•œêµ­ì–´ ì¬í•´ì„'ì— ìµœì í™”
- /easy, /generate, /batch ì œê³µ
"""
from __future__ import annotations

import os
import re
import json
import time
import base64
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List

import anyio
import httpx
import torch
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, ConfigDict, Field
from dotenv import load_dotenv

# -------------------- .env ë¡œë“œ (ë£¨íŠ¸ë§Œ) --------------------
logger = logging.getLogger("polo.easy")
logging.basicConfig(level=logging.INFO)

ROOT_ENV = Path(__file__).resolve().parents[2] / ".env"
if ROOT_ENV.exists():
    load_dotenv(dotenv_path=str(ROOT_ENV), override=True)
    logger.info(f"[dotenv] loaded: {ROOT_ENV}")
else:
    logger.info("[dotenv] no .env at repo root")

HF_TOKEN   = os.getenv("í—ˆê¹…í˜ì´ìŠ¤ í† í°")
BASE_MODEL = os.getenv("EASY_BASE_MODEL", "meta-llama/Llama-3.2-3B-Instruct")
ADAPTER_DIR = os.getenv(
    "EASY_ADAPTER_DIR",
    str(Path(__file__).resolve().parent.parent / "fine-tuning" / "outputs" / "llama32-3b-qlora" / "checkpoint-4000")
)
MAX_NEW_TOKENS      = int(os.getenv("EASY_MAX_NEW_TOKENS", "1200"))
VIZ_MODEL_URL       = os.getenv("VIZ_MODEL_URL", "http://localhost:5005")
EASY_CONCURRENCY    = int(os.getenv("EASY_CONCURRENCY", "8"))
EASY_BATCH_TIMEOUT  = int(os.getenv("EASY_BATCH_TIMEOUT", "600"))

# -------------------- HF ìºì‹œ ê²½ë¡œ 'ë¬´ì¡°ê±´' ì•ˆì „ í´ë”ë¡œ ê³ ì • --------------------
SAFE_CACHE_DIR = Path(__file__).resolve().parent / "hf_cache"
SAFE_CACHE_DIR.mkdir(parents=True, exist_ok=True)

def force_safe_hf_cache():
    # ì‹œìŠ¤í…œ ì „ì—­/ì‚¬ìš©ì í™˜ê²½ë³€ìˆ˜ì— ì´ìƒí•œ ê²½ë¡œ(D:\...)ê°€ ìˆì–´ë„ ì—¬ê¸°ë¡œ í†µì¼
    for k in ("HF_HOME", "TRANSFORMERS_CACHE", "HF_DATASETS_CACHE", "HF_HUB_CACHE"):
        os.environ[k] = str(SAFE_CACHE_DIR)
    logger.info(f"[hf_cache] forced cache dir â†’ {SAFE_CACHE_DIR}")

force_safe_hf_cache()
CACHE_DIR = os.environ["HF_HOME"]  # ë™ì¼ ê²½ë¡œ ì‚¬ìš©

# â›” transformers/peftëŠ” ìºì‹œ ê²½ë¡œê°€ import ì‹œì ì— êµ³ì–´ì§ˆ ìˆ˜ ìˆìŒ
#    ë°˜ë“œì‹œ ìºì‹œ ì„¸íŒ… ì´í›„ì— import
from transformers import AutoModelForCausalLM, AutoTokenizer  # noqa: E402
from peft import PeftModel  # noqa: E402

# -------------------- FastAPI --------------------
app = FastAPI(title="POLO Easy Model", version="1.3.2")

# -------------------- ì „ì—­ ìƒíƒœ --------------------
model = None
tokenizer = None
device = "cuda" if torch.cuda.is_available() else "cpu"
gpu_available = torch.cuda.is_available()
safe_dtype = torch.float16 if gpu_available else torch.float32

# -------------------- ìœ í‹¸ --------------------
def _pick_attn_impl() -> str:
    try:
        import flash_attn  # noqa: F401
        logger.info("âœ… flash_attn ì‚¬ìš©: flash_attention_2")
        return "flash_attention_2"
    except Exception:
        pass
    try:
        from torch.nn.functional import scaled_dot_product_attention  # noqa: F401
        logger.info("â„¹ï¸ sdpa ì‚¬ìš© ê°€ëŠ¥")
        return "sdpa"
    except Exception:
        logger.info("â„¹ï¸ sdpa ë¶ˆê°€ â†’ eagerë¡œ ì§„í–‰")
        return "eager"

def _coerce_json(text: str) -> Dict[str, Any]:
    s = text.find("{"); e = text.rfind("}")
    if s != -1 and e != -1 and e > s:
        text = text[s:e+1]
    return json.loads(text)

def _is_meaningful(d: dict) -> bool:
    try:
        sections = ["abstract","introduction","methods","results","discussion","conclusion"]
        return any(len((d.get(s, {}) or {}).get("easy", "")) > 10 for s in sections)
    except Exception:
        return False

def _extract_sections(src: str) -> dict:
    sections = {k: "" for k in ["abstract","introduction","methods","results","discussion","conclusion"]}
    headers = [
        ("abstract", r"^\s*abstract\b[:\-]?"),
        ("introduction", r"^\s*introduction\b[:\-]?"),
        ("methods", r"^\s*methods?\b[:\-]?|^\s*materials?\s+and\s+methods\b[:\-]?"),
        ("results", r"^\s*results?\b[:\-]?"),
        ("discussion", r"^\s*discussion\b[:\-]?"),
        ("conclusion", r"^\s*conclusion[s]?\b[:\-]?|^\s*concluding\s+remarks\b[:\-]?"),
    ]
    lines = src.splitlines()
    idxs = []
    for i, line in enumerate(lines):
        for key, pat in headers:
            if re.match(pat, line.strip(), flags=re.IGNORECASE):
                idxs.append((i, key))
                break
    idxs.sort()
    for j, (start_i, key) in enumerate(idxs):
        end_i = idxs[j+1][0] if j+1 < len(idxs) else len(lines)
        sections[key] = "\n".join(lines[start_i+1:end_i]).strip()[:2000]
    return sections

# -------------------- ìŠ¤í‚¤ë§ˆ --------------------
GROUND_SCHEMA = {
    "title": "",
    "authors": [],
    "abstract": {"original": "", "easy": ""},
    "introduction": {"original": "", "easy": ""},
    "methods": {"original": "", "easy": ""},
    "results": {"original": "", "easy": ""},
    "discussion": {"original": "", "easy": ""},
    "conclusion": {"original": "", "easy": ""},
    "keywords": [],
    "figures_tables": [],
    "references": [],
    "contributions": [],
    "limitations": [],
    "glossary": [],
    "plain_summary": "",
}

# -------------------- I/O ëª¨ë¸ --------------------
class TextRequest(BaseModel):
    text: str
    translate: Optional[bool] = False
    model_config = ConfigDict(extra="allow")

class TextResponse(BaseModel):
    simplified_text: str
    translated_text: Optional[str] = None

class BatchRequest(BaseModel):
    paper_id: str = Field(..., description="ê²°ê³¼ íŒŒì¼/ê²½ë¡œ ì‹ë³„ì")
    chunks_jsonl: str = Field(..., description="ê° ë¼ì¸ì— {'text': ...} í˜•íƒœì˜ JSONL")
    output_dir: str = Field(..., description="ì´ë¯¸ì§€/ê²°ê³¼ ì €ì¥ ë£¨íŠ¸")

class VizResult(BaseModel):
    ok: bool = True
    index: int
    image_path: Optional[str] = None
    error: Optional[str] = None

class BatchResult(BaseModel):
    ok: bool
    paper_id: str
    count: int
    success: int
    failed: int
    out_dir: str
    images: List[VizResult]

# -------------------- ëª¨ë¸ ë¡œë“œ --------------------
def load_model():
    global model, tokenizer, gpu_available, device, safe_dtype

    logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {BASE_MODEL}")
    logger.info(f"EASY_ADAPTER_DIR={ADAPTER_DIR}")
    logger.info(f"HF_HOME={os.getenv('HF_HOME')}")

    if torch.cuda.is_available():
        gpu_available = True
        device = "cuda"
        safe_dtype = torch.float16
        logger.info(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    else:
        gpu_available = False
        device = "cpu"
        safe_dtype = torch.float32
        logger.info("âš ï¸ GPU ë¯¸ì‚¬ìš© â†’ CPU ëª¨ë“œ")

    # í† í¬ë‚˜ì´ì € (ìºì‹œ ê³ ì •)
    tokenizer_local = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        token=HF_TOKEN,
        trust_remote_code=True,
        cache_dir=CACHE_DIR,
    )
    if tokenizer_local.pad_token_id is None:
        tokenizer_local.pad_token = tokenizer_local.eos_token

    attn_impl = _pick_attn_impl()
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # ë² ì´ìŠ¤ ëª¨ë¸
    try:
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            torch_dtype=safe_dtype,
            trust_remote_code=True,
            attn_implementation=attn_impl,
            low_cpu_mem_usage=True,
            token=HF_TOKEN,
            device_map=None,
            cache_dir=CACHE_DIR,
        )
    except Exception as e:
        logger.warning(f"attn='{attn_impl}' ë¡œë”© ì‹¤íŒ¨({e}) â†’ eagerë¡œ í´ë°±")
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            torch_dtype=safe_dtype,
            trust_remote_code=True,
            attn_implementation="eager",
            low_cpu_mem_usage=True,
            token=HF_TOKEN,
            device_map=None,
            cache_dir=CACHE_DIR,
        )

    # LoRA ì–´ëŒ‘í„°(ì„ íƒ)
    m = base
    if ADAPTER_DIR and os.path.exists(ADAPTER_DIR):
        try:
            m = PeftModel.from_pretrained(
                base,
                os.path.abspath(ADAPTER_DIR),
                is_trainable=False,
                local_files_only=True,
            )
            logger.info("âœ… ì–´ëŒ‘í„° ë¡œë”© ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° ë¡œë”© ì‹¤íŒ¨: {e} â†’ ë² ì´ìŠ¤ë¡œ ì§„í–‰")
            m = base
    else:
        logger.info("â„¹ï¸ ì–´ëŒ‘í„° ê²½ë¡œ ì—†ìŒ(ë² ì´ìŠ¤ë§Œ ì‚¬ìš©)")

    m.eval()
    m = m.to(safe_dtype).to(device)
    logger.info(f"ğŸ§  MODEL DEVICE: {next(m.parameters()).device}, DTYPE: {next(m.parameters()).dtype}")

    # ì „ì—­ ì£¼ì…
    globals()["model"] = m
    globals()["tokenizer"] = tokenizer_local
    logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# -------------------- ìŠ¤íƒ€íŠ¸ì—… --------------------
@app.on_event("startup")
async def startup_event():
    load_model()

# -------------------- ë‚´ë¶€ ìœ í‹¸ (ì¬í•´ì„) --------------------
def _build_easy_prompt(text: str) -> str:
    return (
        "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ **ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‰½ê³  ì¬ë¯¸ìˆê²Œ** ë³€í™˜í•´ë¼.\n\n"
        "ğŸ¯ ë³€í™˜ ì›ì¹™:\n"
        "- ì „ë¬¸ ìš©ì–´ëŠ” ì¼ìƒ ìš©ì–´ë¡œ ë°”ê¾¸ê¸° (ì˜ˆ: 'ì•Œê³ ë¦¬ì¦˜' â†’ 'ë¬¸ì œ í•´ê²° ë°©ë²•')\n"
        "- ë³µì¡í•œ ë¬¸ì¥ì€ ì§§ê³  ëª…í™•í•˜ê²Œ ë‚˜ëˆ„ê¸°\n"
        "- ì¶”ìƒì ì¸ ê°œë…ì€ êµ¬ì²´ì ì¸ ë¹„ìœ ë¡œ ì„¤ëª…í•˜ê¸°\n"
        "- ìˆ˜ì‹ì´ë‚˜ ê¸°í˜¸ëŠ” 'ì´ê²ƒì€ ~ì„ ì˜ë¯¸í•´ìš”'ë¡œ í’€ì–´ì“°ê¸°\n"
        "- í•µì‹¬ ë‚´ìš©ì€ 'ìš”ì•½í•˜ë©´'ìœ¼ë¡œ ì •ë¦¬í•˜ê¸°\n"
        "- ì–´ë ¤ìš´ ë¶€ë¶„ì€ 'ì‰½ê²Œ ë§í•˜ë©´'ìœ¼ë¡œ ë‹¤ì‹œ ì„¤ëª…í•˜ê¸°\n\n"
        "ğŸ“ ì‘ì„± ìŠ¤íƒ€ì¼:\n"
        "- ì¹œê·¼í•˜ê³  ëŒ€í™”í•˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±\n"
        "- '~í•©ë‹ˆë‹¤', '~ì…ë‹ˆë‹¤' ê°™ì€ ì¡´ëŒ“ë§ ì‚¬ìš© (ë‹¨, '~ìš”'ë¡œ ëë‚˜ì§€ ì•Šê²Œ)\n"
        "- ì¤‘ìš”í•œ ë‚´ìš©ì€ **êµµê²Œ** í‘œì‹œ\n"
        "- ë‹¨ê³„ë³„ ì„¤ëª…ì€ 1), 2), 3) í˜•íƒœë¡œ ì •ë¦¬\n\n"
        f"[ì›ë¬¸]\n{text}\n\n[ì‰¬ìš´ ì„¤ëª…]\n"
    )

async def _rewrite_text(text: str) -> str:
    if model is None or tokenizer is None:
        raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    prompt = _build_easy_prompt(text)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.05,
        )
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated[len(prompt):].strip()

# -------------------- Viz í˜¸ì¶œ --------------------
async def _send_to_viz(paper_id: str, index: int, text_ko: str, out_dir: Path) -> VizResult:
    try:
        async with httpx.AsyncClient(timeout=EASY_BATCH_TIMEOUT) as client:
            r = await client.post(
                f"{VIZ_MODEL_URL.rstrip('/')}/viz",
                json={
                    "paper_id": paper_id,
                    "index": index,
                    "rewritten_text": text_ko,
                    "target_lang": "ko",
                    "bilingual": "missing",
                    "text_type": "easy_korean",  # ì‰½ê²Œ ë³€í™˜ëœ í•œêµ­ì–´ì„ì„ ëª…ì‹œ
                },
            )
            r.raise_for_status()
            data = r.json()

        img_path = data.get("image_path")

        if not img_path and data.get("image_base64"):
            out_path = out_dir / f"{index:06d}.png"
            out_path.write_bytes(base64.b64decode(data["image_base64"]))
            return VizResult(ok=True, index=index, image_path=str(out_path))

        if not img_path and data.get("image_url"):
            out_path = out_dir / f"{index:06d}.png"
            async with httpx.AsyncClient(timeout=60) as client:
                rr = await client.get(data["image_url"])
                rr.raise_for_status()
                out_path.write_bytes(rr.content)
            return VizResult(ok=True, index=index, image_path=str(out_path))

        if img_path:
            return VizResult(ok=True, index=index, image_path=str(img_path))

        return VizResult(ok=False, index=index, error="No image_path from viz")
    except Exception as e:
        return VizResult(ok=False, index=index, error=str(e))

# -------------------- ì—”ë“œí¬ì¸íŠ¸ --------------------
@app.get("/")
async def root():
    return {"message": "POLO Easy Model API", "model": BASE_MODEL}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if (model is not None and tokenizer is not None) else "unavailable",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None,
        "gpu_available": gpu_available,
        "gpu_device": torch.cuda.get_device_name(0) if gpu_available else None,
        "model_name": BASE_MODEL,
        "max_new_tokens": MAX_NEW_TOKENS,
        "cache_dir": str(CACHE_DIR),
    }

@app.get("/healthz")
async def healthz():
    return await health()

@app.post("/easy", response_model=TextResponse)
async def simplify_text(request: TextRequest):
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    simplified_text = await _rewrite_text(request.text)
    return TextResponse(simplified_text=simplified_text, translated_text=None)

@app.post("/generate")
async def generate_json(request: TextRequest):
    start_time = time.time()
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    extracted = _extract_sections(request.text)
    data_schema = json.loads(json.dumps(GROUND_SCHEMA))  # deepcopy
    for k in ["abstract","introduction","methods","results","discussion","conclusion"]:
        data_schema[k]["original"] = extracted.get(k, "")

    instruction = (
        "ë„ˆëŠ” ì¹œê·¼í•œ ê³¼í•™ ì„ ìƒë‹˜ì´ë‹¤. ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆì˜ 'í‚¤ì™€ êµ¬ì¡°'ë¥¼ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ê³  "
        "'ê°’'ë§Œ ì±„ì›Œë¼. ì¶œë ¥ì€ ì˜¤ì§ 'ìœ íš¨í•œ JSON' í•˜ë‚˜ë§Œ í—ˆìš©ëœë‹¤(ì½”ë“œë¸”ë¡/ì„¤ëª…/ì£¼ì„ ê¸ˆì§€).\n\n"
        "ğŸ¯ ê° ì„¹ì…˜ì˜ 'easy' ì‘ì„± ë°©ë²•:\n"
        "- ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‰½ê³  ì¬ë¯¸ìˆê²Œ ë³€í™˜\n"
        "- ì „ë¬¸ ìš©ì–´ëŠ” ì¼ìƒ ìš©ì–´ë¡œ ë°”ê¾¸ê¸°\n"
        "- ë³µì¡í•œ ë‚´ìš©ì€ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…\n"
        "- êµ¬ì²´ì ì¸ ë¹„ìœ ì™€ ì˜ˆì‹œ ì‚¬ìš©\n"
        "- 'ìš”ì•½í•˜ë©´', 'ì‰½ê²Œ ë§í•˜ë©´' ê°™ì€ ì •ë¦¬ ë¬¸êµ¬ í™œìš©\n"
        "- ì¹œê·¼í•œ í†¤ìœ¼ë¡œ 4-8ë¬¸ì¥ ì‘ì„± (ì¡´ëŒ“ë§ '~í•©ë‹ˆë‹¤', '~ì…ë‹ˆë‹¤' ì‚¬ìš©, '~ìš”'ë¡œ ëë‚˜ì§€ ì•Šê²Œ)\n\n"
        "originalì´ ë¹„ì–´ ìˆìœ¼ë©´ í•´ë‹¹ 'easy'ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ë‚¨ê²¨ë¼. ì™¸ë¶€ ì§€ì‹/ì¶”ì¸¡ ê¸ˆì§€."
    )

    schema_str = json.dumps(data_schema, ensure_ascii=False, indent=2)
    context_only = {k: extracted[k] for k in ["abstract","introduction","methods","results","discussion","conclusion"]}
    context_str = json.dumps(context_only, ensure_ascii=False, indent=2)

    prompt = (
        f"{instruction}\n\n=== ì¶œë ¥ ìŠ¤í‚¤ë§ˆ(ê°’ë§Œ ì±„ì›Œë¼) ===\n{schema_str}\n\n"
        f"=== ì„¹ì…˜ë³„ original (ì´ í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©) ===\n{context_str}\n\n"
        "JSONë§Œ ì¶œë ¥:"
    )

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    t0 = time.time()
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            use_cache=True,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    inference_time = time.time() - t0

    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    raw = generated[len(prompt):].strip()

    try:
        data = _coerce_json(raw)
        if not _is_meaningful(data):
            raise ValueError("empty_json")
    except Exception:
        strict_instruction = (
            "ìŠ¤í‚¤ë§ˆì˜ í‚¤/êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³  ê°’ë§Œ ì±„ìš´ 'ìœ íš¨í•œ JSON'ë§Œ ì¶œë ¥í•˜ë¼. "
            "ë°˜ë“œì‹œ '{'ë¡œ ì‹œì‘í•´ '}'ë¡œ ëë‚˜ì•¼ í•œë‹¤. ì™¸ë¶€ì§€ì‹/ì¶”ì¸¡ ê¸ˆì§€."
        )
        strict_prompt = f"{strict_instruction}\n\nìŠ¤í‚¤ë§ˆ:\n{schema_str}\n\nì„¹ì…˜ original:\n{context_str}\n\nJSONë§Œ ì¶œë ¥:"
        inputs2 = tokenizer(strict_prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs2 = {k: v.to(device) for k, v in inputs2.items()}
        with torch.inference_mode():
            outputs2 = model.generate(
                **inputs2,
                max_new_tokens=min(MAX_NEW_TOKENS, 800),
                do_sample=False,
                use_cache=True,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        gen2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)
        raw2 = gen2[len(strict_prompt):].strip()
        try:
            data = _coerce_json(raw2)
        except Exception:
            data = data_schema

    total_time = time.time() - start_time
    data["processing_info"] = {
        "gpu_used": gpu_available,
        "inference_time": inference_time,
        "total_time": total_time,
        "input_length": len(request.text),
        "output_length": len(str(data)),
    }
    return data

@app.post("/batch", response_model=BatchResult)
async def batch_generate(req: BatchRequest):
    jsonl_path = Path(req.chunks_jsonl).resolve()
    out_dir = Path(req.output_dir).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    if not jsonl_path.exists():
        raise HTTPException(status_code=400, detail=f"JSONL not found: {jsonl_path}")

    # JSONL ë¡œë“œ
    items: List[dict] = []
    with jsonl_path.open("r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            try:
                obj = json.loads(line)
                text = obj.get("text")
                if not isinstance(text, str) or not text.strip():
                    logger.warning(f"[batch] ë¼ì¸ {i}: text ë¹„ì–´ìˆìŒ â†’ skip")
                    continue
                items.append({"index": i, "text": text})
            except Exception as e:
                logger.warning(f"[batch] ë¼ì¸ {i}: JSON íŒŒì‹± ì‹¤íŒ¨ â†’ skip ({e})")

    sem = anyio.Semaphore(EASY_CONCURRENCY)
    results: List[VizResult] = []

    async def worker(item: dict):
        async with sem:
            idx = item["index"]
            try:
                ko = await _rewrite_text(item["text"])
                vz = await _send_to_viz(req.paper_id, idx, ko, out_dir)
                results.append(vz)
            except Exception as e:
                results.append(VizResult(ok=False, index=idx, error=str(e)))

    async with anyio.create_task_group() as tg:
        for item in items:
            tg.start_soon(worker, item)

    ok_cnt = sum(1 for r in results if r.ok)
    fail_cnt = len(results) - ok_cnt
    return BatchResult(
        ok=fail_cnt == 0,
        paper_id=req.paper_id,
        count=len(items),
        success=ok_cnt,
        failed=fail_cnt,
        out_dir=str(out_dir),
        images=sorted(results, key=lambda r: r.index),
    )

# -------------------- main --------------------
if __name__ == "__main__":
    try:
        if torch.cuda.is_available():
            print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
            print("ğŸ”§ ë””ë°”ì´ìŠ¤: cuda, dtype: float16")
        else:
            print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            print("ğŸ”§ ë””ë°”ì´ìŠ¤: cpu, dtype: float32")

        print("ğŸš€ Easy Model ì„œë²„ ì‹œì‘ ì¤‘...")
        uvicorn.run(app, host="0.0.0.0", port=5003)
    except Exception as e:
        print(f"âŒ Easy Model ì‹œì‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
