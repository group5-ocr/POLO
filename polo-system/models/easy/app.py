"""
POLO Easy Model - ë…¼ë¬¸ì„ ì‰½ê²Œ í’€ì–´ ì„¤ëª…í•˜ëŠ” LLM ì„œë¹„ìŠ¤
"""
import os
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from googletrans import Translator
from dotenv import load_dotenv
import json
import time
import logging

# --- í™˜ê²½ ë³€ìˆ˜ ---
BASE_MODEL = os.getenv("EASY_BASE_MODEL", "meta-llama/Llama-3.2-3B-Instruct")

# ê¸°ë³¸ ì–´ëŒ‘í„° ê²½ë¡œ: fine-tuning ê²°ê³¼ë¬¼(checkpoint-600)ì„ ì°¸ì¡°
_DEFAULT_ADAPTER_DIR = os.path.abspath(
    os.path.join(
        os.path.dirname(__file__),
        "..", "fine-tuning", "outputs", "llama32-3b-qlora", "checkpoint-600",
    )
)
ADAPTER_DIR = os.getenv("EASY_ADAPTER_DIR", _DEFAULT_ADAPTER_DIR)

# polo-system ë£¨íŠ¸ì˜ .env ë¡œë“œ (ëª¨ë¸ì„ easy ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•´ë„ ì¸ì‹ë˜ë„ë¡)
_ENV_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".env"))
load_dotenv(_ENV_PATH)

# Hugging Face í† í° (ê°€ë“œ ë¦¬í¬ ì ‘ê·¼ìš©)
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

app = FastAPI(title="POLO Easy Model", version="1.0.0")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
translator = Translator()
gpu_available = False

class TextRequest(BaseModel):
    text: str
    translate: Optional[bool] = False

class TextResponse(BaseModel):
    simplified_text: str
    translated_text: Optional[str] = None

def load_model():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    global model, tokenizer, gpu_available
    
    logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {BASE_MODEL}")
    
    # GPU ìƒíƒœ í™•ì¸
    gpu_available = torch.cuda.is_available()
    if gpu_available:
        logger.info(f"ğŸš€ GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        logger.info(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        logger.warning("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=HF_TOKEN)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ëª¨ë¸ ë¡œë“œ: ë¡œì»¬ ì„œë¹™ì€ bitsandbytes ë¯¸ì‚¬ìš© â†’ bfloat16 ê³ ì •(GPU), CPUëŠ” float32
    safe_dtype = torch.bfloat16 if gpu_available else torch.float32
    logger.info(f"ğŸ§  ëª¨ë¸ ë¡œë”© ì¤‘... (dtype: {safe_dtype})")
    
    # accelerate/meta í…ì„œ ê²½ë¡œë¥¼ í”¼í•˜ê¸° ìœ„í•´ device_map/low_cpu_mem_usage ë¹„í™œì„±í™”
    base = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=safe_dtype,
        trust_remote_code=True,
        attn_implementation="eager",
        token=HF_TOKEN,
    )
    if gpu_available:
        base.to("cuda")
        logger.info("ğŸ¯ ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì™„ë£Œ")
    
    # ì–´ëŒ‘í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œ (QLoRA ê°€ì¤‘ì¹˜)
    if ADAPTER_DIR and os.path.exists(ADAPTER_DIR):
        logger.info(f"ğŸ”„ ì–´ëŒ‘í„° ë¡œë”© ì¤‘: {ADAPTER_DIR}")
        model = PeftModel.from_pretrained(base, ADAPTER_DIR, is_trainable=False)
        if gpu_available:
            model.to("cuda")
        logger.info("âœ… ì–´ëŒ‘í„° ë¡œë”© ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ ì–´ëŒ‘í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆœìˆ˜ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
        model = base
    
    model.eval()
    logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    load_model()

@app.get("/")
async def root():
    return {"message": "POLO Easy Model API", "model": BASE_MODEL}

@app.post("/simplify", response_model=TextResponse)
async def simplify_text(request: TextRequest):
    """í…ìŠ¤íŠ¸ë¥¼ ì‰½ê²Œ í’€ì–´ ì„¤ëª…"""
    try:
        if model is None or tokenizer is None:
            raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ ë…¼ë¬¸ ë‚´ìš©ì„ ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”:

{request.text}

ì‰¬ìš´ ì„¤ëª…:"""
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        
        # GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        simplified_text = generated_text[len(prompt):].strip()
        
        # ë²ˆì—­ (ìš”ì²­ëœ ê²½ìš°)
        translated_text = None
        if request.translate:
            try:
                translation = translator.translate(simplified_text, dest='ko')
                translated_text = translation.text
            except Exception as e:
                print(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
        
        return TextResponse(
            simplified_text=simplified_text,
            translated_text=translated_text
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/generate")
async def generate_json(request: TextRequest):
    """ë…¼ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì„¹ì…˜ë³„ë¡œ ì‰½ê²Œ ì¬í•´ì„í•œ JSON ìƒì„±"""
    start_time = time.time()
    try:
        if model is None or tokenizer is None:
            raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        logger.info("ğŸš€ JSON ìƒì„± ì‹œì‘")
        logger.info(f"ğŸ“Š ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(request.text)} ë¬¸ì")
        logger.info(f"ğŸ¯ GPU ì‚¬ìš©: {gpu_available}")

        json_schema = {
            "title": "",  # ë…¼ë¬¸ ì œëª©(ì›ë¬¸ ì¶”ì¶œ ë¶ˆê°€ ì‹œ ìš”ì•½ ê¸°ë°˜ ìƒì„±)
            "authors": [],  # ì €ì ëª©ë¡(ì•Œ ìˆ˜ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
            "abstract": {"original": "", "easy": ""},
            "introduction": {"original": "", "easy": ""},
            "methods": {"original": "", "easy": ""},
            "results": {"original": "", "easy": ""},
            "discussion": {"original": "", "easy": ""},
            "conclusion": {"original": "", "easy": ""},
            "keywords": [],
            "figures_tables": [],  # {label, caption, easy}
            "references": [],
            "contributions": [],  # í•µì‹¬ ê¸°ì—¬ í¬ì¸íŠ¸ë¥¼ ì‰¬ìš´ ë¬¸ì¥ìœ¼ë¡œ
            "limitations": [],
            "glossary": [],  # ì¤‘ìš” ìš©ì–´ {term, definition}
            "plain_summary": ""  # ì „ì²´ë¥¼ ì¼ë°˜ì¸ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ 5-7ë¬¸ì¥ ìš”ì•½
        }

        instruction = (
            "ë„ˆëŠ” ê³¼í•™ ì»¤ë®¤ë‹ˆì¼€ì´í„°ë‹¤. ì‚¬ìš©ìê°€ ì œê³µí•œ ë…¼ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ëˆ„êµ¬ë‚˜ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‰½ê²Œ ì¬í•´ì„í•œ JSONì„ ë§Œë“¤ì–´ë¼. "
            "ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ê³ , ë§ˆí¬ë‹¤ìš´ì´ë‚˜ ì¶”ê°€ ì„¤ëª…ì€ ì ˆëŒ€ ë„£ì§€ ë§ˆë¼. "
            "ê° ì„¹ì…˜ì˜ 'original'ì—ëŠ” ì›ë¬¸ì—ì„œ í•´ë‹¹ë˜ëŠ” í•µì‹¬ ë¬¸ì¥ì„ 2-4ë¬¸ì¥ìœ¼ë¡œ ë½‘ê±°ë‚˜ ìš”ì•½í•˜ê³ , 'easy'ì—ëŠ” ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ í’€ì–´ì¨ë¼. "
            "ì›ë¬¸ì— íŠ¹ì • ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ë¹ˆ ë°°ì—´ì„ ì‚¬ìš©í•˜ë¼. í‚¤ ì´ë¦„ì€ ìŠ¤í‚¤ë§ˆì™€ ì •í™•íˆ ê°™ì•„ì•¼ í•œë‹¤."
        )

        schema_str = json.dumps(json_schema, ensure_ascii=False, indent=2)

        prompt = f"""{instruction}

ë‹¤ìŒì€ ì¶œë ¥í•´ì•¼ í•  JSON ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œë‹¤. í‚¤ ì´ë¦„ê³¼ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ë˜, ê°’ì„ ì±„ì›Œë¼:
{schema_str}

ì•„ë˜ëŠ” ì‚¬ìš©ìê°€ ì œê³µí•œ ë…¼ë¬¸ í…ìŠ¤íŠ¸ë‹¤:
"""

        logger.info("ğŸ“ í† í¬ë‚˜ì´ì§• ì‹œì‘...")
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            prompt + request.text,
            return_tensors="pt",
            truncation=True,
            max_length=2048,
        )

        if gpu_available:
            inputs = {k: v.cuda() for k, v in inputs.items()}
            logger.info("ğŸ¯ ì…ë ¥ì„ GPUë¡œ ì´ë™ ì™„ë£Œ")

        logger.info("ğŸ§  ëª¨ë¸ ì¶”ë¡  ì‹œì‘...")
        inference_start = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=768,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
            )

        inference_time = time.time() - inference_start
        logger.info(f"âš¡ ì¶”ë¡  ì™„ë£Œ: {inference_time:.2f}ì´ˆ")

        logger.info("ğŸ“„ ë””ì½”ë”© ì‹œì‘...")
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        raw = generated[len(prompt):].strip()

        # JSON ê°•ì œ íŒŒì‹±: ì²« { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€ ìë¥´ê³  íŒŒì‹± ì‹œë„
        def coerce_json(text: str):
            start = text.find("{")
            end = text.rfind("}")
            if start != -1 and end != -1 and end > start:
                text = text[start:end+1]
            return json.loads(text)

        try:
            data = coerce_json(raw)
            logger.info("âœ… JSON íŒŒì‹± ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ, ì•ˆì „í•œ ìµœì†Œ êµ¬ì¡° ë°˜í™˜
            data = json_schema
            data["plain_summary"] = raw[:1000]

        total_time = time.time() - start_time
        logger.info(f"ğŸ‰ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        data["processing_info"] = {
            "gpu_used": gpu_available,
            "inference_time": inference_time,
            "total_time": total_time,
            "input_length": len(request.text),
            "output_length": len(str(data))
        }

        return data
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ JSON ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"JSON ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None,
        "gpu_available": gpu_available,
        "gpu_device": torch.cuda.get_device_name(0) if gpu_available else None,
        "model_name": BASE_MODEL
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=5003)
