# -*- coding: utf-8 -*-
"""
POLO Easy Model - Grounded JSON Generator

- CUDA ìš°ì„ (ì—†ìœ¼ë©´ CPU í´ë°±)
- ì–´í…ì…˜ ë°±ì—”ë“œ: flash_attn > sdpa > eager
- 'ì‰¬ìš´ í•œêµ­ì–´ ì¬í•´ì„'ì— ìµœì í™”
- /easy, /generate, /batch ì œê³µ
"""
from __future__ import annotations

import os
import re
import json
import time
import base64
import gzip
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
from googletrans import Translator

import anyio
import httpx
import torch
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, ConfigDict, Field
from dotenv import load_dotenv

# -------------------- .env ë¡œë“œ (ë£¨íŠ¸ë§Œ) --------------------
logger = logging.getLogger("polo.easy")
logging.basicConfig(level=logging.INFO)

ROOT_ENV = Path(__file__).resolve().parents[2] / ".env"
if ROOT_ENV.exists():
    load_dotenv(dotenv_path=str(ROOT_ENV), override=True)
    logger.info(f"[dotenv] loaded: {ROOT_ENV}")
else:
    logger.info("[dotenv] no .env at repo root")

HF_TOKEN   = os.getenv("í—ˆê¹…í˜ì´ìŠ¤ í† í°")
BASE_MODEL = os.getenv("EASY_BASE_MODEL", "meta-llama/Llama-3.2-3B-Instruct")
ADAPTER_DIR = os.getenv(
    "EASY_ADAPTER_DIR",
    str(Path(__file__).resolve().parent.parent / "fine-tuning" / "outputs" / "llama32-3b-qlora" / "checkpoint-4000")
)
MAX_NEW_TOKENS      = int(os.getenv("EASY_MAX_NEW_TOKENS", "1200"))
VIZ_MODEL_URL       = os.getenv("VIZ_MODEL_URL", "http://localhost:5005")
EASY_CONCURRENCY    = int(os.getenv("EASY_CONCURRENCY", "8"))
EASY_BATCH_TIMEOUT  = int(os.getenv("EASY_BATCH_TIMEOUT", "600"))

# -------------------- HF ìºì‹œ ê²½ë¡œ 'ë¬´ì¡°ê±´' ì•ˆì „ í´ë”ë¡œ ê³ ì • --------------------
SAFE_CACHE_DIR = Path(__file__).resolve().parent / "hf_cache"
SAFE_CACHE_DIR.mkdir(parents=True, exist_ok=True)

def force_safe_hf_cache():
    # ì‹œìŠ¤í…œ ì „ì—­/ì‚¬ìš©ì í™˜ê²½ë³€ìˆ˜ì— ì´ìƒí•œ ê²½ë¡œ(D:\...)ê°€ ìˆì–´ë„ ì—¬ê¸°ë¡œ í†µì¼
    for k in ("HF_HOME", "TRANSFORMERS_CACHE", "HF_DATASETS_CACHE", "HF_HUB_CACHE"):
        os.environ[k] = str(SAFE_CACHE_DIR)
    logger.info(f"[hf_cache] forced cache dir â†’ {SAFE_CACHE_DIR}")

force_safe_hf_cache()
CACHE_DIR = os.environ["HF_HOME"]  # ë™ì¼ ê²½ë¡œ ì‚¬ìš©

# â›” transformers/peftëŠ” ìºì‹œ ê²½ë¡œê°€ import ì‹œì ì— êµ³ì–´ì§ˆ ìˆ˜ ìˆìŒ
#    ë°˜ë“œì‹œ ìºì‹œ ì„¸íŒ… ì´í›„ì— import
from transformers import AutoModelForCausalLM, AutoTokenizer  # noqa: E402
from peft import PeftModel  # noqa: E402

# -------------------- FastAPI --------------------
app = FastAPI(title="POLO Easy Model", version="1.3.2")

# -------------------- ì „ì—­ ìƒíƒœ --------------------
model = None
tokenizer = None
device = "cuda" if torch.cuda.is_available() else "cpu"
gpu_available = torch.cuda.is_available()
safe_dtype = torch.float16 if gpu_available else torch.float32
translator = Translator()

# -------------------- ìœ í‹¸ --------------------
def _pick_attn_impl() -> str:
    try:
        import flash_attn  # noqa: F401
        logger.info("âœ… flash_attn ì‚¬ìš©: flash_attention_2")
        return "flash_attention_2"
    except Exception:
        pass
    try:
        from torch.nn.functional import scaled_dot_product_attention  # noqa: F401
        logger.info("â„¹ï¸ sdpa ì‚¬ìš© ê°€ëŠ¥")
        return "sdpa"
    except Exception:
        logger.info("â„¹ï¸ sdpa ë¶ˆê°€ â†’ eagerë¡œ ì§„í–‰")
        return "eager"

def _coerce_json(text: str) -> Dict[str, Any]:
    s = text.find("{"); e = text.rfind("}")
    if s != -1 and e != -1 and e > s:
        text = text[s:e+1]
    return json.loads(text)

def _is_meaningful(d: dict) -> bool:
    try:
        sections = ["abstract","introduction","methods","results","discussion","conclusion"]
        return any(len((d.get(s, {}) or {}).get("easy", "")) > 10 for s in sections)
    except Exception:
        return False

def _translate_to_korean(text: str) -> str:
    """Google Translatorë¥¼ ì‚¬ìš©í•´ì„œ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
    try:
        if not text or not text.strip():
            return ""
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ ë²ˆì—­
        if len(text) > 4000:  # Google Translator ì œí•œ
            text = text[:4000] + "..."
        
        result = translator.translate(text, dest='ko', src='en')
        return result.text
    except Exception as e:
        logger.warning(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return text  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

def _extract_sections(src: str) -> dict:
    sections = {k: "" for k in ["abstract","introduction","methods","results","discussion","conclusion"]}
    headers = [
        ("abstract", r"^\s*abstract\b[:\-]?"),
        ("introduction", r"^\s*introduction\b[:\-]?"),
        ("methods", r"^\s*methods?\b[:\-]?|^\s*materials?\s+and\s+methods\b[:\-]?"),
        ("results", r"^\s*results?\b[:\-]?"),
        ("discussion", r"^\s*discussion\b[:\-]?"),
        ("conclusion", r"^\s*conclusion[s]?\b[:\-]?|^\s*concluding\s+remarks\b[:\-]?"),
    ]
    lines = src.splitlines()
    idxs = []
    for i, line in enumerate(lines):
        for key, pat in headers:
            if re.match(pat, line.strip(), flags=re.IGNORECASE):
                idxs.append((i, key))
                break
    idxs.sort()
    for j, (start_i, key) in enumerate(idxs):
        end_i = idxs[j+1][0] if j+1 < len(idxs) else len(lines)
        sections[key] = "\n".join(lines[start_i+1:end_i]).strip()[:2000]
    return sections

# -------------------- ìŠ¤í‚¤ë§ˆ --------------------
GROUND_SCHEMA = {
    "title": "",
    "authors": [],
    "abstract": {"original": "", "easy": ""},
    "introduction": {"original": "", "easy": ""},
    "methods": {"original": "", "easy": ""},
    "results": {"original": "", "easy": ""},
    "discussion": {"original": "", "easy": ""},
    "conclusion": {"original": "", "easy": ""},
    "keywords": [],
    "figures_tables": [],
    "references": [],
    "contributions": [],
    "limitations": [],
    "glossary": [],
    "plain_summary": "",
}

# -------------------- I/O ëª¨ë¸ --------------------
class TextRequest(BaseModel):
    text: str
    translate: Optional[bool] = False
    model_config = ConfigDict(extra="allow")

class TextResponse(BaseModel):
    simplified_text: str
    translated_text: Optional[str] = None

class BatchRequest(BaseModel):
    paper_id: str = Field(..., description="ê²°ê³¼ íŒŒì¼/ê²½ë¡œ ì‹ë³„ì")
    chunks_jsonl: str = Field(..., description="ê° ë¼ì¸ì— {'text': ...} í˜•íƒœì˜ JSONL")
    output_dir: str = Field(..., description="ì´ë¯¸ì§€/ê²°ê³¼ ì €ì¥ ë£¨íŠ¸")

class VizResult(BaseModel):
    ok: bool = True
    index: int
    image_path: Optional[str] = None
    error: Optional[str] = None
    easy_text: Optional[str] = None
    section_title: Optional[str] = None

class BatchResult(BaseModel):
    ok: bool
    paper_id: str
    count: int
    success: int
    failed: int
    out_dir: str
    images: List[VizResult]

class TransportRequest(BaseModel):
    paper_id: str
    transport_path: str
    output_dir: Optional[str] = None

# -------------------- ëª¨ë¸ ë¡œë“œ --------------------
def load_model():
    global model, tokenizer, gpu_available, device, safe_dtype

    logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {BASE_MODEL}")
    logger.info(f"EASY_ADAPTER_DIR={ADAPTER_DIR}")
    logger.info(f"HF_HOME={os.getenv('HF_HOME')}")

    if torch.cuda.is_available():
        gpu_available = True
        device = "cuda"
        safe_dtype = torch.float16
        logger.info(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    else:
        gpu_available = False
        device = "cpu"
        safe_dtype = torch.float32
        logger.info("âš ï¸ GPU ë¯¸ì‚¬ìš© â†’ CPU ëª¨ë“œ")

    # í† í¬ë‚˜ì´ì € (ìºì‹œ ê³ ì •)
    tokenizer_local = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        token=HF_TOKEN,
        trust_remote_code=True,
        cache_dir=CACHE_DIR,
    )
    if tokenizer_local.pad_token_id is None:
        tokenizer_local.pad_token = tokenizer_local.eos_token

    attn_impl = _pick_attn_impl()
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # ë² ì´ìŠ¤ ëª¨ë¸
    try:
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            torch_dtype=safe_dtype,
            trust_remote_code=True,
            attn_implementation=attn_impl,
            low_cpu_mem_usage=True,
            token=HF_TOKEN,
            device_map=None,
            cache_dir=CACHE_DIR,
        )
    except Exception as e:
        logger.warning(f"attn='{attn_impl}' ë¡œë”© ì‹¤íŒ¨({e}) â†’ eagerë¡œ í´ë°±")
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            torch_dtype=safe_dtype,
            trust_remote_code=True,
            attn_implementation="eager",
            low_cpu_mem_usage=True,
            token=HF_TOKEN,
            device_map=None,
            cache_dir=CACHE_DIR,
        )

    # LoRA ì–´ëŒ‘í„°(ì„ íƒ)
    m = base
    if ADAPTER_DIR and os.path.exists(ADAPTER_DIR):
        try:
            m = PeftModel.from_pretrained(
                base,
                os.path.abspath(ADAPTER_DIR),
                is_trainable=False,
                local_files_only=True,
            )
            logger.info("âœ… ì–´ëŒ‘í„° ë¡œë”© ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° ë¡œë”© ì‹¤íŒ¨: {e} â†’ ë² ì´ìŠ¤ë¡œ ì§„í–‰")
            m = base
    else:
        logger.info("â„¹ï¸ ì–´ëŒ‘í„° ê²½ë¡œ ì—†ìŒ(ë² ì´ìŠ¤ë§Œ ì‚¬ìš©)")

    m.eval()
    m = m.to(safe_dtype).to(device)
    logger.info(f"ğŸ§  MODEL DEVICE: {next(m.parameters()).device}, DTYPE: {next(m.parameters()).dtype}")

    # ì „ì—­ ì£¼ì…
    globals()["model"] = m
    globals()["tokenizer"] = tokenizer_local
    logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# -------------------- ìŠ¤íƒ€íŠ¸ì—… --------------------
@app.on_event("startup")
async def startup_event():
    load_model()

# -------------------- ë‚´ë¶€ ìœ í‹¸ (ì¬í•´ì„) --------------------
def _build_easy_prompt(text: str) -> str:
    return (
        "ë‹¤ìŒ ë…¼ë¬¸ í…ìŠ¤íŠ¸ë¥¼ **ì¼ë°˜ì¸ë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆê²Œ** ì¬í•´ì„í•´ì£¼ì„¸ìš”.\n\n"
        "ğŸ¯ ë³€í™˜ ì›ì¹™:\n"
        "- ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜, ì „ë¬¸ ìš©ì–´ë¥¼ ì‰¬ìš´ ë§ë¡œ ë°”ê¿”ì£¼ì„¸ìš”\n"
        "- ë³µì¡í•œ ë¬¸ì¥ì€ ì—¬ëŸ¬ ê°œì˜ ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”\n"
        "- ìˆ˜ì‹ì´ë‚˜ ê¸°í˜¸ëŠ” 'ì´ê²ƒì€ ~ì„ ì˜ë¯¸í•©ë‹ˆë‹¤'ë¡œ í’€ì–´ì“°ì„¸ìš”\n"
        "- ë…¼ë¬¸ì—ì„œ ì„¤ëª…í•˜ëŠ” ë°©ë²•ì´ë‚˜ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\n"
        "- ë…¼ë¬¸ì˜ ê²°ë¡ ì´ë‚˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ê°•ì¡°í•´ì£¼ì„¸ìš”\n"
        "- LaTeX ëª…ë ¹ì–´(\\begin, \\end, \\ref ë“±)ëŠ” ë¬´ì‹œí•˜ê³  ì‹¤ì œ ë‚´ìš©ë§Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\n"
        "- ë…¼ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”\n"
        "- ë…¼ë¬¸ì˜ ì›ë˜ ì˜ë¯¸ë¥¼ ì •í™•íˆ ì „ë‹¬í•´ì£¼ì„¸ìš”\n"
        "- ë°˜ë³µì ì¸ ë‚´ìš©ì€ í•œ ë²ˆë§Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\n"
        "- 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\n\n"
        "ğŸ“ ì‘ì„± ìŠ¤íƒ€ì¼:\n"
        "- ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”\n"
        "- '~í•©ë‹ˆë‹¤', '~ì…ë‹ˆë‹¤' ê°™ì€ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš” (ë‹¨, '~ìš”'ë¡œ ëë‚˜ì§€ ì•Šê²Œ)\n"
        "- ì¤‘ìš”í•œ ë‚´ìš©ì€ **êµµê²Œ** í‘œì‹œí•´ì£¼ì„¸ìš”\n"
        "- ë…¼ë¬¸ì˜ ë…¼ë¦¬ì  íë¦„ì„ ë”°ë¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n"
        "- êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ë¹„ìœ ë¥¼ ì‚¬ìš©í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”\n\n"
        f"[ë…¼ë¬¸ ì›ë¬¸]\n{text}\n\n[ì‰¬ìš´ ì¬í•´ì„]\n"
    )

def _clean_latex_text(text: str) -> str:
    """LaTeX ëª…ë ¹ì–´ë¥¼ ì •ë¦¬í•˜ê³  ì½ê¸° ì‰½ê²Œ ë§Œë“­ë‹ˆë‹¤"""
    import re
    
    # LaTeX ëª…ë ¹ì–´ë¥¼ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    text = re.sub(r'\\title\{([^}]*)\}', r'ì œëª©: \1', text)  # \title{content} â†’ ì œëª©: content
    text = re.sub(r'\\author\{([^}]*)\}', r'ì €ì: \1', text)  # \author{content} â†’ ì €ì: content
    text = re.sub(r'\\section\{([^}]*)\}', r'ì„¹ì…˜: \1', text)  # \section{content} â†’ ì„¹ì…˜: content
    text = re.sub(r'\\subsection\{([^}]*)\}', r'í•˜ìœ„ì„¹ì…˜: \1', text)  # \subsection{content} â†’ í•˜ìœ„ì„¹ì…˜: content
    text = re.sub(r'\\textbf\{([^}]*)\}', r'**\1**', text)  # \textbf{content} â†’ **content**
    text = re.sub(r'\\textit\{([^}]*)\}', r'*\1*', text)  # \textit{content} â†’ *content*
    
    # ìˆ˜ì‹ í™˜ê²½ì„ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    text = re.sub(r'\$([^$]*)\$', r'ìˆ˜ì‹: \1', text)  # $ìˆ˜ì‹$ â†’ ìˆ˜ì‹: ìˆ˜ì‹
    text = re.sub(r'\$\$([^$]*)\$\$', r'ìˆ˜ì‹: \1', text)  # $$ìˆ˜ì‹$$ â†’ ìˆ˜ì‹: ìˆ˜ì‹
    
    # LaTeX í™˜ê²½ì„ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    text = re.sub(r'\\begin\{itemize\}', 'ëª©ë¡:', text)
    text = re.sub(r'\\item\s*', 'â€¢ ', text)
    text = re.sub(r'\\end\{itemize\}', '', text)
    
    text = re.sub(r'\\begin\{enumerate\}', 'ë²ˆí˜¸ëª©ë¡:', text)
    text = re.sub(r'\\end\{enumerate\}', '', text)
    
    # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬ (LRB, RRB ë“±)
    text = re.sub(r'LRB', '(', text)  # LRB â†’ (
    text = re.sub(r'RRB', ')', text)  # RRB â†’ )
    text = re.sub(r'\\ref\{([^}]*)\}', r'ê·¸ë¦¼ \1', text)  # \ref{system} â†’ ê·¸ë¦¼ system
    text = re.sub(r'\\cite\{([^}]*)\}', '', text)  # \cite{paper} â†’ ì œê±° (ì°¸ê³ ë¬¸í—Œ)
    
    # ë‚˜ë¨¸ì§€ LaTeX ëª…ë ¹ì–´ ì œê±°
    text = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', '', text)  # \command{content}
    text = re.sub(r'\\[a-zA-Z]+', '', text)  # \command
    text = re.sub(r'\\[^a-zA-Z]', '', text)  # \íŠ¹ìˆ˜ë¬¸ì
    
    # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
    text = re.sub(r'[{}]', '', text)  # ì¤‘ê´„í˜¸ ì œê±°
    text = re.sub(r'\\[a-zA-Z]', '', text)  # ë‚¨ì€ ë°±ìŠ¬ë˜ì‹œ ëª…ë ¹ì–´
    
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def _parse_latex_sections(tex_path: Path) -> List[dict]:
    """LaTeX íŒŒì¼ì„ ì„¹ì…˜ë³„ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤"""
    import re
    
    with open(tex_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    sections = []
    current_section = None
    current_content = []
    
    lines = content.split('\n')
    
    for line in lines:
        # ì„¹ì…˜ ì‹œì‘ ê°ì§€
        if re.match(r'\\section\{([^}]*)\}', line):
            # ì´ì „ ì„¹ì…˜ ì €ì¥
            if current_section and current_content:
                sections.append({
                    "index": len(sections),
                    "title": current_section,
                    "content": '\n'.join(current_content).strip()
                })
            
            # ìƒˆ ì„¹ì…˜ ì‹œì‘
            title_match = re.match(r'\\section\{([^}]*)\}', line)
            current_section = title_match.group(1) if title_match else "Unknown Section"
            current_content = [line]
            
        elif re.match(r'\\subsection\{([^}]*)\}', line):
            # ì„œë¸Œì„¹ì…˜ë„ ì„¹ì…˜ìœ¼ë¡œ ì²˜ë¦¬
            if current_section and current_content:
                sections.append({
                    "index": len(sections),
                    "title": current_section,
                    "content": '\n'.join(current_content).strip()
                })
            
            title_match = re.match(r'\\subsection\{([^}]*)\}', line)
            current_section = title_match.group(1) if title_match else "Unknown Subsection"
            current_content = [line]
            
        elif re.match(r'\\begin\{abstract\}', line):
            # Abstract ì„¹ì…˜
            if current_section and current_content:
                sections.append({
                    "index": len(sections),
                    "title": current_section,
                    "content": '\n'.join(current_content).strip()
                })
            
            current_section = "Abstract"
            current_content = [line]
            
        elif re.match(r'\\begin\{document\}', line):
            # Document ì‹œì‘
            if current_section and current_content:
                sections.append({
                    "index": len(sections),
                    "title": current_section,
                    "content": '\n'.join(current_content).strip()
                })
            
            current_section = "Introduction"
            current_content = [line]
            
        else:
            # ì¼ë°˜ ë‚´ìš©
            if current_section:
                current_content.append(line)
    
    # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
    if current_section and current_content:
        sections.append({
            "index": len(sections),
            "title": current_section,
            "content": '\n'.join(current_content).strip()
        })
    
    # ë¹ˆ ì„¹ì…˜ ì œê±°
    sections = [s for s in sections if s["content"].strip()]
    
    return sections

async def _rewrite_text(text: str) -> str:
    if model is None or tokenizer is None:
        raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    # LaTeX í…ìŠ¤íŠ¸ ì •ë¦¬ (ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜)
    cleaned_text = _clean_latex_text(text)

    prompt = _build_easy_prompt(cleaned_text)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=float(os.getenv("EASY_TEMPERATURE", "0.7")),
            top_p=float(os.getenv("EASY_TOP_P", "0.9")),
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.2,  # ë°˜ë³µ ë°©ì§€ ê°•í™”
            no_repeat_ngram_size=3,  # 3-gram ë°˜ë³µ ë°©ì§€
        )
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated[len(prompt):].strip()

# -------------------- Viz í˜¸ì¶œ --------------------
async def _send_to_viz(paper_id: str, index: int, text_ko: str, out_dir: Path) -> VizResult:
    try:
        print(f"ğŸ” [DEBUG] Viz ëª¨ë¸ í˜¸ì¶œ: {VIZ_MODEL_URL}/viz")
        print(f"ğŸ” [DEBUG] ì „ì†¡ ë°ì´í„°: paper_id={paper_id}, index={index}, text_length={len(text_ko)}")
        print(f"ğŸ” [DEBUG] ì „ì†¡ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {text_ko[:200]}...")
        
        # Viz ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ ë¨¼ì € í™•ì¸
        try:
            async with httpx.AsyncClient(timeout=10) as client:
                health_response = await client.get(f"{VIZ_MODEL_URL.rstrip('/')}/health")
                if health_response.status_code != 200:
                    print(f"âŒ [ERROR] Viz ëª¨ë¸ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {health_response.status_code}")
                    return VizResult(ok=False, index=index, error="Viz ëª¨ë¸ì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ")
                print(f"âœ… [SUCCESS] Viz ëª¨ë¸ í—¬ìŠ¤ì²´í¬ ì„±ê³µ")
        except Exception as e:
            print(f"âŒ [ERROR] Viz ëª¨ë¸ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return VizResult(ok=False, index=index, error=f"Viz ëª¨ë¸ ì—°ê²° ë¶ˆê°€: {e}")
        
        # ì‹¤ì œ Viz ìš”ì²­
        async with httpx.AsyncClient(timeout=60) as client:  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
            r = await client.post(
                f"{VIZ_MODEL_URL.rstrip('/')}/viz",
                json={
                    "paper_id": paper_id,
                    "index": index,
                    "rewritten_text": text_ko,
                    "target_lang": "ko",
                    "bilingual": "missing",
                    "text_type": "easy_korean",  # ì‰½ê²Œ ë³€í™˜ëœ í•œêµ­ì–´ì„ì„ ëª…ì‹œ
                },
            )
            print(f"ğŸ” [DEBUG] Viz ëª¨ë¸ ì‘ë‹µ: {r.status_code}")
            
            if r.status_code != 200:
                print(f"âŒ [ERROR] Viz ëª¨ë¸ ì‘ë‹µ ì‹¤íŒ¨: {r.status_code} - {r.text}")
                return VizResult(ok=False, index=index, error=f"Viz ëª¨ë¸ ì‘ë‹µ ì‹¤íŒ¨: {r.status_code}")
            
            try:
                data = r.json()
                print(f"ğŸ” [DEBUG] Viz ëª¨ë¸ ì‘ë‹µ ë°ì´í„°: {data}")
            except Exception as json_error:
                print(f"âŒ [ERROR] Viz ëª¨ë¸ ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {json_error}")
                return VizResult(ok=False, index=index, error=f"Viz ëª¨ë¸ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {json_error}")

        img_path = data.get("image_path")

        if not img_path and data.get("image_base64"):
            out_path = out_dir / f"{index:06d}.png"
            out_path.write_bytes(base64.b64decode(data["image_base64"]))
            print(f"âœ… [SUCCESS] ì´ë¯¸ì§€ ì €ì¥: {out_path}")
            return VizResult(ok=True, index=index, image_path=str(out_path))

        if not img_path and data.get("image_url"):
            out_path = out_dir / f"{index:06d}.png"
            async with httpx.AsyncClient(timeout=60) as client:
                rr = await client.get(data["image_url"])
                rr.raise_for_status()
                out_path.write_bytes(rr.content)
            print(f"âœ… [SUCCESS] ì´ë¯¸ì§€ ì €ì¥: {out_path}")
            return VizResult(ok=True, index=index, image_path=str(out_path))

        if img_path:
            print(f"âœ… [SUCCESS] ì´ë¯¸ì§€ ê²½ë¡œ: {img_path}")
            return VizResult(ok=True, index=index, image_path=str(img_path))

        print(f"âŒ [ERROR] ì´ë¯¸ì§€ ê²½ë¡œ ì—†ìŒ: {data}")
        return VizResult(ok=False, index=index, error="No image_path from viz")
    except httpx.ConnectError as e:
        print(f"âŒ [ERROR] Viz ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
        return VizResult(ok=False, index=index, error=f"Viz ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
    except httpx.TimeoutException as e:
        print(f"âŒ [ERROR] Viz ëª¨ë¸ íƒ€ì„ì•„ì›ƒ: {e}")
        return VizResult(ok=False, index=index, error=f"Viz ëª¨ë¸ íƒ€ì„ì•„ì›ƒ: {e}")
    except Exception as e:
        print(f"âŒ [ERROR] Viz ëª¨ë¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return VizResult(ok=False, index=index, error=str(e))

# -------------------- ì—”ë“œí¬ì¸íŠ¸ --------------------
@app.get("/")
async def root():
    return {"message": "POLO Easy Model API", "model": BASE_MODEL}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if (model is not None and tokenizer is not None) else "unavailable",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None,
        "gpu_available": gpu_available,
        "gpu_device": torch.cuda.get_device_name(0) if gpu_available else None,
        "model_name": BASE_MODEL,
        "max_new_tokens": MAX_NEW_TOKENS,
        "cache_dir": str(CACHE_DIR),
    }

@app.get("/healthz")
async def healthz():
    return await health()

@app.post("/easy", response_model=TextResponse)
async def simplify_text(request: TextRequest):
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    simplified_text = await _rewrite_text(request.text)
    return TextResponse(simplified_text=simplified_text, translated_text=None)

@app.post("/generate")
async def generate_json(request: TextRequest):
    start_time = time.time()
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    extracted = _extract_sections(request.text)
    data_schema = json.loads(json.dumps(GROUND_SCHEMA))  # deepcopy
    for k in ["abstract","introduction","methods","results","discussion","conclusion"]:
        data_schema[k]["original"] = extracted.get(k, "")

    instruction = (
        "ë„ˆëŠ” ì¹œê·¼í•œ ê³¼í•™ ì„ ìƒë‹˜ì´ë‹¤. ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆì˜ 'í‚¤ì™€ êµ¬ì¡°'ë¥¼ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ê³  "
        "'ê°’'ë§Œ ì±„ì›Œë¼. ì¶œë ¥ì€ ì˜¤ì§ 'ìœ íš¨í•œ JSON' í•˜ë‚˜ë§Œ í—ˆìš©ëœë‹¤(ì½”ë“œë¸”ë¡/ì„¤ëª…/ì£¼ì„ ê¸ˆì§€).\n\n"
        "ğŸ¯ ê° ì„¹ì…˜ì˜ 'easy' ì‘ì„± ë°©ë²•:\n"
        "- ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‰½ê³  ì¬ë¯¸ìˆê²Œ ë³€í™˜\n"
        "- ì „ë¬¸ ìš©ì–´ëŠ” ì¼ìƒ ìš©ì–´ë¡œ ë°”ê¾¸ê¸°\n"
        "- ë³µì¡í•œ ë‚´ìš©ì€ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…\n"
        "- êµ¬ì²´ì ì¸ ë¹„ìœ ì™€ ì˜ˆì‹œ ì‚¬ìš©\n"
        "- 'ìš”ì•½í•˜ë©´', 'ì‰½ê²Œ ë§í•˜ë©´' ê°™ì€ ì •ë¦¬ ë¬¸êµ¬ í™œìš©\n"
        "- ì¹œê·¼í•œ í†¤ìœ¼ë¡œ 4-8ë¬¸ì¥ ì‘ì„± (ì¡´ëŒ“ë§ '~í•©ë‹ˆë‹¤', '~ì…ë‹ˆë‹¤' ì‚¬ìš©, '~ìš”'ë¡œ ëë‚˜ì§€ ì•Šê²Œ)\n\n"
        "originalì´ ë¹„ì–´ ìˆìœ¼ë©´ í•´ë‹¹ 'easy'ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ë‚¨ê²¨ë¼. ì™¸ë¶€ ì§€ì‹/ì¶”ì¸¡ ê¸ˆì§€."
    )

    schema_str = json.dumps(data_schema, ensure_ascii=False, indent=2)
    context_only = {k: extracted[k] for k in ["abstract","introduction","methods","results","discussion","conclusion"]}
    context_str = json.dumps(context_only, ensure_ascii=False, indent=2)

    prompt = (
        f"{instruction}\n\n=== ì¶œë ¥ ìŠ¤í‚¤ë§ˆ(ê°’ë§Œ ì±„ì›Œë¼) ===\n{schema_str}\n\n"
        f"=== ì„¹ì…˜ë³„ original (ì´ í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©) ===\n{context_str}\n\n"
        "JSONë§Œ ì¶œë ¥:"
    )

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    t0 = time.time()
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            use_cache=True,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    inference_time = time.time() - t0

    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    raw = generated[len(prompt):].strip()

    try:
        data = _coerce_json(raw)
        if not _is_meaningful(data):
            raise ValueError("empty_json")
    except Exception:
        strict_instruction = (
            "ìŠ¤í‚¤ë§ˆì˜ í‚¤/êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³  ê°’ë§Œ ì±„ìš´ 'ìœ íš¨í•œ JSON'ë§Œ ì¶œë ¥í•˜ë¼. "
            "ë°˜ë“œì‹œ '{'ë¡œ ì‹œì‘í•´ '}'ë¡œ ëë‚˜ì•¼ í•œë‹¤. ì™¸ë¶€ì§€ì‹/ì¶”ì¸¡ ê¸ˆì§€."
        )
        strict_prompt = f"{strict_instruction}\n\nìŠ¤í‚¤ë§ˆ:\n{schema_str}\n\nì„¹ì…˜ original:\n{context_str}\n\nJSONë§Œ ì¶œë ¥:"
        inputs2 = tokenizer(strict_prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs2 = {k: v.to(device) for k, v in inputs2.items()}
        with torch.inference_mode():
            outputs2 = model.generate(
                **inputs2,
                max_new_tokens=min(MAX_NEW_TOKENS, 800),
                do_sample=False,
                use_cache=True,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        gen2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)
        raw2 = gen2[len(strict_prompt):].strip()
        try:
            data = _coerce_json(raw2)
        except Exception:
            data = data_schema

    total_time = time.time() - start_time
    data["processing_info"] = {
        "gpu_used": gpu_available,
        "inference_time": inference_time,
        "total_time": total_time,
        "input_length": len(request.text),
        "output_length": len(str(data)),
    }
    return data

@app.post("/batch", response_model=BatchResult)
async def batch_generate(req: BatchRequest):
    print(f"ğŸ” [DEBUG] Easy /batch ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨")
    print(f"ğŸ” [DEBUG] ìš”ì²­ ë°ì´í„°:")
    print(f"  - paper_id: {req.paper_id}")
    print(f"  - chunks_jsonl: {req.chunks_jsonl}")
    print(f"  - output_dir: {req.output_dir}")
    
    # merged_body.tex íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½
    tex_path = Path(req.chunks_jsonl).parent / "merged_body.tex"
    out_dir = Path(req.output_dir).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ” [DEBUG] íŒŒì¼ ê²½ë¡œ í™•ì¸:")
    print(f"  - tex_path: {tex_path}")
    print(f"  - tex_path ì¡´ì¬: {tex_path.exists()}")
    print(f"  - out_dir: {out_dir}")
    print(f"  - out_dir ìƒì„±ë¨: {out_dir.exists()}")

    if not tex_path.exists():
        print(f"âŒ [ERROR] merged_body.tex íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {tex_path}")
        raise HTTPException(status_code=400, detail=f"merged_body.tex not found: {tex_path}")

    # LaTeX íŒŒì¼ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• 
    sections = _parse_latex_sections(tex_path)
    print(f"ğŸ” [DEBUG] ì´ {len(sections)}ê°œ ì„¹ì…˜ íŒŒì‹±ë¨")
    
    if not sections:
        print(f"âŒ [ERROR] ìœ íš¨í•œ ì„¹ì…˜ì´ ì—†ìŒ")
        raise HTTPException(status_code=400, detail="No valid sections found in merged_body.tex")

    # ëª¨ë¸ ìƒíƒœ í™•ì¸
    if model is None or tokenizer is None:
        print(f"âŒ [ERROR] ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        raise HTTPException(status_code=500, detail="Model not loaded")

    print(f"ğŸ” [DEBUG] ëª¨ë¸ ìƒíƒœ: model={model is not None}, tokenizer={tokenizer is not None}")
    print(f"ğŸ” [DEBUG] ë””ë°”ì´ìŠ¤: {device}, GPU ì‚¬ìš©: {gpu_available}")

    sem = anyio.Semaphore(EASY_CONCURRENCY)
    results: List[VizResult] = []

    async def worker(section: dict):
        async with sem:
            idx = section["index"]
            try:
                print(f"ğŸ” [DEBUG] ì„¹ì…˜ {idx}/{len(sections)} ì²˜ë¦¬ ì‹œì‘: {section['title']}")
                ko = await _rewrite_text(section["content"])
                print(f"ğŸ” [DEBUG] ì„¹ì…˜ {idx}/{len(sections)} ë³€í™˜ ì™„ë£Œ: {ko[:100]}...")
                
                # Google Translatorë¡œ í•œêµ­ì–´ ë²ˆì—­
                print(f"ğŸ” [DEBUG] ì„¹ì…˜ {idx}/{len(sections)} í•œêµ­ì–´ ë²ˆì—­ ì‹œì‘...")
                ko_translated = _translate_to_korean(ko)
                print(f"ğŸ” [DEBUG] ì„¹ì…˜ {idx}/{len(sections)} í•œêµ­ì–´ ë²ˆì—­ ì™„ë£Œ: {ko_translated[:100]}...")
                
                # í•œêµ­ì–´ ë²ˆì—­ë³¸ìœ¼ë¡œ Viz ì²˜ë¦¬
                vz = await _send_to_viz(req.paper_id, idx, ko_translated, out_dir)
                print(f"ğŸ” [DEBUG] ì„¹ì…˜ {idx}/{len(sections)} Viz ì™„ë£Œ: {vz.ok}")
                
                # ê²°ê³¼ì— ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ì €ì¥
                vz.easy_text = ko_translated
                vz.section_title = section["title"]
                results.append(vz)
                
                # ì§„í–‰ë¥  í‘œì‹œ
                completed = len(results)
                progress = (completed / len(sections)) * 100
                print(f"ğŸ“Š [PROGRESS] {completed}/{len(sections)} ({progress:.1f}%) ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ [ERROR] ì„¹ì…˜ {idx}/{len(sections)} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                results.append(VizResult(ok=False, index=idx, error=str(e)))

    print(f"ğŸ” [DEBUG] ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘...")
    async with anyio.create_task_group() as tg:
        for section in sections:
            tg.start_soon(worker, section)

    ok_cnt = sum(1 for r in results if r.ok)
    fail_cnt = len(results) - ok_cnt
    
    print(f"ğŸ” [DEBUG] ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ:")
    print(f"  - ì´ ì„¹ì…˜: {len(sections)}")
    print(f"  - ì„±ê³µ: {ok_cnt}")
    print(f"  - ì‹¤íŒ¨: {fail_cnt}")
    
    result = BatchResult(
        ok=fail_cnt == 0,
        paper_id=req.paper_id,
        count=len(sections),
        success=ok_cnt,
        failed=fail_cnt,
        out_dir=str(out_dir),
        images=sorted(results, key=lambda r: r.index),
    )
    
    # JSON ê²°ê³¼ íŒŒì¼ ìƒì„± (í”„ë¡ íŠ¸ì—”ë“œìš©)
    json_result = {
        "paper_id": req.paper_id,
        "total_sections": len(sections),
        "success_count": ok_cnt,
        "failed_count": fail_cnt,
        "sections": []
    }
    
    # ê° ì„¹ì…˜ë³„ ê²°ê³¼ ì¶”ê°€
    for i, section in enumerate(sections):
        section_result = {
            "index": i,
            "title": section["title"],
            "original_content": section["content"][:200] + "..." if len(section["content"]) > 200 else section["content"],
            "easy_text": "",
            "korean_translation": "",
            "image_path": "",
            "status": "failed"
        }
        
        # í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ê²°ê³¼ ì°¾ê¸°
        for r in results:
            if r.index == i:
                section_result["status"] = "success" if r.ok else "failed"
                if r.ok and r.image_path:
                    section_result["image_path"] = r.image_path
                if hasattr(r, 'easy_text') and r.easy_text:
                    section_result["korean_translation"] = r.easy_text
                break
        
        json_result["sections"].append(section_result)
    
    # JSON íŒŒì¼ ì €ì¥
    json_file_path = out_dir / "easy_results.json"
    with open(json_file_path, "w", encoding="utf-8") as f:
        json.dump(json_result, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ [JSON] ê²°ê³¼ íŒŒì¼ ì €ì¥: {json_file_path}")
    print(f"âœ… [SUCCESS] Easy ëª¨ë¸ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {result}")
    return result

@app.post("/from-transport", response_model=BatchResult)
async def generate_from_transport(req: TransportRequest):
    tp = Path(req.transport_path)
    if not tp.exists():
        raise HTTPException(status_code=400, detail=f"transport.json not found: {tp}")

    try:
        data = json.loads(tp.read_text(encoding="utf-8"))
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"invalid transport.json: {e}")

    # chunks ê²½ë¡œ ìš°ì„ : artifacts.chunks.path â†’ tp.parent/chunks.jsonl â†’ tp.parent/chunks.jsonl.gz
    chunks_path: Optional[Path] = None
    try:
        chunks_path_str = (((data.get("artifacts", {}) or {}).get("chunks", {}) or {}).get("path"))
        if chunks_path_str:
            chunks_path = Path(chunks_path_str)
    except Exception:
        chunks_path = None
    if chunks_path is None:
        base_dir = tp.parent
        cand1 = base_dir / "chunks.jsonl"
        cand2 = base_dir / "chunks.jsonl.gz"
        if cand1.exists():
            chunks_path = cand1
        elif cand2.exists():
            chunks_path = cand2

    if chunks_path is None or not chunks_path.exists():
        raise HTTPException(status_code=400, detail=f"chunks file not found near transport: {tp}")

    # ì¶œë ¥ ê²½ë¡œ
    out_dir = Path(req.output_dir).resolve() if req.output_dir else (tp.parent / "easy_outputs").resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    # ê¸°ì¡´ ë°°ì¹˜ ë¡œì§ ì¬ì‚¬ìš©
    items: List[dict] = []
    open_fn = gzip.open if str(chunks_path).endswith(".gz") else open
    with open_fn(chunks_path, "rt", encoding="utf-8") as f:
        for i, line in enumerate(f):
            try:
                obj = json.loads(line)
                text = obj.get("text")
                if not isinstance(text, str) or not text.strip():
                    continue
                items.append({"index": i, "text": text})
            except Exception:
                continue

    sem = anyio.Semaphore(EASY_CONCURRENCY)
    results: List[VizResult] = []

    async def worker(item: dict):
        async with sem:
            idx = item["index"]
            try:
                ko = await _rewrite_text(item["text"])
                vz = await _send_to_viz(req.paper_id, idx, ko, out_dir)
                results.append(vz)
            except Exception as e:
                results.append(VizResult(ok=False, index=idx, error=str(e)))

    async with anyio.create_task_group() as tg:
        for item in items:
            tg.start_soon(worker, item)

    ok_cnt = sum(1 for r in results if r.ok)
    fail_cnt = len(results) - ok_cnt
    return BatchResult(
        ok=fail_cnt == 0,
        paper_id=req.paper_id,
        count=len(items),
        success=ok_cnt,
        failed=fail_cnt,
        out_dir=str(out_dir),
        images=sorted(results, key=lambda r: r.index),
    )

# -------------------- main --------------------
if __name__ == "__main__":
    try:
        if torch.cuda.is_available():
            print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
            print("ğŸ”§ ë””ë°”ì´ìŠ¤: cuda, dtype: float16")
        else:
            print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            print("ğŸ”§ ë””ë°”ì´ìŠ¤: cpu, dtype: float32")

        print("ğŸš€ Easy Model ì„œë²„ ì‹œì‘ ì¤‘...")
        uvicorn.run(app, host="0.0.0.0", port=5003)
    except Exception as e:
        print(f"âŒ Easy Model ì‹œì‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
