"""
POLO Easy Model - ë…¼ë¬¸ì„ ì‰½ê²Œ í’€ì–´ ì„¤ëª…í•˜ëŠ” LLM ì„œë¹„ìŠ¤
"""

import os
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from googletrans import Translator

# --- í™˜ê²½ ë³€ìˆ˜ ---
BASE_MODEL = os.getenv("EASY_BASE_MODEL", "meta-llama/Llama-3.2-3B-Instruct")
ADAPTER_DIR = os.getenv("EASY_ADAPTER_DIR", "")

app = FastAPI(title="POLO Easy Model", version="1.0.0")

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
translator = Translator()

class TextRequest(BaseModel):
    text: str
    translate: Optional[bool] = False

class TextResponse(BaseModel):
    simplified_text: str
    translated_text: Optional[str] = None

def load_model():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    global model, tokenizer
    
    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {BASE_MODEL}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ëª¨ë¸ ë¡œë“œ
    safe_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=safe_dtype,
        device_map="auto",
        low_cpu_mem_usage=True,
        trust_remote_code=True,  # Llama ëª¨ë¸ìš©
    )
    
    # ì–´ëŒ‘í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œ
    if ADAPTER_DIR and os.path.exists(ADAPTER_DIR):
        print(f"ğŸ”„ ì–´ëŒ‘í„° ë¡œë”© ì¤‘: {ADAPTER_DIR}")
        model = PeftModel.from_pretrained(model, ADAPTER_DIR)
    
    model.eval()
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    load_model()

@app.get("/")
async def root():
    return {"message": "POLO Easy Model API", "model": BASE_MODEL}

@app.post("/simplify", response_model=TextResponse)
async def simplify_text(request: TextRequest):
    """í…ìŠ¤íŠ¸ë¥¼ ì‰½ê²Œ í’€ì–´ ì„¤ëª…"""
    try:
        if model is None or tokenizer is None:
            raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ ë…¼ë¬¸ ë‚´ìš©ì„ ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”:

{request.text}

ì‰¬ìš´ ì„¤ëª…:"""
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        
        # GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        simplified_text = generated_text[len(prompt):].strip()
        
        # ë²ˆì—­ (ìš”ì²­ëœ ê²½ìš°)
        translated_text = None
        if request.translate:
            try:
                translation = translator.translate(simplified_text, dest='ko')
                translated_text = translation.text
            except Exception as e:
                print(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
        
        return TextResponse(
            simplified_text=simplified_text,
            translated_text=translated_text
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=5003)
