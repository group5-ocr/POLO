# -*- coding: utf-8 -*-
"""
POLO Easy Model - Grounded JSON Generator

- CUDA ìš°ì„ (ì—†ìœ¼ë©´ CPU í´ë°±)
- ì–´í…ì…˜ ë°±ì—”ë“œ: flash_attn > sdpa > eager
- 'ì‰¬ìš´ í•œêµ­ì–´ ì¬í•´ì„'ì— ìµœì í™”
- /easy, /generate, /batch ì œê³µ
"""
from __future__ import annotations

import os
import re
import json
import time
import base64
import gzip
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
from googletrans import Translator

import anyio
import httpx
import torch
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, ConfigDict, Field
from dotenv import load_dotenv

# -------------------- .env ë¡œë“œ (ë£¨íŠ¸ë§Œ) --------------------
logger = logging.getLogger("polo.easy")
logging.basicConfig(level=logging.INFO)

ROOT_ENV = Path(__file__).resolve().parents[2] / ".env"
if ROOT_ENV.exists():
    load_dotenv(dotenv_path=str(ROOT_ENV), override=True)
    logger.info(f"[dotenv] loaded: {ROOT_ENV}")
else:
    logger.info("[dotenv] no .env at repo root")

HF_TOKEN   = os.getenv("í—ˆê¹…í˜ì´ìŠ¤ í† í°")
BASE_MODEL = os.getenv("EASY_BASE_MODEL", "meta-llama/Llama-3.2-3B-Instruct")
ADAPTER_DIR = os.getenv(
    "EASY_ADAPTER_DIR",
    str(Path(__file__).resolve().parent.parent / "fine-tuning" / "outputs" / "llama32-3b-qlora" / "checkpoint-4000")
)
MAX_NEW_TOKENS      = int(os.getenv("EASY_MAX_NEW_TOKENS", "1200"))
VIZ_MODEL_URL       = os.getenv("VIZ_MODEL_URL", "http://localhost:5005")
EASY_CONCURRENCY    = int(os.getenv("EASY_CONCURRENCY", "8"))
EASY_BATCH_TIMEOUT  = int(os.getenv("EASY_BATCH_TIMEOUT", "600"))

# -------------------- HF ìºì‹œ ê²½ë¡œ 'ë¬´ì¡°ê±´' ì•ˆì „ í´ë”ë¡œ ê³ ì • --------------------
SAFE_CACHE_DIR = Path(__file__).resolve().parent / "hf_cache"
SAFE_CACHE_DIR.mkdir(parents=True, exist_ok=True)

def force_safe_hf_cache():
    # ì‹œìŠ¤í…œ ì „ì—­/ì‚¬ìš©ì í™˜ê²½ë³€ìˆ˜ì— ì´ìƒí•œ ê²½ë¡œ(D:\...)ê°€ ìˆì–´ë„ ì—¬ê¸°ë¡œ í†µì¼
    for k in ("HF_HOME", "TRANSFORMERS_CACHE", "HF_DATASETS_CACHE", "HF_HUB_CACHE"):
        os.environ[k] = str(SAFE_CACHE_DIR)
    logger.info(f"[hf_cache] forced cache dir â†’ {SAFE_CACHE_DIR}")

force_safe_hf_cache()
CACHE_DIR = os.environ["HF_HOME"]  # ë™ì¼ ê²½ë¡œ ì‚¬ìš©

# â›” transformers/peftëŠ” ìºì‹œ ê²½ë¡œê°€ import ì‹œì ì— êµ³ì–´ì§ˆ ìˆ˜ ìˆìŒ
#    ë°˜ë“œì‹œ ìºì‹œ ì„¸íŒ… ì´í›„ì— import
from transformers import AutoModelForCausalLM, AutoTokenizer  # noqa: E402
from peft import PeftModel  # noqa: E402

# -------------------- FastAPI --------------------
app = FastAPI(title="POLO Easy Model", version="1.3.2")

# -------------------- ì „ì—­ ìƒíƒœ --------------------
model = None
tokenizer = None
device = "cuda" if torch.cuda.is_available() else "cpu"
gpu_available = torch.cuda.is_available()
safe_dtype = torch.float16 if gpu_available else torch.float32
translator = Translator()

# -------------------- ìœ í‹¸ --------------------
def _pick_attn_impl() -> str:
    try:
        import flash_attn  # noqa: F401
        logger.info("âœ… flash_attn ì‚¬ìš©: flash_attention_2")
        return "flash_attention_2"
    except Exception:
        pass
    try:
        from torch.nn.functional import scaled_dot_product_attention  # noqa: F401
        logger.info("â„¹ï¸ sdpa ì‚¬ìš© ê°€ëŠ¥")
        return "sdpa"
    except Exception:
        logger.info("â„¹ï¸ sdpa ë¶ˆê°€ â†’ eagerë¡œ ì§„í–‰")
        return "eager"

def _coerce_json(text: str) -> Dict[str, Any]:
    s = text.find("{"); e = text.rfind("}")
    if s != -1 and e != -1 and e > s:
        text = text[s:e+1]
    return json.loads(text)

def _is_meaningful(d: dict) -> bool:
    try:
        sections = ["abstract","introduction","methods","results","discussion","conclusion"]
        return any(len((d.get(s, {}) or {}).get("easy", "")) > 10 for s in sections)
    except Exception:
        return False

def _translate_to_korean(text: str) -> str:
    """Google Translatorë¥¼ ì‚¬ìš©í•´ì„œ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
    try:
        if not text or not text.strip():
            return ""
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ ë²ˆì—­
        if len(text) > 4000:  # Google Translator ì œí•œ
            text = text[:4000] + "..."
        
        result = translator.translate(text, dest='ko', src='en')
        return result.text
    except Exception as e:
        logger.warning(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return text  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

def _extract_sections(src: str) -> dict:
    sections = {k: "" for k in ["abstract","introduction","methods","results","discussion","conclusion"]}
    headers = [
        ("abstract", r"^\s*abstract\b[:\-]?"),
        ("introduction", r"^\s*introduction\b[:\-]?"),
        ("methods", r"^\s*methods?\b[:\-]?|^\s*materials?\s+and\s+methods\b[:\-]?"),
        ("results", r"^\s*results?\b[:\-]?"),
        ("discussion", r"^\s*discussion\b[:\-]?"),
        ("conclusion", r"^\s*conclusion[s]?\b[:\-]?|^\s*concluding\s+remarks\b[:\-]?"),
    ]
    lines = src.splitlines()
    idxs = []
    for i, line in enumerate(lines):
        for key, pat in headers:
            if re.match(pat, line.strip(), flags=re.IGNORECASE):
                idxs.append((i, key))
                break
    idxs.sort()
    for j, (start_i, key) in enumerate(idxs):
        end_i = idxs[j+1][0] if j+1 < len(idxs) else len(lines)
        sections[key] = "\n".join(lines[start_i+1:end_i]).strip()[:2000]
    return sections

# -------------------- ìŠ¤í‚¤ë§ˆ --------------------
GROUND_SCHEMA = {
    "title": "",
    "authors": [],
    "abstract": {"original": "", "easy": ""},
    "introduction": {"original": "", "easy": ""},
    "methods": {"original": "", "easy": ""},
    "results": {"original": "", "easy": ""},
    "discussion": {"original": "", "easy": ""},
    "conclusion": {"original": "", "easy": ""},
    "keywords": [],
    "figures_tables": [],
    "references": [],
    "contributions": [],
    "limitations": [],
    "glossary": [],
    "plain_summary": "",
}

# -------------------- I/O ëª¨ë¸ --------------------
class TextRequest(BaseModel):
    text: str
    translate: Optional[bool] = False
    model_config = ConfigDict(extra="allow")

class TextResponse(BaseModel):
    simplified_text: str
    translated_text: Optional[str] = None

class BatchRequest(BaseModel):
    paper_id: str = Field(..., description="ê²°ê³¼ íŒŒì¼/ê²½ë¡œ ì‹ë³„ì")
    chunks_jsonl: str = Field(..., description="ê° ë¼ì¸ì— {'text': ...} í˜•íƒœì˜ JSONL")
    output_dir: str = Field(..., description="ì´ë¯¸ì§€/ê²°ê³¼ ì €ì¥ ë£¨íŠ¸")

class VizResult(BaseModel):
    ok: bool = True
    index: int
    image_path: Optional[str] = None
    error: Optional[str] = None
    easy_text: Optional[str] = None
    section_title: Optional[str] = None

class BatchResult(BaseModel):
    ok: bool
    paper_id: str
    count: int
    success: int
    failed: int
    out_dir: str
    images: List[VizResult]

class TransportRequest(BaseModel):
    paper_id: str
    transport_path: str
    output_dir: Optional[str] = None

# -------------------- ëª¨ë¸ ë¡œë“œ --------------------
def load_model():
    global model, tokenizer, gpu_available, device, safe_dtype

    logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {BASE_MODEL}")
    logger.info(f"EASY_ADAPTER_DIR={ADAPTER_DIR}")
    logger.info(f"HF_HOME={os.getenv('HF_HOME')}")

    if torch.cuda.is_available():
        gpu_available = True
        device = "cuda"
        safe_dtype = torch.float16
        logger.info(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    else:
        gpu_available = False
        device = "cpu"
        safe_dtype = torch.float32
        logger.info("âš ï¸ GPU ë¯¸ì‚¬ìš© â†’ CPU ëª¨ë“œ")

    # í† í¬ë‚˜ì´ì € (ìºì‹œ ê³ ì •)
    tokenizer_local = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        token=HF_TOKEN,
        trust_remote_code=True,
        cache_dir=CACHE_DIR,
    )
    if tokenizer_local.pad_token_id is None:
        tokenizer_local.pad_token = tokenizer_local.eos_token

    attn_impl = _pick_attn_impl()
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # ë² ì´ìŠ¤ ëª¨ë¸
    try:
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            torch_dtype=safe_dtype,
            trust_remote_code=True,
            attn_implementation=attn_impl,
            low_cpu_mem_usage=True,
            token=HF_TOKEN,
            device_map=None,
            cache_dir=CACHE_DIR,
        )
    except Exception as e:
        logger.warning(f"attn='{attn_impl}' ë¡œë”© ì‹¤íŒ¨({e}) â†’ eagerë¡œ í´ë°±")
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            torch_dtype=safe_dtype,
            trust_remote_code=True,
            attn_implementation="eager",
            low_cpu_mem_usage=True,
            token=HF_TOKEN,
            device_map=None,
            cache_dir=CACHE_DIR,
        )

    # LoRA ì–´ëŒ‘í„°(ì„ íƒ)
    m = base
    if ADAPTER_DIR and os.path.exists(ADAPTER_DIR):
        try:
            m = PeftModel.from_pretrained(
                base,
                os.path.abspath(ADAPTER_DIR),
                is_trainable=False,
                local_files_only=True,
            )
            logger.info("âœ… ì–´ëŒ‘í„° ë¡œë”© ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° ë¡œë”© ì‹¤íŒ¨: {e} â†’ ë² ì´ìŠ¤ë¡œ ì§„í–‰")
            m = base
    else:
        logger.info("â„¹ï¸ ì–´ëŒ‘í„° ê²½ë¡œ ì—†ìŒ(ë² ì´ìŠ¤ë§Œ ì‚¬ìš©)")

    m.eval()
    m = m.to(safe_dtype).to(device)
    logger.info(f"ğŸ§  MODEL DEVICE: {next(m.parameters()).device}, DTYPE: {next(m.parameters()).dtype}")

    # ì „ì—­ ì£¼ì…
    globals()["model"] = m
    globals()["tokenizer"] = tokenizer_local
    logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# -------------------- ìŠ¤íƒ€íŠ¸ì—… --------------------
@app.on_event("startup")
async def startup_event():
    load_model()

# -------------------- ë‚´ë¶€ ìœ í‹¸ (ì¬í•´ì„) --------------------
def _build_easy_prompt(text: str, section_title: str | None = None) -> str:
    title_line = f"[ì„¹ì…˜] {section_title}\n\n" if section_title else ""
    return (
        title_line +
        "ì•„ë˜ í•™ìˆ  í…ìŠ¤íŠ¸ë¥¼ 'ì‰½ê²Œë§í•´,'ë¡œ ì‹œì‘í•˜ì—¬ ê³ ë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ í•œêµ­ì–´ë¡œ ì¬ì„œìˆ í•˜ë¼.\n"
        "- ìƒˆë¡œìš´ ì •ë³´ ì¶”ê°€ ê¸ˆì§€, ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ì •í™•íˆ ë³´ì¡´\n"
        "- ë¬¸ì¥ì„ ì§§ê³  ëª…í™•í•˜ê²Œ ë¶„í• , ë¬¸ë‹¨ì„ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ë¶„\n"
        "- ì¡´ëŒ“ë§ë¡œ ì„œìˆ (ì…ë‹ˆë‹¤/í•©ë‹ˆë‹¤/í•œë‹¤), ~ìš” ê¸ˆì§€\n"
        "- ìˆ˜ì‹/ê¸°í˜¸/ê·¸ë¦¼ ë‚´ìš©ì€ ì„¤ëª…ë§Œ í•˜ê³  í…ìŠ¤íŠ¸ì— ì¬ì‚½ì…í•˜ì§€ ë§ ê²ƒ(ìˆ˜ì‹ì€ ë³„ë„ë¡œ ë³µì›ë¨)\n"
        "- ëª©ë¡ì´ ìì—°ìŠ¤ëŸ¬ìš°ë©´ ê°„ë‹¨í•œ ë¶ˆë¦¿ì„ ì‚¬ìš©\n"
        "- ë¼í…ìŠ¤ ëª…ë ¹ì–´/í‘œ/ê·¸ë¦¼ ì½”ë“œëŠ” ìƒì„±í•˜ì§€ ë§ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œë§Œ ì‘ì„±\n\n"
        "ì¶œë ¥ í˜•ì‹:\n"
        "- 1~3ê°œì˜ ì§§ì€ ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ  ì‘ì„±\n"
        "- ì²« ë¬¸ì¥ì€ ë°˜ë“œì‹œ 'ì‰½ê²Œë§í•´,'ë¡œ ì‹œì‘\n\n"
        f"[ì›ë¬¸]\n{text}\n\n[ì¶œë ¥]\n"
    )

def _build_verify_prompt(first_pass_text: str, section_title: str | None = None) -> str:
    title_line = f"[ì„¹ì…˜] {section_title}\n\n" if section_title else ""
    return (
        title_line +
        "ë„ˆëŠ” ê³ ë“±í•™ìƒ ë…ìì´ì Llama LLMì˜ ê²€í† ìì´ë‹¤. ì•„ë˜ ì¬ì„œìˆ  ê²°ê³¼ë¥¼ ì½ê³  ê°€ë…ì„±ì„ ë†’ì—¬ë¼.\n"
        "- ì˜ë¯¸ ì™œê³¡, ì •ë³´ ì¶”ê°€ ê¸ˆì§€\n"
        "- ë¬¸ë‹¨ì´ ê¸¸ë©´ 2~3ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í• \n"
        "- ë°˜ë³µ/êµ°ë”ë”ê¸° ì œê±°, ë¬¸ì¥ ê°„ ì—°ê²°ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°ì •\n"
        "- ì¡´ëŒ“ë§(ì…ë‹ˆë‹¤/í•©ë‹ˆë‹¤/í•œë‹¤) ìœ ì§€, ~ìš” ê¸ˆì§€\n"
        "- ë¼í…ìŠ¤ ì½”ë“œëŠ” ìƒì„±í•˜ì§€ ë§ ê²ƒ, ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥\n"
        "- ì²« ë¬¸ì¥ì€ ê°€ëŠ¥í•˜ë©´ 'ì‰½ê²Œë§í•´,'ë¡œ ì‹œì‘\n\n"
        f"[ê²€í†  ëŒ€ìƒ]\n{first_pass_text}\n\n[ê°œì„ ëœ ì¶œë ¥]\n"
    )

def _extract_math_placeholders(text: str):
    """ìˆ˜ì‹ì„ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ì¹˜í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë°˜í™˜: (ì¹˜í™˜ëœ_í…ìŠ¤íŠ¸, inline_map, block_map)
    """
    import re

    # ë””ìŠ¤í”Œë ˆì´ ìˆ˜ì‹ (ìš°ì„  ì²˜ë¦¬)
    block_map = {}
    block_idx = 0

    def _sub_block_dollar(m):
        nonlocal block_idx
        key = f"[MATH_BLOCK_{block_idx}]"
        block_map[key] = m.group(0)
        block_idx += 1
        return key

    text = re.sub(r"\$\$[\s\S]*?\$\$", _sub_block_dollar)

    def _sub_equation_env(m):
        nonlocal block_idx
        key = f"[MATH_BLOCK_{block_idx}]"
        block_map[key] = m.group(0)
        block_idx += 1
        return key

    text = re.sub(r"\\begin\{(equation\*?|align\*?|eqnarray\*?)\}[\s\S]*?\\end\{\1\}", _sub_equation_env)

    # ì¸ë¼ì¸ ìˆ˜ì‹
    inline_map = {}
    inline_idx = 0

    def _sub_inline(m):
        nonlocal inline_idx
        key = f"[MATH_INLINE_{inline_idx}]"
        inline_map[key] = m.group(0)
        inline_idx += 1
        return key

    text = re.sub(r"\$(?!\$)(?:[^$\\]|\\.)+\$", _sub_inline)

    return text, inline_map, block_map


def _clean_latex_text(text: str) -> str:
    """LLM ì…ë ¥ìš©ìœ¼ë¡œ LaTeX ë…¸ì´ì¦ˆë¥¼ ìµœëŒ€í•œ ì œê±°í•©ë‹ˆë‹¤(êµ¬ì¡° íŒŒì‹±ì€ ë³„ë„ë¡œ ìˆ˜í–‰)."""
    import re

    # LRB, RRB ë³€í™˜ (ê´„í˜¸)
    text = re.sub(r"LRB", "(", text)
    text = re.sub(r"RRB", ")", text)

    # ì¸ìš©/ë¼ë²¨/ì°¸ì¡° ì œê±°
    text = re.sub(r"\\cite\{[^}]*\}", "", text)
    text = re.sub(r"\\label\{[^}]*\}", "", text)
    text = re.sub(r"\\ref\{[^}]*\}", "", text)
    text = re.sub(r"\\footnote\{[\s\S]*?\}", "", text)

    # URLì€ í…ìŠ¤íŠ¸ë¡œë§Œ ë‚¨ê¹€
    text = re.sub(r"\\url\{([^}]*)\}", r"(\1)", text)

    # ê·¸ë¦¼/í‘œ í™˜ê²½ ì œê±°
    text = re.sub(r"\\begin\{figure\}[\s\S]*?\\end\{figure\}", "", text)
    text = re.sub(r"\\begin\{table\}[\s\S]*?\\end\{table\}", "", text)
    text = re.sub(r"\\begin\{tabular\}[\s\S]*?\\end\{tabular\}", "", text)

    # ì„œì‹ ëª…ë ¹ ë‚´ìš©ë§Œ ë‚¨ê¹€
    text = re.sub(r"\\textbf\{([^}]*)\}", r"\1", text)
    text = re.sub(r"\\textit\{([^}]*)\}", r"\1", text)

    # ì„¹ì…˜/ì†Œì œëª© ëª…ë ¹ì€ íŒŒì‹± ë‹¨ê³„ì—ì„œ ê´€ë¦¬í•˜ë¯€ë¡œ ë³¸ë¬¸ì—ì„œëŠ” ì œê±°
    text = re.sub(r"^\\section\{[^}]*\}\s*", "", text, flags=re.MULTILINE)
    text = re.sub(r"^\\subsection\{[^}]*\}\s*", "", text, flags=re.MULTILINE)

    # ê³µë°± ì •ë¦¬
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"\s*\n\s*", "\n", text)
    return text.strip()

def _extract_technical_terms(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì „ë¬¸ ìš©ì–´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"""
    import re
    
    # ì¼ë°˜ì ì¸ ì»´í“¨í„° ë¹„ì „/ë”¥ëŸ¬ë‹ ì „ë¬¸ ìš©ì–´ íŒ¨í„´
    technical_patterns = [
        r'\b[A-Z]{2,}(?:-[A-Z0-9]+)*\b',  # CNN, R-CNN, YOLO ë“±
        r'\b(?:fast|faster|fastest)\s+rcnn\b',  # fast rcnn
        r'\b(?:anchor|anchors)\b',  # anchor
        r'\b(?:feature|features)\b',  # feature
        r'\b(?:detection|detector)\b',  # detection
        r'\b(?:classification|classifier)\b',  # classification
        r'\b(?:backbone|neck|head)\b',  # ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
        r'\b(?:convolutional|conv)\b',  # convolutional
        r'\b(?:neural|network)\b',  # neural network
        r'\b(?:multi[-\s]?scale|multiscale)\b',  # multi-scale
        r'\b(?:object|objects)\b',  # object
        r'\b(?:bounding|box|boxes)\b',  # bounding box
        r'\b(?:IoU|mAP|AP)\b',  # í‰ê°€ ì§€í‘œ
    ]
    
    terms = set()
    for pattern in technical_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        terms.update([match.lower() for match in matches])
    
    return list(terms)

def _generate_term_explanations(terms: List[str]) -> Dict[str, str]:
    """ì „ë¬¸ ìš©ì–´ì— ëŒ€í•œ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤"""
    explanations = {
        'cnn': 'í•©ì„±ê³± ì‹ ê²½ë§(Convolutional Neural Network): ì´ë¯¸ì§€ ì¸ì‹ì— íŠ¹í™”ëœ ë”¥ëŸ¬ë‹ ëª¨ë¸',
        'rcnn': 'R-CNN(Region-based CNN): ê°ì²´ ê²€ì¶œì„ ìœ„í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸',
        'fast rcnn': 'Fast R-CNN: R-CNNì˜ ì†ë„ë¥¼ ê°œì„ í•œ ê°ì²´ ê²€ì¶œ ëª¨ë¸',
        'faster rcnn': 'Faster R-CNN: Fast R-CNNì„ ë”ìš± ë¹ ë¥´ê²Œ ë§Œë“  ëª¨ë¸',
        'yolo': 'YOLO(You Only Look Once): ì‹¤ì‹œê°„ ê°ì²´ ê²€ì¶œì„ ìœ„í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸',
        'anchor': 'ì•µì»¤(Anchor): ê°ì²´ ê²€ì¶œì—ì„œ ì‚¬ìš©í•˜ëŠ” ì°¸ì¡° ë°•ìŠ¤',
        'feature': 'íŠ¹ì§•(Feature): ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ ì˜ë¯¸ ìˆëŠ” ì •ë³´',
        'detection': 'ê²€ì¶œ(Detection): ì´ë¯¸ì§€ì—ì„œ ê°ì²´ë¥¼ ì°¾ì•„ë‚´ëŠ” ê³¼ì •',
        'classification': 'ë¶„ë¥˜(Classification): ê°ì²´ì˜ ì¢…ë¥˜ë¥¼ êµ¬ë¶„í•˜ëŠ” ê³¼ì •',
        'backbone': 'ë°±ë³¸(Backbone): ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì£¼ìš” íŠ¹ì§• ì¶”ì¶œ ë¶€ë¶„',
        'convolutional': 'í•©ì„±ê³±(Convolutional): ì´ë¯¸ì§€ ì²˜ë¦¬ì— ì‚¬ìš©í•˜ëŠ” ìˆ˜í•™ì  ì—°ì‚°',
        'neural network': 'ì‹ ê²½ë§(Neural Network): ì¸ê°„ì˜ ë‡Œë¥¼ ëª¨ë°©í•œ ì¸ê³µì§€ëŠ¥ ëª¨ë¸',
        'multi-scale': 'ë‹¤ì¤‘ ìŠ¤ì¼€ì¼(Multi-scale): ë‹¤ì–‘í•œ í¬ê¸°ì˜ ê°ì²´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•',
        'object': 'ê°ì²´(Object): ì´ë¯¸ì§€ì—ì„œ ì¸ì‹í•˜ê³ ì í•˜ëŠ” ëŒ€ìƒ',
        'bounding box': 'ë°”ìš´ë”© ë°•ìŠ¤(Bounding Box): ê°ì²´ì˜ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì‚¬ê°í˜•',
        'iou': 'IoU(Intersection over Union): ê°ì²´ ê²€ì¶œ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œ',
        'map': 'mAP(mean Average Precision): ê°ì²´ ê²€ì¶œ ëª¨ë¸ì˜ ì „ì²´ì ì¸ ì„±ëŠ¥ ì§€í‘œ',
    }
    
    result = {}
    for term in terms:
        if term in explanations:
            result[term] = explanations[term]
        else:
            # ê°„ë‹¨í•œ ì„¤ëª… ìƒì„±
            result[term] = f"{term.upper()}: ê´€ë ¨ ì „ë¬¸ ìš©ì–´"
    
    return result

def _parse_latex_sections(tex_path: Path) -> List[dict]:
    """LaTeX íŒŒì¼ì„ ì„¹ì…˜ë³„ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤"""
    import re
    
    with open(tex_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    sections = []
    current_section = None
    current_content = []
    subsections = []  # subsection ì •ë³´ ì €ì¥
    
    lines = content.split('\n')
    
    for line in lines:
        # ì„¹ì…˜ ì‹œì‘ ê°ì§€
        if re.match(r'\\section\{([^}]*)\}', line):
            # ì´ì „ ì„¹ì…˜ ì €ì¥
            if current_section and current_content:
                sections.append({
                    "index": len(sections),
                    "title": current_section,
                    "content": '\n'.join(current_content).strip(),
                    "subsections": subsections.copy()
                })
            
            # ìƒˆ ì„¹ì…˜ ì‹œì‘
            title_match = re.match(r'\\section\{([^}]*)\}', line)
            current_section = title_match.group(1) if title_match else "Unknown Section"
            current_content = [line]
            subsections = []  # ìƒˆ ì„¹ì…˜ì˜ subsection ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
            
        elif re.match(r'\\subsection\{([^}]*)\}', line):
            # subsection ì •ë³´ ì¶”ì¶œ
            title_match = re.match(r'\\subsection\{([^}]*)\}', line)
            subsection_title = title_match.group(1) if title_match else "Unknown Subsection"
            subsections.append(subsection_title)
            
            # ì„œë¸Œì„¹ì…˜ë„ ì„¹ì…˜ìœ¼ë¡œ ì²˜ë¦¬
            if current_section and current_content:
                sections.append({
                    "index": len(sections),
                    "title": current_section,
                    "content": '\n'.join(current_content).strip(),
                    "subsections": subsections.copy()
                })
            
            current_section = subsection_title
            current_content = [line]
            
        elif re.match(r'\\begin\{abstract\}', line):
            # Abstract ì„¹ì…˜
            if current_section and current_content:
                sections.append({
                    "index": len(sections),
                    "title": current_section,
                    "content": '\n'.join(current_content).strip(),
                    "subsections": subsections.copy()
                })
            
            current_section = "Abstract"
            current_content = [line]
            subsections = []
            
        elif re.match(r'\\begin\{document\}', line):
            # Document ì‹œì‘
            if current_section and current_content:
                sections.append({
                    "index": len(sections),
                    "title": current_section,
                    "content": '\n'.join(current_content).strip(),
                    "subsections": subsections.copy()
                })
            
            current_section = "Introduction"
            current_content = [line]
            subsections = []
            
        else:
            # ì¼ë°˜ ë‚´ìš©
            if current_section:
                current_content.append(line)
    
    # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
    if current_section and current_content:
        sections.append({
            "index": len(sections),
            "title": current_section,
            "content": '\n'.join(current_content).strip(),
            "subsections": subsections.copy()
        })
    
    # ë¹ˆ ì„¹ì…˜ ì œê±°
    sections = [s for s in sections if s["content"].strip()]
    
    return sections

async def _rewrite_text(text: str) -> str:
    if model is None or tokenizer is None:
        raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    # ìˆ˜ì‹ í”Œë ˆì´ìŠ¤í™€ë” ì¹˜í™˜ â†’ ë¹„ìˆ˜ì‹ LaTeX ì •ë¦¬
    text_no_math, inline_map, block_map = _extract_math_placeholders(text)
    cleaned_text = _clean_latex_text(text_no_math)
    print(f"ğŸ” [DEBUG] ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {cleaned_text[:200]}...")

    # ì„¹ì…˜ ì œëª© íŒíŠ¸ê°€ ìˆìœ¼ë©´ ì „ë‹¬(ì—†ìœ¼ë©´ None)
    section_title = None
    prompt = _build_easy_prompt(cleaned_text, section_title)
    print(f"ğŸ” [DEBUG] í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {prompt[:300]}...")
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=float(os.getenv("EASY_TEMPERATURE", "0.7")),
            top_p=float(os.getenv("EASY_TOP_P", "0.9")),
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.2,  # ë°˜ë³µ ë°©ì§€ ê°•í™”
            no_repeat_ngram_size=3,  # 3-gram ë°˜ë³µ ë°©ì§€
        )
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"ğŸ” [DEBUG] ìƒì„±ëœ ì „ì²´ í…ìŠ¤íŠ¸: {generated[:500]}...")
    
    result = generated[len(prompt):].strip()
    print(f"ğŸ” [DEBUG] 1ì°¨ ê²°ê³¼: {result[:300]}...")

    # 2ë‹¨ê³„ ê²€ì¦/ê°œì„  (ì˜µì…˜, ê¸°ë³¸ í™œì„±í™”)
    use_verify = os.getenv("EASY_VERIFY", "true").lower() in ("1","true","yes")
    if use_verify:
        verify_prompt = _build_verify_prompt(result, section_title)
        v_inputs = tokenizer(verify_prompt, return_tensors="pt", truncation=True, max_length=2048)
        v_inputs = {k: v.to(device) for k, v in v_inputs.items()}
        with torch.inference_mode():
            v_out = model.generate(
                **v_inputs,
                max_new_tokens=MAX_NEW_TOKENS // 2,
                do_sample=True,
                temperature=float(os.getenv("EASY_TEMPERATURE", "0.6")),
                top_p=float(os.getenv("EASY_TOP_P", "0.9")),
                use_cache=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.2,
                no_repeat_ngram_size=3,
            )
        v_text = tokenizer.decode(v_out[0], skip_special_tokens=True)
        result = v_text[len(verify_prompt):].strip() or result
        print(f"ğŸ” [DEBUG] 2ì°¨ ê²€ì¦ ê²°ê³¼: {result[:300]}...")
    
    # í”Œë ˆì´ìŠ¤í™€ë” ë³µì›(ë¸”ë¡â†’ì¸ë¼ì¸ ìˆœì„œ)
    for key, val in block_map.items():
        result = result.replace(key, val)
    for key, val in inline_map.items():
        result = result.replace(key, val)

    print(f"ğŸ” [DEBUG] ìµœì¢… ê²°ê³¼(ë³µì› í›„) ë¯¸ë¦¬ë³´ê¸°: {result[:300]}...")
    return result

def _format_latex_output(text: str) -> str:
    """ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ LaTeX í˜•íƒœë¡œ ì •ë¦¬í•˜ë˜ ì›ë³¸ êµ¬ì¡°ëŠ” ìµœëŒ€í•œ ë³´ì¡´í•©ë‹ˆë‹¤"""
    import re
    
    # ì„¹ì…˜ ì œëª© ì •ë¦¬ (ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ì´ ìˆëŠ” ê²½ìš°ë§Œ)
    text = re.sub(r'^#+\s*(.+)$', r'\\section{\1}', text, flags=re.MULTILINE)
    text = re.sub(r'^##+\s*(.+)$', r'\\subsection{\1}', text, flags=re.MULTILINE)
    
    # êµµì€ ê¸€ì”¨ ì •ë¦¬ (ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ì´ ìˆëŠ” ê²½ìš°ë§Œ)
    text = re.sub(r'(?<!\$)\*\*([^$]+?)\*\*(?!\$)', r'\\textbf{\1}', text)
    
    # ê¸°ìš¸ì„ ê¸€ì”¨ ì •ë¦¬ (ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ì´ ìˆëŠ” ê²½ìš°ë§Œ)
    text = re.sub(r'(?<!\$)\*([^$*]+?)\*(?!\$)', r'\\textit{\1}', text)
    
    # ëª©ë¡ ì •ë¦¬ (ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ì´ ìˆëŠ” ê²½ìš°ë§Œ)
    text = re.sub(r'^[-â€¢]\s*(.+)$', r'\\item \1', text, flags=re.MULTILINE)
    
    # ì—°ì†ëœ itemì„ itemizeë¡œ ê°ì‹¸ê¸°
    lines = text.split('\n')
    result_lines = []
    in_itemize = False
    
    for line in lines:
        if line.strip().startswith('\\item'):
            if not in_itemize:
                result_lines.append('\\begin{itemize}')
                in_itemize = True
            result_lines.append(line)
        else:
            if in_itemize:
                result_lines.append('\\end{itemize}')
                in_itemize = False
            result_lines.append(line)
    
    if in_itemize:
        result_lines.append('\\end{itemize}')
    
    return '\n'.join(result_lines)

# -------------------- Viz í˜¸ì¶œ --------------------
async def _send_to_viz(paper_id: str, index: int, text_ko: str, out_dir: Path) -> VizResult:
    try:
        print(f"ğŸ” [DEBUG] Viz ëª¨ë¸ í˜¸ì¶œ: {VIZ_MODEL_URL}/viz")
        print(f"ğŸ” [DEBUG] ì „ì†¡ ë°ì´í„°: paper_id={paper_id}, index={index}, text_length={len(text_ko)}")
        print(f"ğŸ” [DEBUG] ì „ì†¡ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {text_ko[:200]}...")
        
        # Viz ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ ë¨¼ì € í™•ì¸
        try:
            async with httpx.AsyncClient(timeout=10) as client:
                health_response = await client.get(f"{VIZ_MODEL_URL.rstrip('/')}/health")
                if health_response.status_code != 200:
                    print(f"âŒ [ERROR] Viz ëª¨ë¸ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {health_response.status_code}")
                    return VizResult(ok=False, index=index, error="Viz ëª¨ë¸ì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ")
                print(f"âœ… [SUCCESS] Viz ëª¨ë¸ í—¬ìŠ¤ì²´í¬ ì„±ê³µ")
        except Exception as e:
            print(f"âŒ [ERROR] Viz ëª¨ë¸ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return VizResult(ok=False, index=index, error=f"Viz ëª¨ë¸ ì—°ê²° ë¶ˆê°€: {e}")
        
        # ì‹¤ì œ Viz ìš”ì²­
        async with httpx.AsyncClient(timeout=60) as client:  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
            r = await client.post(
                f"{VIZ_MODEL_URL.rstrip('/')}/viz",
                json={
                    "paper_id": paper_id,
                    "index": index,
                    "rewritten_text": text_ko,
                    "target_lang": "ko",
                    "bilingual": "missing",
                    "text_type": "easy_korean",  # ì‰½ê²Œ ë³€í™˜ëœ í•œêµ­ì–´ì„ì„ ëª…ì‹œ
                },
            )
            print(f"ğŸ” [DEBUG] Viz ëª¨ë¸ ì‘ë‹µ: {r.status_code}")
            
            if r.status_code != 200:
                print(f"âŒ [ERROR] Viz ëª¨ë¸ ì‘ë‹µ ì‹¤íŒ¨: {r.status_code} - {r.text}")
                return VizResult(ok=False, index=index, error=f"Viz ëª¨ë¸ ì‘ë‹µ ì‹¤íŒ¨: {r.status_code}")
            
            try:
                data = r.json()
                print(f"ğŸ” [DEBUG] Viz ëª¨ë¸ ì‘ë‹µ ë°ì´í„°: {data}")
            except Exception as json_error:
                print(f"âŒ [ERROR] Viz ëª¨ë¸ ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {json_error}")
                return VizResult(ok=False, index=index, error=f"Viz ëª¨ë¸ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {json_error}")

        img_path = data.get("image_path")

        if not img_path and data.get("image_base64"):
            out_path = out_dir / f"{index:06d}.png"
            out_path.write_bytes(base64.b64decode(data["image_base64"]))
            print(f"âœ… [SUCCESS] ì´ë¯¸ì§€ ì €ì¥: {out_path}")
            return VizResult(ok=True, index=index, image_path=str(out_path))

        if not img_path and data.get("image_url"):
            out_path = out_dir / f"{index:06d}.png"
            async with httpx.AsyncClient(timeout=60) as client:
                rr = await client.get(data["image_url"])
                rr.raise_for_status()
                out_path.write_bytes(rr.content)
            print(f"âœ… [SUCCESS] ì´ë¯¸ì§€ ì €ì¥: {out_path}")
            return VizResult(ok=True, index=index, image_path=str(out_path))

        if img_path:
            # Viz ëª¨ë¸ì—ì„œ ìƒì„±ëœ ì´ë¯¸ì§€ê°€ ë‹¤ë¥¸ ê²½ë¡œì— ìˆìœ¼ë©´ ë³µì‚¬
            img_path_obj = Path(img_path)
            if img_path_obj.exists() and not img_path_obj.parent.samefile(out_dir):
                # ë‹¤ë¥¸ ê²½ë¡œì— ìˆìœ¼ë©´ easy_outputsë¡œ ë³µì‚¬
                out_path = out_dir / f"{index:06d}.png"
                import shutil
                shutil.copy2(img_path_obj, out_path)
                print(f"âœ… [SUCCESS] ì´ë¯¸ì§€ ë³µì‚¬: {img_path} -> {out_path}")
                return VizResult(ok=True, index=index, image_path=str(out_path))
            else:
                print(f"âœ… [SUCCESS] ì´ë¯¸ì§€ ê²½ë¡œ: {img_path}")
                return VizResult(ok=True, index=index, image_path=str(img_path))

        print(f"âŒ [ERROR] ì´ë¯¸ì§€ ê²½ë¡œ ì—†ìŒ: {data}")
        return VizResult(ok=False, index=index, error="No image_path from viz")
    except httpx.ConnectError as e:
        print(f"âŒ [ERROR] Viz ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
        return VizResult(ok=False, index=index, error=f"Viz ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
    except httpx.TimeoutException as e:
        print(f"âŒ [ERROR] Viz ëª¨ë¸ íƒ€ì„ì•„ì›ƒ: {e}")
        return VizResult(ok=False, index=index, error=f"Viz ëª¨ë¸ íƒ€ì„ì•„ì›ƒ: {e}")
    except Exception as e:
        print(f"âŒ [ERROR] Viz ëª¨ë¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return VizResult(ok=False, index=index, error=str(e))

# -------------------- ì—”ë“œí¬ì¸íŠ¸ --------------------
@app.get("/")
async def root():
    return {"message": "POLO Easy Model API", "model": BASE_MODEL}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if (model is not None and tokenizer is not None) else "unavailable",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None,
        "gpu_available": gpu_available,
        "gpu_device": torch.cuda.get_device_name(0) if gpu_available else None,
        "model_name": BASE_MODEL,
        "max_new_tokens": MAX_NEW_TOKENS,
        "cache_dir": str(CACHE_DIR),
    }

@app.get("/healthz")
async def healthz():
    return await health()

@app.post("/easy", response_model=TextResponse)
async def simplify_text(request: TextRequest):
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    simplified_text = await _rewrite_text(request.text)
    return TextResponse(simplified_text=simplified_text, translated_text=None)

@app.post("/generate")
async def generate_json(request: TextRequest):
    start_time = time.time()
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    extracted = _extract_sections(request.text)
    data_schema = json.loads(json.dumps(GROUND_SCHEMA))  # deepcopy
    for k in ["abstract","introduction","methods","results","discussion","conclusion"]:
        data_schema[k]["original"] = extracted.get(k, "")

    instruction = (
        "ë„ˆëŠ” ì¹œê·¼í•œ ê³¼í•™ ì„ ìƒë‹˜ì´ë‹¤. ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆì˜ 'í‚¤ì™€ êµ¬ì¡°'ë¥¼ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ê³  "
        "'ê°’'ë§Œ ì±„ì›Œë¼. ì¶œë ¥ì€ ì˜¤ì§ 'ìœ íš¨í•œ JSON' í•˜ë‚˜ë§Œ í—ˆìš©ëœë‹¤(ì½”ë“œë¸”ë¡/ì„¤ëª…/ì£¼ì„ ê¸ˆì§€).\n\n"
        "ğŸ¯ ê° ì„¹ì…˜ì˜ 'easy' ì‘ì„± ë°©ë²•:\n"
        "- ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‰½ê³  ì¬ë¯¸ìˆê²Œ ë³€í™˜\n"
        "- ì „ë¬¸ ìš©ì–´ëŠ” ì¼ìƒ ìš©ì–´ë¡œ ë°”ê¾¸ê¸°\n"
        "- ë³µì¡í•œ ë‚´ìš©ì€ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…\n"
        "- êµ¬ì²´ì ì¸ ë¹„ìœ ì™€ ì˜ˆì‹œ ì‚¬ìš©\n"
        "- 'ìš”ì•½í•˜ë©´', 'ì‰½ê²Œ ë§í•˜ë©´' ê°™ì€ ì •ë¦¬ ë¬¸êµ¬ í™œìš©\n"
        "- ì¹œê·¼í•œ í†¤ìœ¼ë¡œ 4-8ë¬¸ì¥ ì‘ì„± (ì¡´ëŒ“ë§ '~í•©ë‹ˆë‹¤', '~ì…ë‹ˆë‹¤' ì‚¬ìš©, '~ìš”'ë¡œ ëë‚˜ì§€ ì•Šê²Œ)\n\n"
        "originalì´ ë¹„ì–´ ìˆìœ¼ë©´ í•´ë‹¹ 'easy'ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ë‚¨ê²¨ë¼. ì™¸ë¶€ ì§€ì‹/ì¶”ì¸¡ ê¸ˆì§€."
    )

    schema_str = json.dumps(data_schema, ensure_ascii=False, indent=2)
    context_only = {k: extracted[k] for k in ["abstract","introduction","methods","results","discussion","conclusion"]}
    context_str = json.dumps(context_only, ensure_ascii=False, indent=2)

    prompt = (
        f"{instruction}\n\n=== ì¶œë ¥ ìŠ¤í‚¤ë§ˆ(ê°’ë§Œ ì±„ì›Œë¼) ===\n{schema_str}\n\n"
        f"=== ì„¹ì…˜ë³„ original (ì´ í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©) ===\n{context_str}\n\n"
        "JSONë§Œ ì¶œë ¥:"
    )

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    t0 = time.time()
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            use_cache=True,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    inference_time = time.time() - t0

    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    raw = generated[len(prompt):].strip()

    try:
        data = _coerce_json(raw)
        if not _is_meaningful(data):
            raise ValueError("empty_json")
    except Exception:
        strict_instruction = (
            "ìŠ¤í‚¤ë§ˆì˜ í‚¤/êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³  ê°’ë§Œ ì±„ìš´ 'ìœ íš¨í•œ JSON'ë§Œ ì¶œë ¥í•˜ë¼. "
            "ë°˜ë“œì‹œ '{'ë¡œ ì‹œì‘í•´ '}'ë¡œ ëë‚˜ì•¼ í•œë‹¤. ì™¸ë¶€ì§€ì‹/ì¶”ì¸¡ ê¸ˆì§€."
        )
        strict_prompt = f"{strict_instruction}\n\nìŠ¤í‚¤ë§ˆ:\n{schema_str}\n\nì„¹ì…˜ original:\n{context_str}\n\nJSONë§Œ ì¶œë ¥:"
        inputs2 = tokenizer(strict_prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs2 = {k: v.to(device) for k, v in inputs2.items()}
        with torch.inference_mode():
            outputs2 = model.generate(
                **inputs2,
                max_new_tokens=min(MAX_NEW_TOKENS, 800),
                do_sample=False,
                use_cache=True,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        gen2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)
        raw2 = gen2[len(strict_prompt):].strip()
        try:
            data = _coerce_json(raw2)
        except Exception:
            data = data_schema

    total_time = time.time() - start_time
    data["processing_info"] = {
        "gpu_used": gpu_available,
        "inference_time": inference_time,
        "total_time": total_time,
        "input_length": len(request.text),
        "output_length": len(str(data)),
    }
    return data

@app.post("/batch", response_model=BatchResult)
async def batch_generate(req: BatchRequest):
    print(f"ğŸ” [DEBUG] Easy /batch ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨")
    print(f"ğŸ” [DEBUG] ìš”ì²­ ë°ì´í„°:")
    print(f"  - paper_id: {req.paper_id}")
    print(f"  - chunks_jsonl: {req.chunks_jsonl}")
    print(f"  - output_dir: {req.output_dir}")
    
    # merged_body.tex íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½
    tex_path = Path(req.chunks_jsonl).parent / "merged_body.tex"
    out_dir = Path(req.output_dir).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ” [DEBUG] íŒŒì¼ ê²½ë¡œ í™•ì¸:")
    print(f"  - tex_path: {tex_path}")
    print(f"  - tex_path ì¡´ì¬: {tex_path.exists()}")
    print(f"  - out_dir: {out_dir}")
    print(f"  - out_dir ìƒì„±ë¨: {out_dir.exists()}")

    if not tex_path.exists():
        print(f"âŒ [ERROR] merged_body.tex íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {tex_path}")
        raise HTTPException(status_code=400, detail=f"merged_body.tex not found: {tex_path}")

    # LaTeX íŒŒì¼ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• 
    sections = _parse_latex_sections(tex_path)
    print(f"ğŸ” [DEBUG] ì´ {len(sections)}ê°œ ì„¹ì…˜ íŒŒì‹±ë¨")
    
    if not sections:
        print(f"âŒ [ERROR] ìœ íš¨í•œ ì„¹ì…˜ì´ ì—†ìŒ")
        raise HTTPException(status_code=400, detail="No valid sections found in merged_body.tex")

    # ëª¨ë¸ ìƒíƒœ í™•ì¸
    if model is None or tokenizer is None:
        print(f"âŒ [ERROR] ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        raise HTTPException(status_code=500, detail="Model not loaded")

    print(f"ğŸ” [DEBUG] ëª¨ë¸ ìƒíƒœ: model={model is not None}, tokenizer={tokenizer is not None}")
    print(f"ğŸ” [DEBUG] ë””ë°”ì´ìŠ¤: {device}, GPU ì‚¬ìš©: {gpu_available}")

    results: List[VizResult] = []

    print(f"ğŸ” [DEBUG] ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘...")
    # ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì¸í•œ ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€)
    for idx, section in enumerate(sections):
        try:
            print(f"ğŸ” [DEBUG] ì„¹ì…˜ {idx}/{len(sections)} ì²˜ë¦¬ ì‹œì‘: {section['title']}")
            ko = await _rewrite_text(section["content"])
            print(f"ğŸ” [DEBUG] ì„¹ì…˜ {idx}/{len(sections)} ë³€í™˜ ì™„ë£Œ: {ko[:100]}...")
            
            # Google Translatorë¡œ í•œêµ­ì–´ ë²ˆì—­
            print(f"ğŸ” [DEBUG] ì„¹ì…˜ {idx}/{len(sections)} í•œêµ­ì–´ ë²ˆì—­ ì‹œì‘...")
            ko_translated = _translate_to_korean(ko)
            print(f"ğŸ” [DEBUG] ì„¹ì…˜ {idx}/{len(sections)} í•œêµ­ì–´ ë²ˆì—­ ì™„ë£Œ: {ko_translated[:100]}...")
            
            # í•œêµ­ì–´ ë²ˆì—­ë³¸ìœ¼ë¡œ Viz ì²˜ë¦¬
            vz = await _send_to_viz(req.paper_id, idx, ko_translated, out_dir)
            print(f"ğŸ” [DEBUG] ì„¹ì…˜ {idx}/{len(sections)} Viz ì™„ë£Œ: {vz.ok}")
            
            # ê²°ê³¼ì— ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ì €ì¥
            vz.easy_text = ko_translated
            vz.section_title = section["title"]
            results.append(vz)
            
            # ì§„í–‰ë¥  í‘œì‹œ
            completed = len(results)
            progress = (completed / len(sections)) * 100
            print(f"ğŸ“Š [PROGRESS] {completed}/{len(sections)} ({progress:.1f}%) ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ [ERROR] ì„¹ì…˜ {idx}/{len(sections)} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            results.append(VizResult(ok=False, index=idx, error=str(e)))

    ok_cnt = sum(1 for r in results if r.ok)
    fail_cnt = len(results) - ok_cnt
    
    print(f"ğŸ” [DEBUG] ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ:")
    print(f"  - ì´ ì„¹ì…˜: {len(sections)}")
    print(f"  - ì„±ê³µ: {ok_cnt}")
    print(f"  - ì‹¤íŒ¨: {fail_cnt}")
    
    result = BatchResult(
        ok=fail_cnt == 0,
        paper_id=req.paper_id,
        count=len(sections),
        success=ok_cnt,
        failed=fail_cnt,
        out_dir=str(out_dir),
        images=sorted(results, key=lambda r: r.index),
    )
    
    # JSON ê²°ê³¼ íŒŒì¼ ìƒì„± (í”„ë¡ íŠ¸ì—”ë“œìš©)
    json_result = {
        "paper_id": req.paper_id,
        "total_sections": len(sections),
        "success_count": ok_cnt,
        "failed_count": fail_cnt,
        "sections": []
    }
    
    # ê° ì„¹ì…˜ë³„ ê²°ê³¼ ì¶”ê°€
    for i, section in enumerate(sections):
        section_result = {
            "index": i,
            "title": section["title"],
            "original_content": section["content"][:200] + "..." if len(section["content"]) > 200 else section["content"],
            "easy_text": "",
            "korean_translation": "",
            "image_path": "",
            "status": "failed"
        }
        
        # í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ê²°ê³¼ ì°¾ê¸°
        for r in results:
            if r.index == i:
                section_result["status"] = "success" if r.ok else "failed"
                if r.ok and r.image_path:
                    section_result["image_path"] = r.image_path
                if hasattr(r, 'easy_text') and r.easy_text:
                    section_result["korean_translation"] = r.easy_text
                break
        
        json_result["sections"].append(section_result)
    
    # LaTeX ê²°ê³¼ íŒŒì¼ ìƒì„±
    latex_result_path = out_dir / "easy_results.tex"
    _save_latex_results(sections, results, latex_result_path)
    
    # HTML ê²°ê³¼ íŒŒì¼ ìƒì„±
    html_result_path = out_dir / "easy_results.html"
    _save_html_results(sections, results, html_result_path, req.paper_id)
    
    # JSON íŒŒì¼ ì €ì¥
    json_file_path = out_dir / "easy_results.json"
    with open(json_file_path, "w", encoding="utf-8") as f:
        json.dump(json_result, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ [JSON] ê²°ê³¼ íŒŒì¼ ì €ì¥: {json_file_path}")
    print(f"ğŸ“„ [LaTeX] ê²°ê³¼ íŒŒì¼ ì €ì¥: {latex_result_path}")
    print(f"ğŸ“„ [HTML] ê²°ê³¼ íŒŒì¼ ì €ì¥: {html_result_path}")
    print(f"âœ… [SUCCESS] Easy ëª¨ë¸ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {result}")
    return result

def _save_latex_results(sections: List[dict], results: List[VizResult], output_path: Path):
    """LaTeX í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤"""
    latex_content = []
    latex_content.append("\\documentclass{article}")
    latex_content.append("\\usepackage[utf8]{inputenc}")
    latex_content.append("\\usepackage{korean}")
    latex_content.append("\\usepackage{graphicx}")
    latex_content.append("\\usepackage{amsmath}")
    latex_content.append("\\usepackage{amsfonts}")
    latex_content.append("\\begin{document}")
    latex_content.append("")
    
    # ì„¹ì…˜ë³„ ê²°ê³¼ ì¶”ê°€
    for i, (section, result) in enumerate(zip(sections, results)):
        if result.ok and result.easy_text:
            # ì„¹ì…˜ ì œëª©
            latex_content.append(f"\\section{{{section['title']}}}")
            latex_content.append("")
            
            # ë³€í™˜ëœ í…ìŠ¤íŠ¸ (LaTeX í˜•íƒœ)
            latex_content.append(result.easy_text)
            latex_content.append("")
            
            # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if result.image_path and Path(result.image_path).exists():
                latex_content.append("\\begin{figure}[h]")
                latex_content.append("\\centering")
                latex_content.append(f"\\includegraphics[width=0.8\\textwidth]{{{result.image_path}}}")
                latex_content.append(f"\\caption{{{section['title']} ê´€ë ¨ ì‹œê°í™”}}")
                latex_content.append("\\end{figure}")
                latex_content.append("")
    
    latex_content.append("\\end{document}")
    
    # íŒŒì¼ ì €ì¥
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(latex_content))

def _get_current_datetime() -> str:
    """í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤"""
    from datetime import datetime
    return datetime.now().strftime("%Yë…„ %mì›” %dì¼ %H:%M:%S")

def _save_html_results(sections: List[dict], results: List[VizResult], output_path: Path, paper_id: str):
    """HTML í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤"""
    html_content = []
    
    # HTML í—¤ë” (ArXiv ìŠ¤íƒ€ì¼)
    html_content.append("""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>POLO - ì‰¬ìš´ ë…¼ë¬¸ ì„¤ëª…</title>
    <style>
        body {
            font-family: 'Times New Roman', 'Noto Serif KR', serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: white;
            color: #000;
            font-size: 12pt;
        }
        .paper-container {
            background: white;
            padding: 40px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            border: 1px solid #ddd;
        }
        .copyright-notice {
            background: #f0f0f0;
            border: 2px solid #333;
            padding: 15px;
            margin-bottom: 30px;
            text-align: center;
            font-weight: bold;
            font-size: 11pt;
        }
        .copyright-notice .title {
            font-size: 14pt;
            margin-bottom: 10px;
            color: #d32f2f;
        }
        .copyright-notice .content {
            font-size: 10pt;
            line-height: 1.4;
        }
        .paper-header {
            text-align: center;
            margin-bottom: 40px;
            border-bottom: 2px solid #000;
            padding-bottom: 20px;
        }
        .paper-title {
            font-size: 18pt;
            font-weight: bold;
            margin-bottom: 15px;
            line-height: 1.3;
        }
        .paper-subtitle {
            font-size: 14pt;
            color: #666;
            margin-bottom: 20px;
            font-style: italic;
        }
        .paper-meta {
            font-size: 10pt;
            color: #666;
            margin-bottom: 10px;
        }
        .section {
            margin-bottom: 30px;
        }
        .section h2 {
            font-size: 16pt;
            font-weight: bold;
            margin: 30px 0 15px 0;
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        .section h3 {
            font-size: 14pt;
            font-weight: bold;
            margin: 20px 0 10px 0;
        }
        .subsection-title {
            font-size: 13pt;
            font-weight: bold;
            margin: 15px 0 8px 0;
            color: #333;
            border-left: 3px solid #1976d2;
            padding-left: 10px;
        }
        .subsection-section {
            margin-bottom: 25px;
            padding: 15px;
            background: #fafafa;
            border-radius: 5px;
            border: 1px solid #e0e0e0;
        }
        .content {
            font-size: 12pt;
            line-height: 1.6;
            text-align: justify;
            margin-bottom: 15px;
        }
        .content p {
            margin-bottom: 12px;
            text-indent: 1.5em;
        }
        .content strong {
            font-weight: bold;
        }
        .content em {
            font-style: italic;
        }
        .content ul, .content ol {
            margin: 15px 0;
            padding-left: 30px;
        }
        .content li {
            margin-bottom: 6px;
        }
        .math {
            background: #f9f9f9;
            padding: 10px;
            border: 1px solid #ddd;
            margin: 15px 0;
            font-family: 'Times New Roman', serif;
            font-size: 11pt;
            text-align: center;
            overflow-x: auto;
        }
        .image-container {
            text-align: center;
            margin: 25px 0;
            padding: 15px;
            border: 1px solid #ddd;
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ccc;
        }
        .image-caption {
            margin-top: 10px;
            font-style: italic;
            font-size: 10pt;
            color: #666;
        }
        .download-btn {
            position: fixed;
            top: 20px;
            right: 20px;
            background: #1976d2;
            color: white;
            border: none;
            padding: 10px 20px;
            cursor: pointer;
            font-size: 11pt;
            font-weight: bold;
            box-shadow: 0 2px 8px rgba(0,0,0,0.3);
            transition: all 0.3s ease;
        }
        .download-btn:hover {
            background: #1565c0;
            transform: translateY(-1px);
        }
        .footer {
            text-align: center;
            margin-top: 50px;
            padding: 20px;
            font-size: 10pt;
            color: #666;
            border-top: 1px solid #ddd;
        }
        .abstract {
            background: #f8f8f8;
            padding: 20px;
            border-left: 4px solid #1976d2;
            margin: 20px 0;
        }
        .abstract h3 {
            font-size: 12pt;
            font-weight: bold;
            margin: 0 0 10px 0;
            text-transform: uppercase;
        }
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            .paper-container {
                padding: 20px;
            }
            .paper-title {
                font-size: 16pt;
            }
        }
    </style>
</head>
<body>""")
    
    # ì €ì‘ê¶Œ í‘œì‹œ ë° í—¤ë” ì„¹ì…˜
    html_content.append(f"""
    <div class="paper-container">
        <div class="copyright-notice">
            <div class="title">âš ï¸ ì €ì‘ê¶Œ ê³ ì§€</div>
            <div class="content">
                ì´ ë¬¸ì„œëŠ” POLO AI ë…¼ë¬¸ ì´í•´ ë„ìš°ë¯¸ì— ì˜í•´ ì›ë³¸ ë…¼ë¬¸ì„ ê³ ë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‰½ê²Œ ë³€í™˜í•œ ê²ƒì…ë‹ˆë‹¤.<br>
                ì›ë³¸ ë…¼ë¬¸ì˜ ì €ì‘ê¶Œì€ ì› ì €ìì—ê²Œ ìˆìœ¼ë©°, ì´ ë³€í™˜ëœ ë¬¸ì„œëŠ” êµìœ¡ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.<br>
                ìƒì—…ì  ì´ìš©ì´ë‚˜ ì¬ë°°í¬ ì‹œì—ëŠ” ì›ë³¸ ë…¼ë¬¸ì˜ ì €ì‘ê¶Œ ì •ì±…ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.
            </div>
        </div>
        
        <div class="paper-header">
            <div class="paper-title">POLO ë…¼ë¬¸ ì´í•´ ë„ìš°ë¯¸ ë³€í™˜ ê²°ê³¼</div>
            <div class="paper-subtitle">ë³µì¡í•œ ë…¼ë¬¸ì„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜í•œ ê²°ê³¼</div>
            <div class="paper-meta">
                ë…¼ë¬¸ ID: {paper_id} | ë³€í™˜ ì¼ì‹œ: {_get_current_datetime()}
            </div>
        </div>
    """)
    
    # ì„¹ì…˜ë³„ ê²°ê³¼ ì¶”ê°€
    for i, (section, result) in enumerate(zip(sections, results)):
        if result.ok and result.easy_text:
            # Abstract ì„¹ì…˜ì€ íŠ¹ë³„í•œ ìŠ¤íƒ€ì¼ ì ìš©
            if section['title'].lower() in ['abstract', 'ìš”ì•½']:
                html_content.append(f"""
        <div class="abstract">
            <h3>{section['title']}</h3>
            <div class="content">
                {_latex_to_html(result.easy_text)}
            </div>
        </div>
                """)
            else:
                # subsectionì´ ìˆìœ¼ë©´ ê°ê° ë¶„ë¦¬í•˜ì—¬ í‘œì‹œ
                if 'subsections' in section and section['subsections']:
                    # ë©”ì¸ ì„¹ì…˜ ì œëª©
                    html_content.append(f"""
        <div class="section">
            <h2>{section['title']}</h2>
        </div>
                    """)
                    
                    # ê° subsectionê³¼ ë‚´ìš©ì„ ë¶„ë¦¬í•˜ì—¬ í‘œì‹œ
                    for sub_idx, subsection in enumerate(section['subsections']):
                        html_content.append(f"""
        <div class="subsection-section">
            <div class="subsection-title">{subsection}</div>
            <div class="content">
                {_latex_to_html(result.easy_text)}
            </div>
        </div>
                        """)
                else:
                    # subsectionì´ ì—†ìœ¼ë©´ ì¼ë°˜ ì„¹ì…˜ìœ¼ë¡œ í‘œì‹œ
                    html_content.append(f"""
        <div class="section">
            <h2>{section['title']}</h2>
            <div class="content">
                {_latex_to_html(result.easy_text)}
            </div>
        </div>
                    """)
            
            # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if result.image_path and Path(result.image_path).exists():
                image_name = Path(result.image_path).name
                html_content.append(f"""
        <div class="image-container">
            <img src="{image_name}" alt="{section['title']} ê´€ë ¨ ì‹œê°í™”">
            <div class="image-caption">Figure {i+1}: {section['title']} ê´€ë ¨ ì‹œê°í™”</div>
        </div>
                """)
    
    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ê³¼ í‘¸í„°
    html_content.append(f"""
        <button class="download-btn" onclick="downloadHTML()">ğŸ“¥ HTML ë‹¤ìš´ë¡œë“œ</button>
        
        <div class="footer">
            <p><strong>POLO AI ë…¼ë¬¸ ì´í•´ ë„ìš°ë¯¸</strong></p>
            <p>ì´ ë¬¸ì„œëŠ” ì›ë³¸ ë…¼ë¬¸ì„ ê³ ë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‰½ê²Œ ë³€í™˜í•œ ê²ƒì…ë‹ˆë‹¤</p>
            <p>ë³€í™˜ ì¼ì‹œ: {_get_current_datetime()} | ë…¼ë¬¸ ID: {paper_id}</p>
            <p style="font-size: 9pt; color: #999; margin-top: 20px;">
                ë³¸ ë¬¸ì„œëŠ” êµìœ¡ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©ë˜ì–´ì•¼ í•˜ë©°, ìƒì—…ì  ì´ìš© ì‹œ ì›ë³¸ ë…¼ë¬¸ì˜ ì €ì‘ê¶Œ ì •ì±…ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.
            </p>
        </div>
    </div>
        
        <script>
            function downloadHTML() {{
                const element = document.documentElement.outerHTML;
                const blob = new Blob([element], {{type: 'text/html'}});
                const url = URL.createObjectURL(blob);
                const a = document.createElement('a');
                a.href = url;
                a.download = 'polo_easy_explanation_{paper_id}.html';
                document.body.appendChild(a);
                a.click();
                document.body.removeChild(a);
                URL.revokeObjectURL(url);
            }}
        </script>
    </body>
</html>""")
    
    # íŒŒì¼ ì €ì¥
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(html_content))

def _latex_to_html(latex_text: str, technical_terms: Dict[str, str] = None) -> str:
    """LaTeX í…ìŠ¤íŠ¸ë¥¼ HTMLë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (ArXiv ìŠ¤íƒ€ì¼)"""
    import re
    
    # LaTeX ëª…ë ¹ì–´ë¥¼ HTMLë¡œ ë³€í™˜
    html_text = latex_text
    
    # ì„¹ì…˜ ì œëª© (ArXiv ìŠ¤íƒ€ì¼ë¡œ ëŒ€ë¬¸ì ë³€í™˜)
    html_text = re.sub(r'\\section\{([^}]+)\}', r'<h2>\1</h2>', html_text)
    html_text = re.sub(r'\\subsection\{([^}]+)\}', r'<h3>\1</h3>', html_text)
    
    # í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
    html_text = re.sub(r'\\textbf\{([^}]+)\}', r'<strong>\1</strong>', html_text)
    html_text = re.sub(r'\\textit\{([^}]+)\}', r'<em>\1</em>', html_text)
    
    # ìˆ˜ì‹ ì²˜ë¦¬ (ArXiv ìŠ¤íƒ€ì¼)
    html_text = re.sub(r'\$([^$]+)\$', r'<span class="math">\1</span>', html_text)
    html_text = re.sub(r'\$\$([^$]+)\$\$', r'<div class="math">\1</div>', html_text)
    
    # ëª©ë¡ ì²˜ë¦¬
    html_text = re.sub(r'\\begin\{itemize\}', '<ul>', html_text)
    html_text = re.sub(r'\\end\{itemize\}', '</ul>', html_text)
    html_text = re.sub(r'\\begin\{enumerate\}', '<ol>', html_text)
    html_text = re.sub(r'\\end\{enumerate\}', '</ol>', html_text)
    html_text = re.sub(r'\\item\s*', '<li>', html_text)
    
    # ë§í¬ ì²˜ë¦¬ (LaTeX \href ëª…ë ¹ì–´)
    html_text = re.sub(r'\\href\{([^}]+)\}\{([^}]+)\}', r'<a href="\1" target="_blank">\2</a>', html_text)
    
    # URL ìë™ ë§í¬ ì²˜ë¦¬
    html_text = re.sub(r'(https?://[^\s<>"]+)', r'<a href="\1" target="_blank">\1</a>', html_text)
    
    # ë¬¸ë‹¨ ì²˜ë¦¬ (ArXiv ìŠ¤íƒ€ì¼ - ë“¤ì—¬ì“°ê¸° ì ìš©)
    html_text = re.sub(r'\n\n+', '</p><p>', html_text)
    html_text = '<p>' + html_text + '</p>'
    
    # ë¹ˆ ë¬¸ë‹¨ ì œê±°
    html_text = re.sub(r'<p>\s*</p>', '', html_text)
    
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    html_text = re.sub(r' +', ' ', html_text)
    
    # ìš©ì–´ ì„¤ëª… ì¶œë ¥ì€ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ(ìš”ì²­ì— ë”°ë¼ ì œê±°)
    
    return html_text

@app.post("/from-transport", response_model=BatchResult)
async def generate_from_transport(req: TransportRequest):
    tp = Path(req.transport_path)
    if not tp.exists():
        raise HTTPException(status_code=400, detail=f"transport.json not found: {tp}")

    try:
        data = json.loads(tp.read_text(encoding="utf-8"))
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"invalid transport.json: {e}")

    # chunks ê²½ë¡œ ìš°ì„ : artifacts.chunks.path â†’ tp.parent/chunks.jsonl â†’ tp.parent/chunks.jsonl.gz
    chunks_path: Optional[Path] = None
    try:
        chunks_path_str = (((data.get("artifacts", {}) or {}).get("chunks", {}) or {}).get("path"))
        if chunks_path_str:
            chunks_path = Path(chunks_path_str)
    except Exception:
        chunks_path = None
    if chunks_path is None:
        base_dir = tp.parent
        cand1 = base_dir / "chunks.jsonl"
        cand2 = base_dir / "chunks.jsonl.gz"
        if cand1.exists():
            chunks_path = cand1
        elif cand2.exists():
            chunks_path = cand2

    if chunks_path is None or not chunks_path.exists():
        raise HTTPException(status_code=400, detail=f"chunks file not found near transport: {tp}")

    # ì¶œë ¥ ê²½ë¡œ
    out_dir = Path(req.output_dir).resolve() if req.output_dir else (tp.parent / "easy_outputs").resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    # ê¸°ì¡´ ë°°ì¹˜ ë¡œì§ ì¬ì‚¬ìš©
    items: List[dict] = []
    open_fn = gzip.open if str(chunks_path).endswith(".gz") else open
    with open_fn(chunks_path, "rt", encoding="utf-8") as f:
        for i, line in enumerate(f):
            try:
                obj = json.loads(line)
                text = obj.get("text")
                if not isinstance(text, str) or not text.strip():
                    continue
                items.append({"index": i, "text": text})
            except Exception:
                continue

    sem = anyio.Semaphore(EASY_CONCURRENCY)
    results: List[VizResult] = []

    async def worker(item: dict):
        async with sem:
            idx = item["index"]
            try:
                ko = await _rewrite_text(item["text"])
                vz = await _send_to_viz(req.paper_id, idx, ko, out_dir)
                results.append(vz)
            except Exception as e:
                results.append(VizResult(ok=False, index=idx, error=str(e)))

    async with anyio.create_task_group() as tg:
        for item in items:
            tg.start_soon(worker, item)

    ok_cnt = sum(1 for r in results if r.ok)
    fail_cnt = len(results) - ok_cnt
    return BatchResult(
        ok=fail_cnt == 0,
        paper_id=req.paper_id,
        count=len(items),
        success=ok_cnt,
        failed=fail_cnt,
        out_dir=str(out_dir),
        images=sorted(results, key=lambda r: r.index),
    )

# -------------------- main --------------------
if __name__ == "__main__":
    try:
        if torch.cuda.is_available():
            print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
            print("ğŸ”§ ë””ë°”ì´ìŠ¤: cuda, dtype: float16")
        else:
            print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            print("ğŸ”§ ë””ë°”ì´ìŠ¤: cpu, dtype: float32")

        print("ğŸš€ Easy Model ì„œë²„ ì‹œì‘ ì¤‘...")
        uvicorn.run(app, host="0.0.0.0", port=5003)
    except Exception as e:
        print(f"âŒ Easy Model ì‹œì‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
