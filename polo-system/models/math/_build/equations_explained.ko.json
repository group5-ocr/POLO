{
  "overview": "보조\n\\end{문서}",
  "items": [
    {
      "index": 1,
      "line_start": 120,
      "line_end": 120,
      "kind": "inline($ $)",
      "env": "",
      "equation": "\\Pr(\\textrm{Object}) * \\textrm{IOU}_{\\textrm{pred}}^{\\textrm{truth}}",
      "explanation": "assistant\n### 예제\n$$\n\\Pr(\\textrm{Object}) * \\textrm{IOU}_{\\textrm{pred}}^{\\textrm{truth}}\n$$\n### 설명\n방정식을 단계별로 분석해 보겠습니다.\n\n1. \\(\\Pr(\\textrm{ Object})\\): 주어진 객체가 참일 확률을 나타냅니다. 객체가 옳거나 유효할 가능성을 측정하는 척도입니다.\n\n2. : 이는 객체가 참일 초기 조건 또는 초기 확률을 나타냅니다. 이는 객체가 참일 확률을 계산하는 시작점입니다.\n\n이 방정식은 두 확률을 곱합니다. 즉, 객체가 참일 전체 확률은 객체가 참일 초기 확률과 초기 조건에서 객체가 참일 확률의 곱입니다.\n\n### 결론\n이 방정식의 핵심 목적은 객체가 참일 초기 확률과 초기 조건에서 객체가 참일 확률을 결합하여 객체가 참일 전체 확률을 계산하는 것입니다. 이는 초기 조건과 후속 확률을 기반으로 사건이나 객체가 참일 가능성을 결정해야 하는 다양한 응용 분야에서 유용하게 사용될 수 있습니다."
    },
    {
      "index": 2,
      "line_start": 124,
      "line_end": 124,
      "kind": "inline($ $)",
      "env": "",
      "equation": "\\Pr(\\textrm{Class}_i | \\textrm{Object})",
      "explanation": "보조\n### 예시\n$$\n\\Pr(\\textrm{Class}_i | \\textrm{Object})\n$$\n### 설명\n\\(\\Pr(\\textrm{Class}_i | \\textrm{Object})\\) 방정식은 객체가 객체일 때 특정 클래스 \\(i\\)에 속할 가능성을 나타내는 조건부 확률입니다. \\(\\Pr(\\textrm{Class}_i | \\textrm{Object})\\) 표기법은 객체가 객체일 때 클래스 \\(i\\)에 속할 확률을 의미합니다. 이는 머신 러닝과 패턴 인식에서 기본 개념으로, 입력값이 주어졌을 때 클래스 레이블의 확률을 추정해야 하는 경우가 많습니다.\n\n### 결론\n이 방정식의 핵심 목적은 주어진 객체가 특정 클래스에 속할 가능성을 측정하는 것입니다. 이는 이미지 분석, 자연어 처리, 머신 러닝 모델과 같은 다양한 응용 분야에서 유용합니다. 이 확률을 계산함으로써 객체의 특징을 기반으로 객체의 클래스를 예측할 수 있습니다.\n\n⟦수학0⟧"
    },
    {
      "index": 3,
      "line_start": 127,
      "line_end": 130,
      "kind": "env",
      "env": "equation",
      "equation": "\\scriptsize\n\\Pr(\\textrm{Class}_i | \\textrm{Object}) * \\Pr(\\textrm{Object}) * \\textrm{IOU}_{\\textrm{pred}}^{\\textrm{truth}} = \\Pr(\\textrm{Class}_i)*\\textrm{IOU}_{\\textrm{pred}}^{\\textrm{truth}}",
      "explanation": "assistant\n### 예제\n$$\n\\scriptsize\n\\Pr(\\textrm{Class}_i | \\textrm{Object}) * \\Pr(\\textrm{Object}) * \\textrm{IOU}_{\\textrm{pred}}^{\\textrm{truth}} = \\Pr(\\textrm{Class}_i)*\\textrm{IOU}_{\\textrm{pred}}^{\\textrm{truth}}\n$$\n### 설명\n방정식을 단계별로 분석해 보겠습니다.\n\n1. **Object가 주어졌을 때 클래스 \\(i\\)의 확률**: \\(\\Pr(\\textrm{Class}_i | \\textrm{Object})\\)은 객체가 클래스 \\(i\\)에 속할 확률을 나타냅니다.\n\n2. **Object의 확률**: \\(\\Pr(\\textrm{Object})\\)는 객체가 도메인 내의 모든 객체일 확률을 나타냅니다.\n\n3. **推 (hypothesis)의 초기 확률**: 객체가 실제 클래스일 초기 확률을 나타냅니다.\n\n4. **방정식의 오른쪽**: 객체가 클래스 \\(i\\)에 속할 확률과 객체가 실제 클래스일 초기 확률의 곱을 나타냅니다.\n\n### 결론\n이 방정식은 객체가 객체라는 가정 하에 클래스 \\(i\\)에 속할 확률과 객체가 임의의 객체일 확률의 곱은 객체가 클래스 \\(i\\)에 속할 확률과 객체가 참 클래스일 초기 확률의 곱과 같음을 나타냅니다. 이 방정식은 확률론의 기본 원리인 전체 확률 법칙을 나타낸 것입니다."
    },
    {
      "index": 4,
      "line_start": 179,
      "line_end": 185,
      "kind": "env",
      "env": "equation",
      "equation": "\\phi(x) =\n\\begin{cases}\n    x, & \\text{if } x > 0\\\\\n    0.1x, & \\text{otherwise}\n    \\end{cases}",
      "explanation": "assistant\n### 예제\n$$\n\\phi(x) =\n\\begin{cases}\n    x, & \\text{if } x > 0\\\\\n    0.1x, & \\text{otherwise}\n    \\end{cases}\n$$\n### 설명\n\\(\\phi(x)\\) 함수는 입력 \\(x\\)에 따라 다른 값을 출력하는 구간 함수입니다. 구체적으로는 다음과 같습니다.\n- \\(x\\)가 0보다 크면 \\(\\phi(x)\\)는 \\(x\\)를 반환합니다.\n- \\(x\\)가 0보다 작거나 같으면 \\(\\phi(x)\\)는 \\(0.1x\\)를 반환합니다.\n\n### 결론\n함수 \\(\\phi(x)\\)는 \\(x\\)가 양수가 아닐 때 입력 \\(x\\)를 0.1배로 스케일링하고, \\(x\\)가 양수일 때 \\(x\\)를 반환하는 간단한 임계값 함수입니다. 이 함수는 양수 값에는 선형 관계가 존재하고 양수 값에는 다른 관계인 (in this case, a scaled linear relationship)가 존재하는 다양한 응용 프로그램에서 유용하게 사용할 수 있습니다.\n\n⟦수학0⟧"
    },
    {
      "index": 5,
      "line_start": 198,
      "line_end": 246,
      "kind": "env",
      "env": "multline",
      "equation": "\\lambda_\\textbf{coord}\n\\sum_{i = 0}^{S^2}\n    \\sum_{j = 0}^{B}\n     \\mathlarger{\\mathbbm{1}}_{ij}^{\\text{obj}}\n            \\left[\n            \\left(\n                x_i - \\hat{x}_i\n            \\right)^2 +\n            \\left(\n                y_i - \\hat{y}_i\n            \\right)^2\n            \\right]\n\\\\\n+ \\lambda_\\textbf{coord} \n\\sum_{i = 0}^{S^2}\n    \\sum_{j = 0}^{B}\n         \\mathlarger{\\mathbbm{1}}_{ij}^{\\text{obj}}\n         \\left[\n        \\left(\n            \\sqrt{w_i} - \\sqrt{\\hat{w}_i}\n        \\right)^2 +\n        \\left(\n            \\sqrt{h_i} - \\sqrt{\\hat{h}_i}\n        \\right)^2\n        \\right]\n\\\\\n+ \\sum_{i = 0}^{S^2}\n    \\sum_{j = 0}^{B}\n        \\mathlarger{\\mathbbm{1}}_{ij}^{\\text{obj}}\n        \\left(\n            C_i - \\hat{C}_i\n        \\right)^2\n\\\\\n+ \\lambda_\\textrm{noobj}\n\\sum_{i = 0}^{S^2}\n    \\sum_{j = 0}^{B}\n    \\mathlarger{\\mathbbm{1}}_{ij}^{\\text{noobj}}\n        \\left(\n            C_i - \\hat{C}_i\n        \\right)^2\n\\\\\n+ \\sum_{i = 0}^{S^2}\n\\mathlarger{\\mathbbm{1}}_i^{\\text{obj}}\n    \\sum_{c \\in \\textrm{classes}}\n        \\left(\n            p_i(c) - \\hat{p}_i(c)\n        \\right)^2",
      "explanation": "assistant\n### 예제\n$$\n\\lambda_\\textbf{coord}\n\\sum_{i = 0}^{S^2}\n    \\sum_{j = 0}^{B}\n     \\mathlarger{\\mathbbm{1}}_{ij}^{\\text{obj}}\n            \\left[\n            \\left(\n                x_i - \\hat{x}_i\n            \\right)^2 +\n            \\left(\n                y_i - \\hat{y}_i\n            \\right)^2\n            \\right]\n\\\\\n+ \\lambda_\\textbf{coord} \n\\sum_{i = 0}^{S^2}\n    \\sum_{j = 0}^{B}\n         \\mathlarger{\\mathbbm{1}}_{ij}^{\\text{obj}}\n         \\left[\n        \\left(\n            \\sqrt{w_i} - \\sqrt{\\hat{w}_i}\n        \\right)^2 +\n        \\left(\n            \\sqrt{h_i} - \\sqrt{\\hat{h}_i}\n        \\right)^2\n        \\right]\n\\\\\n+ \\sum_{i = 0}^{S^2}\n    \\sum_{j = 0}^{B}\n        \\mathlarger{\\mathbbm{1}}_{ij}^{\\text{obj}}\n        \\left(\n            C_i - \\hat{C}_i\n        \\right)^2\n\\\\\n+ \\lambda_\\textrm{noobj}\n\\sum_{i = 0}^{S^2}\n    \\sum_{j = 0}^{B}\n    \\mathlarger{\\mathbbm{1}}_{ij}^{\\text{noobj}}\n        \\left(\n            C_i - \\hat{C}_i\n        \\right)^2\n\\\\\n+ \\sum_{i = 0}^{S^2}\n\\mathlarger{\\mathbbm{1}}_i^{\\text{obj}}\n    \\sum_{c \\in \\textrm{classes}}\n        \\left(\n            p_i(c) - \\hat{p}_i(c)\n        \\right)^2\n$$\n### 설명\n방정식은 여러 합과 지표 함수로 구성되어 있습니다. 단계별로 분석해 보겠습니다.\n\n1. **첫 번째 합산:**\n\\[\n   \\lambda_\\textbf{coord} \\sum_{i = 0}^{S^2} \\sum_{j = 0}^{B} \\mathlarger{\\mathbbm{1}}_{ij}^{\\text{Obj}} \\left[ \\left( x_i - \\hat{x}_i \\right)^2 + \\left( y_i - \\hat{y}_i \\right)^2 \\right]\n   \\]\n- \\(\\lambda_\\textbf{coord}\\)은 가중치 인자입니다.\n- 이중 합산은 \\(i\\)를 0에서 \\(S^2\\)까지, \\(j\\)를 0에서 \\(B\\)까지 반복합니다.\n- \\(\\mathlarger{\\mathbbm{1}}_{ij}^{\\text{ objective}}\\)은 \\((i, j)\\) 쌍이 목적 좌표이면 1이고, 그렇지 않으면 0인 지시 함수입니다.\n- 합산 내의 항은 실제 좌표 \\((x_i, y_i)\\)와 예측 좌표 \\((\\hat{x}_i, \\hat{y}_i)\\) 사이의 제곱 오차입니다.\n\n2. **두 번째 합:**\n\\[\n   \\lambda_\\textbf{coord} \\sum_{i = 0}^{S^2} \\sum_{j = 0}^{B} \\mathlarger{\\mathbbm{1}}_{ij}^{\\text{ objective}} \\left[ \\left( \\sqrt{w_i} - \\sqrt{\\hat{w}_i} \\right)^2 + \\left( \\sqrt{h_i} - \\sqrt{\\hat{h}_i} \\right)^2 \\right]\n   \\]\n- 이 항은 첫 번째 항과 유사하지만 좌표 대신 가중치 \\(w_i\\)와 \\(h_i\\)의 제곱근을 사용합니다.\n- 지시 함수 \\(\\mathlarger{\\mathbbm{1}}_{ij}^{\\text{ objective}}\\)는 동일합니다."
    }
  ]
}