# -*- coding: utf-8 -*-
"""
LaTeX ìˆ˜ì‹ í•´ì„¤ API (FastAPI)

ì‹¤í–‰ ë°©ë²•
- ê°œë°œ ëª¨ë“œ(í•« ë¦¬ë¡œë“œ): uvicorn --reload app:app
- í”„ë¡œë•ì…˜ ëª¨ë“œ(ê¶Œì¥):   uvicorn app:app

ì œê³µ ì—”ë“œí¬ì¸íŠ¸
- GET  /health
- GET  /count/{file_path:path}
- POST /count
- GET  /math/{file_path:path}
- POST /math

ì£¼ì˜/íŠ¹ì§•
- stdout ë¼ì¸ ë²„í¼ë§
- pad_token ë³´ì • + attention_mask ëª…ì‹œ ì „ë‹¬
- LLM í”„ë¡¬í”„íŠ¸(ê°œìš”/í•´ì„¤)ëŠ” ì˜ì–´
- (ì¶”ê°€) googletransë¡œ í•œêµ­ì–´ ë²ˆì—­ë³¸ JSON/TeXë„ ìƒì„±
"""

# === ì…€ 1: í™˜ê²½ ì¤€ë¹„ & ëª¨ë¸ ë¡œë“œ ===
import os, sys, json, re, textwrap, datetime, torch
from typing import List, Dict, Tuple
from pathlib import Path
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# (ì¶”ê°€) êµ¬ê¸€ ë²ˆì—­ê¸°
try:
    # pip install googletrans==4.0.0rc1 ê¶Œì¥
    from googletrans import Translator
    _GT_AVAILABLE = True
except Exception:
    Translator = None
    _GT_AVAILABLE = False

# [ê¶Œì¥] ì½˜ì†” ì¶œë ¥ ì¦‰ì‹œí™”
try:
    sys.stdout.reconfigure(line_buffering=True)
except Exception:
    pass

VERSION = "POLO-Math-API v5 (EN->KO translate + math-protect)"
print(VERSION, flush=True)

# ----- ê²½ë¡œ ì„¤ì • -----
INPUT_TEX_PATH = r"C:\\POLO\\polo-system\\models\\math\\yolo.tex"
OUT_DIR        = "C:/POLO/polo-system/models/math/_build"
os.makedirs(OUT_DIR, exist_ok=True)

# ----- ëª¨ë¸/í† í¬ë‚˜ì´ì € ì„¤ì • -----
MODEL_ID = "Qwen/Qwen2.5-Math-1.5B-Instruct"
USE_4BIT = False
DEVICE   = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Python: {sys.version.split()[0]}", flush=True)
print(f"PyTorch: {torch.__version__}", flush=True)
print(f"CUDA available: {torch.cuda.is_available()}", flush=True)
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}", flush=True)
print(f"Device selected: {DEVICE}", flush=True)

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)

    PAD_ADDED = False
    if tokenizer.pad_token_id is None or tokenizer.pad_token_id == tokenizer.eos_token_id:
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        PAD_ADDED = True

    bnb_config = None
    if USE_4BIT:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.float16
        )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map="auto",
        torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
        quantization_config=bnb_config,
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )

    if PAD_ADDED:
        model.resize_token_embeddings(len(tokenizer))

    GEN_KW = dict(max_new_tokens=512, temperature=0.2, top_p=0.9, do_sample=True)
    print("Model & tokenizer loaded.", flush=True)

except Exception as e:
    tokenizer = None
    model = None
    GEN_KW = {}
    print("[Model Load Error]", e, flush=True)

# (ì¶”ê°€) ë²ˆì—­ê¸° ì¸ìŠ¤í„´ìŠ¤
translator = None
if _GT_AVAILABLE:
    try:
        # googletransëŠ” ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ ì´ìŠˆê°€ ìˆì„ ìˆ˜ ìˆì–´ ë³´ì¡° URLì„ ì§€ì •
        translator = Translator(service_urls=["translate.googleapis.com", "translate.google.com"])
        print("Google Translator initialized.", flush=True)
    except Exception as e:
        print("[Translator Init Error]", e, flush=True)
        translator = None

# === ê³µí†µ ìœ í‹¸: ë¼ì¸ ì˜¤í”„ì…‹ ì¸ë±ìŠ¤ ===
def make_line_offsets(text: str) -> List[int]:
    lines = text.splitlines()
    offsets, pos = [], 0
    for ln in lines:
        offsets.append(pos)
        pos += len(ln) + 1
    return offsets

def build_pos_to_line(offsets: List[int]):
    def pos_to_line(p: int) -> int:
        lo, hi = 0, len(offsets)-1
        while lo <= hi:
            mid = (lo+hi)//2
            if offsets[mid] <= p:
                lo = mid + 1
            else:
                hi = mid - 1
        return hi + 1
    return pos_to_line

# === ì…€ 2: LaTeX ìˆ˜ì‹ ì¶”ì¶œ ===
def extract_equations(tex: str, pos_to_line) -> List[Dict]:
    matches: List[Dict] = []

    def add(kind, start, end, body, env=""):
        matches.append({
            "kind": kind, "env": env, "start": start, "end": end,
            "line_start": pos_to_line(start), "line_end": pos_to_line(end),
            "body": body.strip()
        })

    for m in re.finditer(r"\$\$(.+?)\$\$", tex, flags=re.DOTALL):
        add("display($$ $$)", m.start(), m.end(), m.group(1))

    for m in re.finditer(r"\\\[(.+?)\\\]", tex, flags=re.DOTALL):
        add("display(\\[ \\])", m.start(), m.end(), m.group(1))

    for m in re.finditer(r"\\\((.+?)\\\)", tex, flags=re.DOTALL):
        add("inline(\\( \\))", m.start(), m.end(), m.group(1))

    for m in re.finditer(r"(?<!\$)\$(?!\$)(.+?)(?<!\$)\$(?!\$)", tex, flags=re.DOTALL):
        add("inline($ $)", m.start(), m.end(), m.group(1))

    envs = ["equation","equation*","align","align*","multline","multline*",
            "gather","gather*","flalign","flalign*","eqnarray","eqnarray*","split"]
    for env in envs:
        pattern = rf"\\begin{{{re.escape(env)}}}(.+?)\\end{{{re.escape(env)}}}"
        for m in re.finditer(pattern, tex, flags=re.DOTALL):
            add("env", m.start(), m.end(), m.group(1), env=env)

    uniq = {}
    for it in matches:
        key = (it["start"], it["end"])
        if key not in uniq:
            uniq[key] = it

    out = list(uniq.values())
    out.sort(key=lambda x: x["start"])
    return out

# === ì…€ 3: ë‚œì´ë„ íœ´ë¦¬ìŠ¤í‹± ===
ADV_TOKENS = [
    r"\\sum", r"\\prod", r"\\int", r"\\lim", r"\\nabla", r"\\partial",
    r"\\mathbb", r"\\mathcal", r"\\mathbf", r"\\boldsymbol",
    r"\\argmax", r"\\argmin", r"\\operatorname", r"\\mathrm\{KL\}",
    r"\\mathbb\{E\}", r"\\Pr", r"\\sigma", r"\\mu", r"\\Sigma", r"\\theta",
    r"\\frac\{[^{}]*\{[^{}]*\}[^{}]*\}",
    r"\\hat\{", r"\\tilde\{", r"\\bar\{", r"\\widehat\{", r"\\widetilde\{",
    r"\\sqrt\{[^{}]*\{",
    r"\\left", r"\\right",
    r"\\in", r"\\subset", r"\\forall", r"\\exists",
    r"\\cdot", r"\\times", r"\\otimes",
    r"IoU", r"\\log", r"\\exp",
    r"\\mathbb\{R\}", r"\\mathbb\{N\}", r"\\mathbb\{Z\}",
    r"\\Delta", r"\\delta", r"\\epsilon", r"\\varepsilon",
]
ADV_RE = re.compile("|".join(ADV_TOKENS))

def count_subscripts(expr: str) -> int:
    return len(re.findall(r"_[a-zA-Z0-9{\\]", expr))

def is_advanced(eq: str) -> bool:
    if ADV_RE.search(eq):
        return True
    if len(eq) > 40 and count_subscripts(eq) >= 2:
        return True
    if "\n" in eq and len(eq) > 30:
        return True
    return False

# === ì…€ 4: ê°œìš” ìƒì„± ===
def take_slices(text: str, head_chars=4000, mid_chars=2000, tail_chars=4000):
    n = len(text)
    head = text[:min(head_chars, n)]
    mid_start = max((n // 2) - (mid_chars // 2), 0)
    mid = text[mid_start: mid_start + min(mid_chars, n)]
    tail = text[max(0, n - tail_chars):]
    return head, mid, tail

def _generate_with_mask_from_messages(messages: List[Dict]) -> str:
    inputs = tokenizer.apply_chat_template(
        messages, add_generation_prompt=True, return_tensors="pt", padding=True
    )
    attention_mask = (inputs != tokenizer.pad_token_id).long()
    with torch.no_grad():
        out = model.generate(
            input_ids=inputs.to(model.device),
            attention_mask=attention_mask.to(model.device),
            **GEN_KW
        )
    return tokenizer.decode(out[0], skip_special_tokens=True)

def chat_overview(prompt: str) -> str:
    if tokenizer is None or model is None:
        raise RuntimeError("Model is not loaded.")
    messages = [
        {"role": "system", "content":
         "You are a clear, concise technical writer who summarizes LaTeX-based AI papers for a general technical audience."},
        {"role": "user", "content": prompt}
    ]
    text = _generate_with_mask_from_messages(messages)
    return text.split(messages[-1]["content"])[-1].strip()

# === ì…€ 5: ìˆ˜ì‹ í•´ì„¤ ìƒì„± ===
EXPLAIN_SYSTEM = (
    "You are a teacher who explains math/AI research equations in clear, simple English. "
    "Always be precise, polite, and easy to understand."
)
EXPLAIN_TEMPLATE = """Please explain the following equation so that it can be understood by someone at least at a middle school level.
Follow this exact order in your output: Example â†’ Explanation â†’ Conclusion

- Example: Show the equation exactly as LaTeX in a single block (do not modify or add anything).
- Explanation: Provide bullet points explaining the meaning of symbols (âˆ‘, ğŸ™, ^, _, âˆš, \\, etc.) and the role of each term, in a clear and concise way.
- Conclusion: Summarize in one sentence the core purpose of this equation in the context of the paper (e.g., loss composition, normalization, coordinate error, probability/log-likelihood, etc.).
- (Important) Do not change the symbols or the order of the equation, and do not invent new symbols.
- (Important) Write only in English.

[Equation]
{EQUATION}
"""

def explain_equation_with_llm(eq_latex: str) -> str:
    if tokenizer is None or model is None:
        raise RuntimeError("Model is not loaded.")
    messages = [
        {"role": "system", "content": EXPLAIN_SYSTEM},
        {"role": "user",   "content": EXPLAIN_TEMPLATE.format(EQUATION=eq_latex)}
    ]
    text = _generate_with_mask_from_messages(messages)
    return text.split(messages[-1]["content"])[-1].strip()

# === ì…€ 6: LaTeX ë¦¬í¬íŠ¸(.tex) ìƒì„± ===
def latex_escape_verbatim(s: str) -> str:
    s = s.replace("\\", r"\\")
    s = s.replace("#", r"\#").replace("$", r"\$")
    s = s.replace("%", r"\%").replace("&", r"\&")
    s = s.replace("_", r"\_").replace("{", r"\{").replace("}", r"\}")
    s = s.replace("^", r"\^{}").replace("~", r"\~{}")
    return s

def build_report(overview: str, items: List[Dict]) -> str:
    header = (r"""\\documentclass[11pt]{article}
\\usepackage[margin=1in]{geometry}
\\usepackage{amsmath, amssymb, amsfonts}
\\usepackage{hyperref}
\\usepackage{kotex}
\\setlength{\\parskip}{6pt}
\\setlength{\\parindent}{0pt}
\\title{LaTeX Equation Explanation Report (Middle-School Level+)}
\\author{Automatic Pipeline}
\\date{""" + datetime.date.today().isoformat() + r"""}
\\begin{document}
\\maketitle
\\tableofcontents
\\newpage
""")
    parts = [header]
    parts.append(r"\\section*{Document Overview}")
    parts.append(latex_escape_verbatim(overview))
    parts.append("\n\\newpage\n")

    for it in items:
        title = f"Lines {it['line_start']}â€“{it['line_end']} / {it['kind']} {('['+it['env']+']') if it['env'] else ''}"
        parts.append(f"\\section*{{{latex_escape_verbatim(title)}}}")
        # ì„¤ëª… í…ìŠ¤íŠ¸ëŠ” (ì˜ë„ì ìœ¼ë¡œ) verbatim ì´ìŠ¤ì¼€ì´í”„í•˜ì§€ ì•ŠìŒ:
        # - ìˆ˜ì‹ ë¸”ë¡(ì˜ˆ: \[...\])ì´ ê·¸ëŒ€ë¡œ LaTeXë¡œ ë Œë”ë˜ë„ë¡
        parts.append(it["explanation"])
        parts.append("\n")

    parts.append("\\end{document}\n")
    return "\n".join(parts)

# (ì¶”ê°€) í•œêµ­ì–´ ë¦¬í¬íŠ¸ ë¹Œë”
def build_report_ko(overview_ko: str, items_ko: List[Dict]) -> str:
    header = (r"""\\documentclass[11pt]{article}
\\usepackage[margin=1in]{geometry}
\\usepackage{amsmath, amssymb, amsfonts}
\\usepackage{hyperref}
\\usepackage{kotex}
\\setlength{\\parskip}{6pt}
\\setlength{\\parindent}{0pt}
\\title{LaTeX ìˆ˜ì‹ í•´ì„¤ ë¦¬í¬íŠ¸ (ì¤‘í•™ìƒ ì´ìƒ)}
\\author{ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸}
\\date{""" + datetime.date.today().isoformat() + r"""}
\\begin{document}
\\maketitle
\\tableofcontents
\\newpage
""")
    parts = [header]
    parts.append(r"\\section*{ë¬¸ì„œ ê°œìš”}")
    parts.append(latex_escape_verbatim(overview_ko))
    parts.append("\n\\newpage\n")

    for it in items_ko:
        title = f"ë¼ì¸ {it['line_start']}â€“{it['line_end']} / {it['kind']} {('['+it['env']+']') if it['env'] else ''}"
        parts.append(f"\\section*{{{latex_escape_verbatim(title)}}}")
        parts.append(it["explanation"])  # ìˆ˜ì‹ì€ ë³´í˜¸/ë³µì›ë˜ì–´ LaTeX ìœ ì§€
        parts.append("\n")

    parts.append("\\end{document}\n")
    return "\n".join(parts)

# === (ì¶”ê°€) ë²ˆì—­ ìœ í‹¸: ìˆ˜ì‹ ë³´í˜¸/ë³µì› + ë²ˆì—­ ===
# === (ìˆ˜ì •) ë²ˆì—­ ìœ í‹¸: ìˆ˜ì‹ ë³´í˜¸/ë³µì› + ë²ˆì—­ ===
_MATH_ENV_NAMES = r"(?:equation|align|gather|multline|eqnarray|cases|split)\*?"
_MATH_PATTERN = re.compile(
    r"(?P<D2>\${2}[\s\S]*?\${2})"           # $$ ... $$
    r"|(?P<D1>(?<!\\)\$[\s\S]*?(?<!\\)\$)"  # $ ... $ (ì´ìŠ¤ì¼€ì´í”„ ì œì™¸)
    r"|(?P<LB>\\\[[\s\S]*?\\\])"            # \[ ... \]
    r"|(?P<LP>\\\([\s\S]*?\\\))"            # \( ... \)
    r"|(?P<ENV>\\begin\{" + _MATH_ENV_NAMES + r"\}[\s\S]*?\\end\{" + _MATH_ENV_NAMES + r"\})",
    re.MULTILINE
)


def protect_math(text: str) -> Tuple[str, Dict[str, str]]:
    """
    ìˆ˜ì‹ ë¸”ë¡ì„ ë³´í˜¸ í† í°ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ ë²ˆì—­ ì‹œ ë³€í˜•ì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    placeholders = {}
    def _repl(m):
        key = f"âŸ¦MATH{len(placeholders)}âŸ§"
        placeholders[key] = m.group(0)
        return key
    protected = _MATH_PATTERN.sub(_repl, text)
    return protected, placeholders

def restore_math(text: str, placeholders: Dict[str, str]) -> str:
    for k, v in placeholders.items():
        text = text.replace(k, v)
    return text

def translate_text_ko(text: str) -> str:
    """
    ì˜ë¬¸ ì„¤ëª…/ê°œìš”ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
    - ìˆ˜ì‹ ë³´í˜¸ í›„ ë²ˆì—­ â†’ ë³µì›
    - googletrans ì‚¬ìš© ë¶ˆê°€ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜(ë¡œê·¸ë§Œ ì¶œë ¥)
    """
    if translator is None:
        print("[Translate] Translator unavailable; returning original.", flush=True)
        return text

    # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìª¼ê°œ ë²ˆì—­(googletrans ê¸¸ì´ ì œí•œ/ì•ˆì •ì„± ë³´ì™„)
    paras = text.split("\n\n")
    out_paras = []
    for para in paras:
        prot, ph = protect_math(para)
        try:
            t = translator.translate(prot, dest="ko").text
        except Exception as e:
            print("[Translate Error]", e, flush=True)
            t = prot  # ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€
        out_paras.append(restore_math(t, ph))
    return "\n\n".join(out_paras)

# === ë³´ì¡°: ìˆ˜ì‹ ê°œìˆ˜ë§Œ ë¹ ë¥´ê²Œ ì„¸ê¸° ===
def count_equations_only(input_tex_path: str) -> Dict[str, int]:
    p = Path(input_tex_path)
    if not p.exists():
        raise FileNotFoundError(f"Cannot find TeX file: {input_tex_path}")

    src = p.read_text(encoding="utf-8", errors="ignore")
    offsets = make_line_offsets(src)
    pos_to_line = build_pos_to_line(offsets)
    equations_all = extract_equations(src, pos_to_line)
    equations_advanced = [e for e in equations_all if is_advanced(e["body"])]

    print(f"ì´ ìˆ˜ì‹: {len(equations_all)}", flush=True)
    print(f"ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒ: {len(equations_advanced)} / {len(equations_all)}", flush=True)

    return {"ì´ ìˆ˜ì‹": len(equations_all),
            "ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒ": len(equations_advanced)}

# === ë©”ì¸ íŒŒì´í”„ë¼ì¸ ===
def run_pipeline(input_tex_path: str) -> Dict:
    p = Path(input_tex_path)
    if not p.exists():
        raise FileNotFoundError(f"Cannot find TeX file: {input_tex_path}")

    Path(OUT_DIR).mkdir(parents=True, exist_ok=True)

    # 1) íŒŒì¼ ì½ê¸°
    src = p.read_text(encoding="utf-8", errors="ignore")

    # 2) ë¼ì¸ ì¸ë±ìŠ¤
    offsets = make_line_offsets(src)
    pos_to_line = build_pos_to_line(offsets)

    # 3) ìˆ˜ì‹ ì¶”ì¶œ & ê³ ë‚œë„ ë¶„ë¥˜
    equations_all = extract_equations(src, pos_to_line)
    equations_advanced = [e for e in equations_all if is_advanced(e["body"])]

    print(f"ì´ ìˆ˜ì‹: {len(equations_all)}", flush=True)
    print(f"ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒ: {len(equations_advanced)} / {len(equations_all)}", flush=True)

    # 4) ë¬¸ì„œ ê°œìš”(ì˜ì–´)
    head, mid, tail = take_slices(src)
    overview_prompt = textwrap.dedent(f"""
    You will be given three slices of a LaTeX document (head / middle / tail).
    Please produce a concise English overview with:
    - One short paragraph describing the core topic and objective of the paper.
    - A few bullet points listing the main sections or ideas (if discernible).
    - Focus on how mathematical notation and symbols are used to support the overall flow.

    [HEAD]
    {head}

    [MIDDLE]
    {mid}

    [TAIL]
    {tail}
    """).strip()
    doc_overview = chat_overview(overview_prompt)

    # 5) ê³ ë‚œë„ ìˆ˜ì‹ í•´ì„¤(ì˜ì–´)
    explanations: List[Dict] = []
    for idx, item in enumerate(equations_advanced, start=1):
        print(f"[{idx}/{len(equations_advanced)}] ë¼ì¸ {item['line_start']}â€“{item['line_end']}", flush=True)
        exp = explain_equation_with_llm(item["body"])
        explanations.append({
            "index": idx,
            "line_start": item["line_start"],
            "line_end": item["line_end"],
            "kind": item["kind"],
            "env": item["env"],
            "equation": item["body"],
            "explanation": exp
        })

    # 6) JSON ì €ì¥ (ì˜ë¬¸)
    json_path = os.path.join(OUT_DIR, "equations_explained.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({"overview": doc_overview, "items": explanations}, f, ensure_ascii=False, indent=2)
    print(f"ì €ì¥ëœ JSON: {json_path}", flush=True)

    # 7) LaTeX ë¦¬í¬íŠ¸(.tex) ì €ì¥ (ì˜ë¬¸)
    report_tex_path = os.path.join(OUT_DIR, "yolo_math_report.tex")
    report_tex = build_report(doc_overview, explanations)
    Path(report_tex_path).write_text(report_tex, encoding="utf-8")
    print(f"ì €ì¥ëœ TeX: {report_tex_path}", flush=True)

    # 8) (ì¶”ê°€) í•œêµ­ì–´ ë²ˆì—­ë³¸ ìƒì„± ë° ì €ì¥
    # 8-1) ê°œìš” ë²ˆì—­
    overview_ko = translate_text_ko(doc_overview)

    # 8-2) ê° í•´ì„¤ ë²ˆì—­ (ìˆ˜ì‹ ë³´í˜¸)
    ko_items: List[Dict] = []
    for it in explanations:
        exp_ko = translate_text_ko(it["explanation"])
        ko_items.append({
            **{k: it[k] for k in ["index","line_start","line_end","kind","env","equation"]},
            "explanation": exp_ko
        })

    # 8-3) JSON ì €ì¥ (í•œêµ­ì–´)
    json_ko_path = os.path.join(OUT_DIR, "equations_explained.ko.json")
    with open(json_ko_path, "w", encoding="utf-8") as f:
        json.dump({"overview": overview_ko, "items": ko_items}, f, ensure_ascii=False, indent=2)
    print(f"ì €ì¥ëœ í•œêµ­ì–´ JSON: {json_ko_path}", flush=True)

    # 8-4) LaTeX ë¦¬í¬íŠ¸(.tex) ì €ì¥ (í•œêµ­ì–´)
    report_ko_tex_path = os.path.join(OUT_DIR, "yolo_math_report.ko.tex")
    report_ko_tex = build_report_ko(overview_ko, ko_items)
    Path(report_ko_tex_path).write_text(report_ko_tex, encoding="utf-8")
    print(f"ì €ì¥ëœ í•œêµ­ì–´ TeX: {report_ko_tex_path}", flush=True)

    # 9) ì²˜ë¦¬ ìš”ì•½ ë°˜í™˜
    return {
        "input": str(p),
        "counts": {
            "ì´ ìˆ˜ì‹": len(equations_all),
            "ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒ": len(equations_advanced)
        },
        "outputs": {
            "json": json_path,
            "report_tex": report_tex_path,
            "json_ko": json_ko_path,
            "report_tex_ko": report_ko_tex_path,
            "out_dir": OUT_DIR
        },
        "translate": {
            "googletrans_available": (_GT_AVAILABLE and translator is not None)
        }
    }

# === FastAPI ì•± ì •ì˜ ===
app = FastAPI(title="POLO Math Explainer API", version="1.0.0")

class MathRequest(BaseModel):
    path: str

@app.get("/health")
async def health():
    return {
        "status": "ok",
        "python": sys.version.split()[0],
        "torch": torch.__version__,
        "cuda": torch.cuda.is_available(),
        "device": DEVICE,
        "model_loaded": (tokenizer is not None and model is not None),
        "googletrans": (_GT_AVAILABLE and translator is not None)
    }

@app.get("/count/{file_path:path}")
async def count_get(file_path: str):
    try:
        return count_equations_only(file_path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {e}")

@app.post("/count")
async def count_post(req: MathRequest):
    try:
        return count_equations_only(req.path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {e}")

@app.get("/math/{file_path:path}")
async def math_get(file_path: str):
    try:
        return run_pipeline(file_path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {e}")

@app.post("/math")
async def math_post(req: MathRequest):
    try:
        return run_pipeline(req.path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {e}")

# ì§ì ‘ ì‹¤í–‰
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="127.0.0.1", port=8000, reload=True)
