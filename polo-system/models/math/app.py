# -*- coding: utf-8 -*-
"""
LaTeX ÏàòÏãù Ìï¥ÏÑ§ API (FastAPI)

Ïã§Ìñâ Î∞©Î≤ï
- Í∞úÎ∞ú Î™®Îìú(Ìï´ Î¶¨Î°úÎìú): uvicorn --reload app:app
- ÌîÑÎ°úÎçïÏÖò Î™®Îìú(Í∂åÏû•):   uvicorn app:app

Ï†úÍ≥µ ÏóîÎìúÌè¨Ïù∏Ìä∏
- GET  /health
  : ÏÑúÎ≤Ñ/Î™®Îç∏ ÏÉÅÌÉú Ï†êÍ≤ÄÏö© ÏóîÎìúÌè¨Ïù∏Ìä∏ÏûÖÎãàÎã§.

- GET  /count/{file_path:path}
  : ÌäπÏ†ï TeX ÌååÏùºÏùò ÏàòÏãùÏùÑ Ï∂îÏ∂úÌï¥ "Ï¥ù ÏàòÏãù Í∞úÏàò"ÏôÄ "Í≥†ÎÇúÎèÑ(Ï§ëÌïôÏÉù Ïù¥ÏÉÅ) ÏàòÏãù Í∞úÏàò"Îßå Îπ†Î•¥Í≤å Í≥ÑÏÇ∞Ìï©ÎãàÎã§.
    ÏΩòÏÜîÏóêÎèÑ Ï¶âÏãú Ï∂úÎ†•(printf)ÎêòÎ©∞, JSONÏúºÎ°ú Í∞úÏàòÎßå Î∞òÌôòÌï©ÎãàÎã§.

- POST /count
  : {"path": "C:\\...\\yolo.tex"} ÌòïÏãùÏùò JSONÏúºÎ°ú ÏúÑÏôÄ ÎèôÏùºÌïú Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.

- GET  /math/{file_path:path}
  : Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ(ÏàòÏãù Ï∂îÏ∂ú/Î∂ÑÎ•ò ‚Üí Î¨∏ÏÑú Í∞úÏöî(ÏòÅÏñ¥) ÏÉùÏÑ± ‚Üí Í≥†ÎÇúÎèÑ ÏàòÏãùÏóê ÎåÄÌïú Ìï¥ÏÑ§(ÏòÅÏñ¥) ÏÉùÏÑ± ‚Üí JSON/TeX Ï†ÄÏû•)

- POST /math
  : {"path": "C:\\...\\yolo.tex"} ÌòïÏãùÏùò JSONÏúºÎ°ú ÏúÑÏôÄ ÎèôÏùºÌïú Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.

Ï£ºÏùò/ÌäπÏßï
- ÏΩòÏÜî Ï∂úÎ†•Ïù¥ ÏßÄÏó∞ÎêòÏßÄ ÏïäÎèÑÎ°ù stdout ÎùºÏù∏ Î≤ÑÌçºÎßÅ + print(..., flush=True)Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.
- ÏùºÎ∂Ä Î™®Îç∏ÏóêÏÑú pad_tokenÍ≥º eos_tokenÏù¥ Í∞ôÏùÑ Îïå Îú®Îäî Í≤ΩÍ≥†Î•º ÌîºÌïòÍ∏∞ ÏúÑÌï¥,
  pad ÌÜ†ÌÅ∞Ïù¥ ÏóÜÍ±∞ÎÇò eosÏôÄ Í∞ôÏúºÎ©¥ [PAD] ÌÜ†ÌÅ∞ÏùÑ Ï∂îÍ∞ÄÌïòÍ≥†, generate Ïãú attention_maskÎ•º Î™ÖÏãúÏ†ÅÏúºÎ°ú Ï†ÑÎã¨Ìï©ÎãàÎã§.
- LLM ÌîÑÎ°¨ÌîÑÌä∏(Î¨∏ÏÑú Í∞úÏöî/ÏàòÏãù Ìï¥ÏÑ§)Îäî ÏöîÏ≤≠Ïóê Îî∞Îùº ÏòÅÏñ¥Î°ú ÏûëÏÑ±ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.
"""

# === ÏÖÄ 1: ÌôòÍ≤Ω Ï§ÄÎπÑ & Î™®Îç∏ Î°úÎìú ===
import os, sys, json, re, textwrap, datetime, torch
from typing import List, Dict
from pathlib import Path
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
# dotenv ÏóÜÏù¥ Í∞ÑÎã®ÌïòÍ≤å ÌôòÍ≤ΩÎ≥ÄÏàò Î°úÎìú
def load_env_file(env_path):
    """Í∞ÑÎã®Ìïú .env ÌååÏùº Î°úÎìú"""
    if not os.path.exists(env_path):
        return
    try:
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()
    except Exception as e:
        print(f"Warning: Could not load .env file: {e}")

# [Í∂åÏû•] ÏΩòÏÜî Ï∂úÎ†•Ïù¥ Î∞îÎ°ú Î≥¥Ïù¥ÎèÑÎ°ù stdoutÏùÑ Ï§Ñ Îã®ÏúÑÎ°ú Î≤ÑÌçºÎßÅÌï©ÎãàÎã§.
try:
    sys.stdout.reconfigure(line_buffering=True)
except Exception:
    pass

VERSION = "POLO-Math-API v4 (EN-prompts + flush + mask + pad)"
print(VERSION, flush=True)

# ----- Í≤ΩÎ°ú ÏÑ§Ï†ï -----
INPUT_TEX_PATH = r"C:\\POLO\\polo-system\\models\\math\\yolo.tex"
OUT_DIR        = "C:/POLO/polo-system/models/math/_build"
os.makedirs(OUT_DIR, exist_ok=True)

# ----- Î™®Îç∏/ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏÑ§Ï†ï -----
MODEL_ID = "Qwen/Qwen2.5-Math-1.5B-Instruct"
USE_4BIT = False
DEVICE   = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Python: {sys.version.split()[0]}", flush=True)
print(f"PyTorch: {torch.__version__}", flush=True)
print(f"CUDA available: {torch.cuda.is_available()}", flush=True)
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    print(f"‚úÖ GPU ÏÇ¨Ïö© Í∞ÄÎä•: {gpu_name}", flush=True)
    print(f"üîß ÎîîÎ∞îÏù¥Ïä§: {DEVICE}, Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ: float16", flush=True)
else:
    print("‚ö†Ô∏è GPUÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§. CPU Î™®ÎìúÎ°ú Ïã§ÌñâÌï©ÎãàÎã§.", flush=True)
    print(f"üîß ÎîîÎ∞îÏù¥Ïä§: {DEVICE}, Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ: float32", flush=True)

# --- Í∞ÑÎã®Ìïú .env Î°úÎìú ---
ROOT_ENV = Path(__file__).resolve().parents[2] / ".env"
load_env_file(str(ROOT_ENV))
print(f"[env] .env loaded from: {ROOT_ENV}", flush=True)

SAFE_CACHE_DIR = Path(__file__).resolve().parent / "hf_cache"
SAFE_CACHE_DIR.mkdir(parents=True, exist_ok=True)

def _force_safe_hf_cache():
    for k in ("HF_HOME", "TRANSFORMERS_CACHE", "HF_DATASETS_CACHE", "HUGGINGFACE_HUB_CACHE"):
        os.environ[k] = str(SAFE_CACHE_DIR)
    print(f"[hf_cache] forced ‚Üí {SAFE_CACHE_DIR}", flush=True)

_force_safe_hf_cache()
# Hugging Face ÌÜ†ÌÅ∞ ÏÑ§Ï†ï (Ïó¨Îü¨ Í∞ÄÎä•Ìïú Ïù¥Î¶ÑÏúºÎ°ú ÏãúÎèÑ)
HF_TOKEN = os.getenv("ÌóàÍπÖÌéòÏù¥Ïä§ ÌÜ†ÌÅ∞") or os.getenv("HUGGINGFACE_TOKEN") or os.getenv("HF_TOKEN")
print(f"HF_TOKEN={'ÏÑ§Ï†ïÎê®' if HF_TOKEN else 'ÏóÜÏùå'} (ÌôòÍ≤ΩÎ≥ÄÏàò: 'ÌóàÍπÖÌéòÏù¥Ïä§ ÌÜ†ÌÅ∞' ÎòêÎäî 'HUGGINGFACE_TOKEN')", flush=True)

def load_model():
    """Î™®Îç∏ Î°úÎìú Ìï®Ïàò"""
    global tokenizer, model, GEN_KW
    
    try:
        print(f"üîÑ Math Î™®Îç∏ Î°úÎî© ÏãúÏûë: {MODEL_ID}", flush=True)
        print(f"HF_HOME={os.getenv('HF_HOME')}", flush=True)
        print(f"HF_TOKEN={'ÏÑ§Ï†ïÎê®' if HF_TOKEN else 'ÏóÜÏùå'} (ÌôòÍ≤ΩÎ≥ÄÏàò: 'ÌóàÍπÖÌéòÏù¥Ïä§ ÌÜ†ÌÅ∞')", flush=True)
        
        # 1) ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä (Ï∫êÏãú/ÌÜ†ÌÅ∞ Î™ÖÏãú)
        print("üìù ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎî© Ï§ë...", flush=True)
        print(f"üìù MODEL_ID: {MODEL_ID}", flush=True)
        print(f"üìù CACHE_DIR: {SAFE_CACHE_DIR}", flush=True)
        print(f"üìù HF_TOKEN: {'ÏÑ§Ï†ïÎê®' if HF_TOKEN else 'ÏóÜÏùå'} (ÌôòÍ≤ΩÎ≥ÄÏàò: 'ÌóàÍπÖÌéòÏù¥Ïä§ ÌÜ†ÌÅ∞')", flush=True)
        
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_ID,
            trust_remote_code=True,
            token=HF_TOKEN,
            cache_dir=str(SAFE_CACHE_DIR),
        )
        print("‚úÖ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎî© ÏôÑÎ£å", flush=True)

        # 2) pad ÌÜ†ÌÅ∞ Î≥¥Ï†ï
        PAD_ADDED = False
        if tokenizer.pad_token_id is None or tokenizer.pad_token_id == tokenizer.eos_token_id:
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            PAD_ADDED = True
            print("üîß PAD ÌÜ†ÌÅ∞ Ï∂îÍ∞ÄÎê®", flush=True)

        # 3) Î™®Îç∏ Î°úÎìú (ÌïÑÏöî Ïãú 4bit)
        print("üß† Î™®Îç∏ Î°úÎî© Ï§ë...", flush=True)
        print(f"üß† DEVICE: {DEVICE}", flush=True)
        print(f"üß† USE_4BIT: {USE_4BIT}", flush=True)
        
        bnb_config = None
        if USE_4BIT:
            print("üîß 4bit ÏñëÏûêÌôî ÏÑ§Ï†ï Ï†ÅÏö©", flush=True)
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.float16 if DEVICE == "cuda" else torch.float32
            )

        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            device_map="auto",
            torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
            quantization_config=bnb_config,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            token=HF_TOKEN,
            cache_dir=str(SAFE_CACHE_DIR),
        )

        if PAD_ADDED:
            model.resize_token_embeddings(len(tokenizer))
            print("üîß ÌÜ†ÌÅ∞ ÏûÑÎ≤†Îî© ÌÅ¨Í∏∞ Ï°∞Ï†ïÎê®", flush=True)

        GEN_KW = dict(
            max_new_tokens=512,
            temperature=0.2,
            top_p=0.9,
            do_sample=True
        )
        print("‚úÖ Math Î™®Îç∏ Î°úÎî© ÏôÑÎ£å", flush=True)
        print(f"‚úÖ Î™®Îç∏ ÎîîÎ∞îÏù¥Ïä§: {next(model.parameters()).device}", flush=True)
        print(f"‚úÖ Î™®Îç∏ dtype: {next(model.parameters()).dtype}", flush=True)
        return True

    except Exception as e:
        print(f"‚ùå Math Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}", flush=True)
        print(f"‚ùå ÏóêÎü¨ ÌÉÄÏûÖ: {type(e).__name__}", flush=True)
        print(f"‚ùå ÏóêÎü¨ ÏÉÅÏÑ∏: {str(e)}", flush=True)
        import traceback
        traceback.print_exc()
        tokenizer = None
        model = None
        GEN_KW = {}
        return False

# Î™®Îç∏ Î°úÎìú ÏãúÎèÑ
print("üöÄ Math Î™®Îç∏ Î°úÎî© ÏãúÏûë...", flush=True)
model_loaded = load_model()
if not model_loaded:
    print("‚ö†Ô∏è Î™®Îç∏ Î°úÎî© Ïã§Ìå® - ÏÑúÎ≤ÑÎäî ÏãúÏûëÎêòÏßÄÎßå Í∏∞Îä•Ïù¥ Ï†úÌïúÎê©ÎãàÎã§.", flush=True)
    print("‚ö†Ô∏è Í∞ÄÎä•Ìïú ÏõêÏù∏:", flush=True)
    print("  - 'ÌóàÍπÖÌéòÏù¥Ïä§ ÌÜ†ÌÅ∞' ÌôòÍ≤ΩÎ≥ÄÏàòÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏùå", flush=True)
    print("  - Ïù∏ÌÑ∞ÎÑ∑ Ïó∞Í≤∞ Î¨∏Ï†ú", flush=True)
    print("  - Î™®Îç∏ Îã§Ïö¥Î°úÎìú Ïã§Ìå®", flush=True)
    print("  - Î©îÎ™®Î¶¨ Î∂ÄÏ°±", flush=True)
    print("  - CUDA/GPU Î¨∏Ï†ú", flush=True)
else:
    print("üéâ Math Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ!", flush=True)


# === Í≥µÌÜµ Ïú†Ìã∏: ÎùºÏù∏ Ïò§ÌîÑÏÖã Ïù∏Îç±Ïä§ ===
def make_line_offsets(text: str) -> List[int]:
    lines = text.splitlines()
    offsets, pos = [], 0
    for ln in lines:
        offsets.append(pos)
        pos += len(ln) + 1  # '\n' Ìè¨Ìï®
    return offsets

def build_pos_to_line(offsets: List[int]):
    def pos_to_line(p: int) -> int:
        lo, hi = 0, len(offsets)-1
        while lo <= hi:
            mid = (lo+hi)//2
            if offsets[mid] <= p:
                lo = mid + 1
            else:
                hi = mid - 1
        return hi + 1  # 1-based
    return pos_to_line


# === ÏÖÄ 2: LaTeX ÏàòÏãù Ï∂îÏ∂ú ===
def extract_equations(tex: str, pos_to_line) -> List[Dict]:
    matches: List[Dict] = []

    def add(kind, start, end, body, env=""):
        matches.append({
            "kind": kind, "env": env, "start": start, "end": end,
            "line_start": pos_to_line(start), "line_end": pos_to_line(end),
            "body": body.strip()
        })

    # $$ ... $$
    for m in re.finditer(r"\$\$(.+?)\$\$", tex, flags=re.DOTALL):
        add("display($$ $$)", m.start(), m.end(), m.group(1))
    # \[ ... \]
    for m in re.finditer(r"\\\[(.+?)\\\]", tex, flags=re.DOTALL):
        add("display(\\[ \\])", m.start(), m.end(), m.group(1))
    # \( ... \)
    for m in re.finditer(r"\\\((.+?)\\\)", tex, flags=re.DOTALL):
        add("inline(\\( \\))", m.start(), m.end(), m.group(1))
    # inline $...$ (but not $$)
    for m in re.finditer(r"(?<!\$)\$(?!\$)(.+?)(?<!\$)\$(?!\$)", tex, flags=re.DOTALL):
        add("inline($ $)", m.start(), m.end(), m.group(1))

    # environments
    envs = ["equation","equation*","align","align*","multline","multline*",
            "gather","gather*","flalign","flalign*","eqnarray","eqnarray*","split"]
    for env in envs:
        pattern = rf"\\begin{{{re.escape(env)}}}(.+?)\\end{{{re.escape(env)}}}"
        for m in re.finditer(pattern, tex, flags=re.DOTALL):
            add("env", m.start(), m.end(), m.group(1), env=env)

    uniq = {}
    for it in matches:
        key = (it["start"], it["end"])
        if key not in uniq:
            uniq[key] = it

    out = list(uniq.values())
    out.sort(key=lambda x: x["start"])
    return out


# === ÏÖÄ 3: ÎÇúÏù¥ÎèÑ Ìú¥Î¶¨Ïä§Ìã± ===
ADV_TOKENS = [
    r"\\sum", r"\\prod", r"\\int", r"\\lim", r"\\nabla", r"\\partial",
    r"\\mathbb", r"\\mathcal", r"\\mathbf", r"\\boldsymbol",
    r"\\argmax", r"\\argmin", r"\\operatorname", r"\\mathrm\{KL\}",
    r"\\mathbb\{E\}", r"\\Pr", r"\\sigma", r"\\mu", r"\\Sigma", r"\\theta",
    r"\\frac\{[^{}]*\{[^{}]*\}[^{}]*\}",  # nested fraction
    r"\\hat\{", r"\\tilde\{", r"\\bar\{", r"\\widehat\{", r"\\widetilde\{",
    r"\\sqrt\{[^{}]*\{",                  # nested sqrt
    r"\\left", r"\\right",
    r"\\in", r"\\subset", r"\\forall", r"\\exists",
    r"\\cdot", r"\\times", r"\\otimes",
    r"IoU", r"\\log", r"\\exp",
    r"\\mathbb\{R\}", r"\\mathbb\{N\}", r"\\mathbb\{Z\}",
    r"\\Delta", r"\\delta", r"\\epsilon", r"\\varepsilon",
]
ADV_RE = re.compile("|".join(ADV_TOKENS))

def count_subscripts(expr: str) -> int:
    return len(re.findall(r"_[a-zA-Z0-9{\\]", expr))

def is_advanced(eq: str) -> bool:
    if ADV_RE.search(eq):
        return True
    if len(eq) > 40 and count_subscripts(eq) >= 2:
        return True
    if "\n" in eq and len(eq) > 30:
        return True
    return False


# === ÏÖÄ 4: Î¨∏ÏÑú Í∞úÏöî LLM ===
def take_slices(text: str, head_chars=4000, mid_chars=2000, tail_chars=4000):
    n = len(text)
    head = text[:min(head_chars, n)]
    mid_start = max((n // 2) - (mid_chars // 2), 0)
    mid = text[mid_start: mid_start + min(mid_chars, n)]
    tail = text[max(0, n - tail_chars):]
    return head, mid, tail

def _generate_with_mask_from_messages(messages: List[Dict]) -> str:
    if not model_loaded or tokenizer is None or model is None:
        raise RuntimeError("Math model is not loaded")
    inputs = tokenizer.apply_chat_template(
        messages, add_generation_prompt=True, return_tensors="pt", padding=True
    )
    attention_mask = (inputs != tokenizer.pad_token_id).long()
    with torch.no_grad():
        out = model.generate(
            input_ids=inputs.to(model.device),
            attention_mask=attention_mask.to(model.device),
            **GEN_KW
        )
    return tokenizer.decode(out[0], skip_special_tokens=True)

def chat_overview(prompt: str) -> str:
    if not model_loaded or tokenizer is None or model is None:
        raise RuntimeError("Math model is not loaded")
    messages = [
        {"role": "system", "content":
         "You are a clear, concise technical writer who summarizes LaTeX-based AI papers for a general technical audience."},
        {"role": "user", "content": prompt}
    ]
    text = _generate_with_mask_from_messages(messages)
    return text.split(messages[-1]["content"])[-1].strip()


# === ÏÖÄ 5: ÏàòÏãù Ìï¥ÏÑ§ LLM ===
EXPLAIN_SYSTEM = (
    "You are a teacher who explains math/AI research equations in clear, simple English. "
    "Always be precise, polite, and easy to understand."
)
EXPLAIN_TEMPLATE = """Please explain the following equation so that it can be understood by someone at least at a middle school level.
Follow this exact order in your output: Example ‚Üí Explanation ‚Üí Conclusion

- Example: Show the equation exactly as LaTeX in a single block (do not modify or add anything).
- Explanation: Provide bullet points explaining the meaning of symbols (‚àë, ùüô, ^, _, ‚àö, \\, etc.) and the role of each term, in a clear and concise way.
- Conclusion: Summarize in one sentence the core purpose of this equation in the context of the paper (e.g., loss composition, normalization, coordinate error, probability/log-likelihood, etc.).
- (Important) Do not change the symbols or the order of the equation, and do not invent new symbols.
- (Important) Write only in English.

[Equation]
{EQUATION}
"""

def explain_equation_with_llm(eq_latex: str) -> str:
    if not model_loaded or tokenizer is None or model is None:
        raise RuntimeError("Math model is not loaded")
    messages = [
        {"role": "system", "content": EXPLAIN_SYSTEM},
        {"role": "user",   "content": EXPLAIN_TEMPLATE.format(EQUATION=eq_latex)}
    ]
    text = _generate_with_mask_from_messages(messages)
    return text.split(messages[-1]["content"])[-1].strip()


# === ÏÖÄ 6: LaTeX Î¶¨Ìè¨Ìä∏(.tex) ===
def latex_escape_verbatim(s: str) -> str:
    s = s.replace("\\", r"\\")
    s = s.replace("#", r"\#").replace("$", r"\$")
    s = s.replace("%", r"\%").replace("&", r"\&")
    s = s.replace("_", r"\_").replace("{", r"\{").replace("}", r"\}")
    s = s.replace("^", r"\^{}").replace("~", r"\~{}")
    return s

def build_report(overview: str, items: List[Dict]) -> str:
    header = (r"""\\documentclass[11pt]{article}
\\usepackage[margin=1in]{geometry}
\\usepackage{amsmath, amssymb, amsfonts}
\\usepackage{hyperref}
\\usepackage{kotex}
\\setlength{\\parskip}{6pt}
\\setlength{\\parindent}{0pt}
\\title{LaTeX Equation Explanation Report (Middle-School Level+)}
\\author{Automatic Pipeline}
\\date{""" + datetime.date.today().isoformat() + r"""}
\\begin{document}
\\maketitle
\\tableofcontents
\\newpage
""")
    parts = [header]
    parts.append(r"\\section*{Document Overview}")
    parts.append(latex_escape_verbatim(overview))
    parts.append("\n\\newpage\n")

    for it in items:
        title = f"Lines {it['line_start']}‚Äì{it['line_end']} / {it['kind']} {('['+it['env']+']') if it['env'] else ''}"
        parts.append(f"\\section*{{{latex_escape_verbatim(title)}}}")
        parts.append(it["explanation"])
        parts.append("\n")

    parts.append("\\end{document}\n")
    return "\n".join(parts)


# === Î≥¥Ï°∞: ÏàòÏãù Í∞úÏàòÎßå ===
def count_equations_only(input_tex_path: str) -> Dict[str, int]:
    if not model_loaded:
        raise RuntimeError("Math model is not loaded")
    
    p = Path(input_tex_path)
    if not p.exists():
        raise FileNotFoundError(f"Cannot find TeX file: {input_tex_path}")

    src = p.read_text(encoding="utf-8", errors="ignore")
    offsets = make_line_offsets(src)
    pos_to_line = build_pos_to_line(offsets)
    equations_all = extract_equations(src, pos_to_line)
    equations_advanced = [e for e in equations_all if is_advanced(e["body"])]

    print(f"Ï¥ù ÏàòÏãù: {len(equations_all)}", flush=True)
    print(f"Ï§ëÌïôÏÉù ÏàòÏ§Ä Ïù¥ÏÉÅ: {len(equations_advanced)} / {len(equations_all)}", flush=True)

    return {"Ï¥ù ÏàòÏãù": len(equations_all),
            "Ï§ëÌïôÏÉù ÏàòÏ§Ä Ïù¥ÏÉÅ": len(equations_advanced)}


# === Î©îÏù∏ ÌååÏù¥ÌîÑÎùºÏù∏ ===
def run_pipeline(input_tex_path: str) -> Dict:
    if not model_loaded:
        raise RuntimeError("Math model is not loaded")
    
    p = Path(input_tex_path)
    if not p.exists():
        raise FileNotFoundError(f"Cannot find TeX file: {input_tex_path}")

    Path(OUT_DIR).mkdir(parents=True, exist_ok=True)

    src = p.read_text(encoding="utf-8", errors="ignore")

    offsets = make_line_offsets(src)
    pos_to_line = build_pos_to_line(offsets)

    equations_all = extract_equations(src, pos_to_line)
    equations_advanced = [e for e in equations_all if is_advanced(e["body"])]

    print(f"Ï¥ù ÏàòÏãù: {len(equations_all)}", flush=True)
    print(f"Ï§ëÌïôÏÉù ÏàòÏ§Ä Ïù¥ÏÉÅ: {len(equations_advanced)} / {len(equations_all)}", flush=True)

    head, mid, tail = take_slices(src)
    overview_prompt = textwrap.dedent(f"""
    You will be given three slices of a LaTeX document (head / middle / tail).
    Please produce a concise English overview with:
    - One short paragraph describing the core topic and objective of the paper.
    - A few bullet points listing the main sections or ideas (if discernible).
    - Focus on how mathematical notation and symbols are used to support the overall flow.

    [HEAD]
    {head}

    [MIDDLE]
    {mid}

    [TAIL]
    {tail}
    """).strip()
    doc_overview = chat_overview(overview_prompt)

    explanations: List[Dict] = []
    for idx, item in enumerate(equations_advanced, start=1):
        print(f"[{idx}/{len(equations_advanced)}] ÎùºÏù∏ {item['line_start']}‚Äì{item['line_end']}", flush=True)
        exp = explain_equation_with_llm(item["body"])
        explanations.append({
            "index": idx,
            "line_start": item["line_start"],
            "line_end": item["line_end"],
            "kind": item["kind"],
            "env": item["env"],
            "equation": item["body"],
            "explanation": exp
        })

    json_path = os.path.join(OUT_DIR, "equations_explained.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({"overview": doc_overview, "items": explanations}, f, ensure_ascii=False, indent=2)
    print(f"Ï†ÄÏû•Îêú JSON: {json_path}", flush=True)

    report_tex_path = os.path.join(OUT_DIR, "yolo_math_report.tex")
    report_tex = build_report(doc_overview, explanations)
    Path(report_tex_path).write_text(report_tex, encoding="utf-8")
    print(f"Ï†ÄÏû•Îêú TeX: {report_tex_path}", flush=True)

    return {
        "input": str(p),
        "counts": {
            "Ï¥ù ÏàòÏãù": len(equations_all),
            "Ï§ëÌïôÏÉù ÏàòÏ§Ä Ïù¥ÏÉÅ": len(equations_advanced)
        },
        "outputs": {
            "json": json_path,
            "report_tex": report_tex_path,
            "out_dir": OUT_DIR
        }
    }


# === FastAPI Ïï± ===
app = FastAPI(title="POLO Math Explainer API", version="1.0.0")

class MathRequest(BaseModel):
    path: str

@app.get("/health")
async def health():
    return {
        "status": "ok" if model_loaded else "degraded",
        "python": sys.version.split()[0],
        "torch": torch.__version__,
        "cuda": torch.cuda.is_available(),
        "device": DEVICE,
        "model_loaded": model_loaded,
        "tokenizer_loaded": tokenizer is not None,
        "model_name": MODEL_ID,
        "cache_dir": str(SAFE_CACHE_DIR)
    }

@app.get("/count/{file_path:path}")
async def count_get(file_path: str):
    if not model_loaded:
        raise HTTPException(status_code=503, detail="Math model is not loaded")
    try:
        return count_equations_only(file_path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {e}")

@app.post("/count")
async def count_post(req: MathRequest):
    if not model_loaded:
        raise HTTPException(status_code=503, detail="Math model is not loaded")
    try:
        return count_equations_only(req.path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {e}")

@app.get("/math/{file_path:path}")
async def math_get(file_path: str):
    if not model_loaded:
        raise HTTPException(status_code=503, detail="Math model is not loaded")
    try:
        return run_pipeline(file_path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {e}")

@app.post("/math")
async def math_post(req: MathRequest):
    if not model_loaded:
        raise HTTPException(status_code=503, detail="Math model is not loaded")
    try:
        return run_pipeline(req.path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {e}")

# ÏßÅÏ†ë Ïã§Ìñâ
if __name__ == "__main__":
    try:
        import uvicorn
        print("üöÄ Math Model ÏÑúÎ≤Ñ ÏãúÏûë Ï§ë...")
        uvicorn.run("app:app", host="0.0.0.0", port=5004, reload=False)
    except Exception as e:
        print(f"‚ùå Math Model ÏãúÏûë Ïã§Ìå®: {e}")
        import traceback
        traceback.print_exc()
        input("Press Enter to exit...")
