# -*- coding: utf-8 -*-
"""
LaTeX ìˆ˜ì‹ í•´ì„¤ API (FastAPI)

ì‹¤í–‰ ë°©ë²•
- ê°œë°œ ëª¨ë“œ(í•« ë¦¬ë¡œë“œ): uvicorn --reload app:app
- í”„ë¡œë•ì…˜ ëª¨ë“œ(ê¶Œì¥):   uvicorn app:app

ì œê³µ ì—”ë“œí¬ì¸íŠ¸
- GET  /health
  : ì„œë²„/ëª¨ë¸ ìƒíƒœ ì ê²€ìš© ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.

- GET  /count/{file_path:path}
  : íŠ¹ì • TeX íŒŒì¼ì˜ ìˆ˜ì‹ì„ ì¶”ì¶œí•´ "ì´ ìˆ˜ì‹ ê°œìˆ˜"ì™€ "ê³ ë‚œë„(ì¤‘í•™ìƒ ì´ìƒ) ìˆ˜ì‹ ê°œìˆ˜"ë§Œ ë¹ ë¥´ê²Œ ê³„ì‚°í•©ë‹ˆë‹¤.
    ì½˜ì†”ì—ë„ ì¦‰ì‹œ ì¶œë ¥(printf)ë˜ë©°, JSONìœ¼ë¡œ ê°œìˆ˜ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.

- POST /count
  : {"path": "C:\\...\\yolo.tex"} í˜•ì‹ì˜ JSONìœ¼ë¡œ ìœ„ì™€ ë™ì¼í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

- GET  /math/{file_path:path}
  : ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰(ìˆ˜ì‹ ì¶”ì¶œ/ë¶„ë¥˜ â†’ ë¬¸ì„œ ê°œìš”(ì˜ì–´) ìƒì„± â†’ ê³ ë‚œë„ ìˆ˜ì‹ì— ëŒ€í•œ í•´ì„¤(ì˜ì–´) ìƒì„± â†’ JSON/TeX ì €ì¥)

- POST /math
  : {"path": "C:\\...\\yolo.tex"} í˜•ì‹ì˜ JSONìœ¼ë¡œ ìœ„ì™€ ë™ì¼í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ì˜/íŠ¹ì§•
- ì½˜ì†” ì¶œë ¥ì´ ì§€ì—°ë˜ì§€ ì•Šë„ë¡ stdout ë¼ì¸ ë²„í¼ë§ + print(..., flush=True)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì¼ë¶€ ëª¨ë¸ì—ì„œ pad_tokenê³¼ eos_tokenì´ ê°™ì„ ë•Œ ëœ¨ëŠ” ê²½ê³ ë¥¼ í”¼í•˜ê¸° ìœ„í•´,
  pad í† í°ì´ ì—†ê±°ë‚˜ eosì™€ ê°™ìœ¼ë©´ [PAD] í† í°ì„ ì¶”ê°€í•˜ê³ , generate ì‹œ attention_maskë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
- LLM í”„ë¡¬í”„íŠ¸(ë¬¸ì„œ ê°œìš”/ìˆ˜ì‹ í•´ì„¤)ëŠ” ìš”ì²­ì— ë”°ë¼ ì˜ì–´ë¡œ ì‘ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""

# === ì…€ 1: í™˜ê²½ ì¤€ë¹„ & ëª¨ë¸ ë¡œë“œ ===
import os, sys, json, re, textwrap, datetime, torch
from typing import List, Dict
from pathlib import Path
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# [ê¶Œì¥] ì½˜ì†” ì¶œë ¥ì´ ë°”ë¡œ ë³´ì´ë„ë¡ stdoutì„ ì¤„ ë‹¨ìœ„ë¡œ ë²„í¼ë§í•©ë‹ˆë‹¤.
try:
    sys.stdout.reconfigure(line_buffering=True)
except Exception:
    pass

VERSION = "POLO-Math-API v4 (EN-prompts + flush + mask + pad)"
print(VERSION, flush=True)

# ----- ê²½ë¡œ ì„¤ì • -----
INPUT_TEX_PATH = r"C:\\POLO\\polo-system\\models\\math\\yolo.tex"
OUT_DIR        = "C:/POLO/polo-system/models/math/_build"
os.makedirs(OUT_DIR, exist_ok=True)

# ----- ëª¨ë¸/í† í¬ë‚˜ì´ì € ì„¤ì • -----
MODEL_ID = "Qwen/Qwen2.5-Math-1.5B-Instruct"
USE_4BIT = False
DEVICE   = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Python: {sys.version.split()[0]}", flush=True)
print(f"PyTorch: {torch.__version__}", flush=True)
print(f"CUDA available: {torch.cuda.is_available()}", flush=True)
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_name}", flush=True)
    print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {DEVICE}, ë°ì´í„° íƒ€ì…: float16", flush=True)
else:
    print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.", flush=True)
    print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {DEVICE}, ë°ì´í„° íƒ€ì…: float32", flush=True)

# --- ì¶”ê°€: ì•ˆì „ HF ìºì‹œ + .env ---
try:
    from dotenv import load_dotenv
    ROOT_ENV = Path(__file__).resolve().parents[2] / ".env"
    if ROOT_ENV.exists():
        load_dotenv(dotenv_path=str(ROOT_ENV), override=True)
        print(f"[dotenv] loaded: {ROOT_ENV}", flush=True)
except Exception:
    pass

SAFE_CACHE_DIR = Path(__file__).resolve().parent / "hf_cache"
SAFE_CACHE_DIR.mkdir(parents=True, exist_ok=True)

def _force_safe_hf_cache():
    for k in ("HF_HOME", "TRANSFORMERS_CACHE", "HF_DATASETS_CACHE", "HUGGINGFACE_HUB_CACHE"):
        os.environ[k] = str(SAFE_CACHE_DIR)
    print(f"[hf_cache] forced â†’ {SAFE_CACHE_DIR}", flush=True)

_force_safe_hf_cache()
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

try:
    # 1) í† í¬ë‚˜ì´ì € (ìºì‹œ/í† í° ëª…ì‹œ)
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_ID,
        trust_remote_code=True,
        token=HF_TOKEN,
        cache_dir=str(SAFE_CACHE_DIR),
    )

    # 2) pad í† í° ë³´ì •
    PAD_ADDED = False
    if tokenizer.pad_token_id is None or tokenizer.pad_token_id == tokenizer.eos_token_id:
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        PAD_ADDED = True

    # 3) ëª¨ë¸ ë¡œë“œ (í•„ìš” ì‹œ 4bit)
    bnb_config = None
    if USE_4BIT:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.float16 if DEVICE == "cuda" else torch.float32
        )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map="auto",
        torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
        quantization_config=bnb_config,
        low_cpu_mem_usage=True,
        trust_remote_code=True,
        token=HF_TOKEN,
        cache_dir=str(SAFE_CACHE_DIR),
    )

    if PAD_ADDED:
        model.resize_token_embeddings(len(tokenizer))

    GEN_KW = dict(
        max_new_tokens=512,
        temperature=0.2,
        top_p=0.9,
        do_sample=True
    )
    print("Model & tokenizer loaded.", flush=True)

except Exception as e:
    tokenizer = None
    model = None
    GEN_KW = {}
    print("[Model Load Error]", e, flush=True)


# === ê³µí†µ ìœ í‹¸: ë¼ì¸ ì˜¤í”„ì…‹ ì¸ë±ìŠ¤ ===
def make_line_offsets(text: str) -> List[int]:
    lines = text.splitlines()
    offsets, pos = [], 0
    for ln in lines:
        offsets.append(pos)
        pos += len(ln) + 1  # '\n' í¬í•¨
    return offsets

def build_pos_to_line(offsets: List[int]):
    def pos_to_line(p: int) -> int:
        lo, hi = 0, len(offsets)-1
        while lo <= hi:
            mid = (lo+hi)//2
            if offsets[mid] <= p:
                lo = mid + 1
            else:
                hi = mid - 1
        return hi + 1  # 1-based
    return pos_to_line


# === ì…€ 2: LaTeX ìˆ˜ì‹ ì¶”ì¶œ ===
def extract_equations(tex: str, pos_to_line) -> List[Dict]:
    matches: List[Dict] = []

    def add(kind, start, end, body, env=""):
        matches.append({
            "kind": kind, "env": env, "start": start, "end": end,
            "line_start": pos_to_line(start), "line_end": pos_to_line(end),
            "body": body.strip()
        })

    # $$ ... $$
    for m in re.finditer(r"\$\$(.+?)\$\$", tex, flags=re.DOTALL):
        add("display($$ $$)", m.start(), m.end(), m.group(1))
    # \[ ... \]
    for m in re.finditer(r"\\\[(.+?)\\\]", tex, flags=re.DOTALL):
        add("display(\\[ \\])", m.start(), m.end(), m.group(1))
    # \( ... \)
    for m in re.finditer(r"\\\((.+?)\\\)", tex, flags=re.DOTALL):
        add("inline(\\( \\))", m.start(), m.end(), m.group(1))
    # inline $...$ (but not $$)
    for m in re.finditer(r"(?<!\$)\$(?!\$)(.+?)(?<!\$)\$(?!\$)", tex, flags=re.DOTALL):
        add("inline($ $)", m.start(), m.end(), m.group(1))

    # environments
    envs = ["equation","equation*","align","align*","multline","multline*",
            "gather","gather*","flalign","flalign*","eqnarray","eqnarray*","split"]
    for env in envs:
        pattern = rf"\\begin{{{re.escape(env)}}}(.+?)\\end{{{re.escape(env)}}}"
        for m in re.finditer(pattern, tex, flags=re.DOTALL):
            add("env", m.start(), m.end(), m.group(1), env=env)

    uniq = {}
    for it in matches:
        key = (it["start"], it["end"])
        if key not in uniq:
            uniq[key] = it

    out = list(uniq.values())
    out.sort(key=lambda x: x["start"])
    return out


# === ì…€ 3: ë‚œì´ë„ íœ´ë¦¬ìŠ¤í‹± ===
ADV_TOKENS = [
    r"\\sum", r"\\prod", r"\\int", r"\\lim", r"\\nabla", r"\\partial",
    r"\\mathbb", r"\\mathcal", r"\\mathbf", r"\\boldsymbol",
    r"\\argmax", r"\\argmin", r"\\operatorname", r"\\mathrm\{KL\}",
    r"\\mathbb\{E\}", r"\\Pr", r"\\sigma", r"\\mu", r"\\Sigma", r"\\theta",
    r"\\frac\{[^{}]*\{[^{}]*\}[^{}]*\}",  # nested fraction
    r"\\hat\{", r"\\tilde\{", r"\\bar\{", r"\\widehat\{", r"\\widetilde\{",
    r"\\sqrt\{[^{}]*\{",                  # nested sqrt
    r"\\left", r"\\right",
    r"\\in", r"\\subset", r"\\forall", r"\\exists",
    r"\\cdot", r"\\times", r"\\otimes",
    r"IoU", r"\\log", r"\\exp",
    r"\\mathbb\{R\}", r"\\mathbb\{N\}", r"\\mathbb\{Z\}",
    r"\\Delta", r"\\delta", r"\\epsilon", r"\\varepsilon",
]
ADV_RE = re.compile("|".join(ADV_TOKENS))

def count_subscripts(expr: str) -> int:
    return len(re.findall(r"_[a-zA-Z0-9{\\]", expr))

def is_advanced(eq: str) -> bool:
    if ADV_RE.search(eq):
        return True
    if len(eq) > 40 and count_subscripts(eq) >= 2:
        return True
    if "\n" in eq and len(eq) > 30:
        return True
    return False


# === ì…€ 4: ë¬¸ì„œ ê°œìš” LLM ===
def take_slices(text: str, head_chars=4000, mid_chars=2000, tail_chars=4000):
    n = len(text)
    head = text[:min(head_chars, n)]
    mid_start = max((n // 2) - (mid_chars // 2), 0)
    mid = text[mid_start: mid_start + min(mid_chars, n)]
    tail = text[max(0, n - tail_chars):]
    return head, mid, tail

def _generate_with_mask_from_messages(messages: List[Dict]) -> str:
    inputs = tokenizer.apply_chat_template(
        messages, add_generation_prompt=True, return_tensors="pt", padding=True
    )
    attention_mask = (inputs != tokenizer.pad_token_id).long()
    with torch.no_grad():
        out = model.generate(
            input_ids=inputs.to(model.device),
            attention_mask=attention_mask.to(model.device),
            **GEN_KW
        )
    return tokenizer.decode(out[0], skip_special_tokens=True)

def chat_overview(prompt: str) -> str:
    if tokenizer is None or model is None:
        raise RuntimeError("Model is not loaded.")
    messages = [
        {"role": "system", "content":
         "You are a clear, concise technical writer who summarizes LaTeX-based AI papers for a general technical audience."},
        {"role": "user", "content": prompt}
    ]
    text = _generate_with_mask_from_messages(messages)
    return text.split(messages[-1]["content"])[-1].strip()


# === ì…€ 5: ìˆ˜ì‹ í•´ì„¤ LLM ===
EXPLAIN_SYSTEM = (
    "You are a teacher who explains math/AI research equations in clear, simple English. "
    "Always be precise, polite, and easy to understand."
)
EXPLAIN_TEMPLATE = """Please explain the following equation so that it can be understood by someone at least at a middle school level.
Follow this exact order in your output: Example â†’ Explanation â†’ Conclusion

- Example: Show the equation exactly as LaTeX in a single block (do not modify or add anything).
- Explanation: Provide bullet points explaining the meaning of symbols (âˆ‘, ğŸ™, ^, _, âˆš, \\, etc.) and the role of each term, in a clear and concise way.
- Conclusion: Summarize in one sentence the core purpose of this equation in the context of the paper (e.g., loss composition, normalization, coordinate error, probability/log-likelihood, etc.).
- (Important) Do not change the symbols or the order of the equation, and do not invent new symbols.
- (Important) Write only in English.

[Equation]
{EQUATION}
"""

def explain_equation_with_llm(eq_latex: str) -> str:
    if tokenizer is None or model is None:
        raise RuntimeError("Model is not loaded.")
    messages = [
        {"role": "system", "content": EXPLAIN_SYSTEM},
        {"role": "user",   "content": EXPLAIN_TEMPLATE.format(EQUATION=eq_latex)}
    ]
    text = _generate_with_mask_from_messages(messages)
    return text.split(messages[-1]["content"])[-1].strip()


# === ì…€ 6: LaTeX ë¦¬í¬íŠ¸(.tex) ===
def latex_escape_verbatim(s: str) -> str:
    s = s.replace("\\", r"\\")
    s = s.replace("#", r"\#").replace("$", r"\$")
    s = s.replace("%", r"\%").replace("&", r"\&")
    s = s.replace("_", r"\_").replace("{", r"\{").replace("}", r"\}")
    s = s.replace("^", r"\^{}").replace("~", r"\~{}")
    return s

def build_report(overview: str, items: List[Dict]) -> str:
    header = (r"""\\documentclass[11pt]{article}
\\usepackage[margin=1in]{geometry}
\\usepackage{amsmath, amssymb, amsfonts}
\\usepackage{hyperref}
\\usepackage{kotex}
\\setlength{\\parskip}{6pt}
\\setlength{\\parindent}{0pt}
\\title{LaTeX Equation Explanation Report (Middle-School Level+)}
\\author{Automatic Pipeline}
\\date{""" + datetime.date.today().isoformat() + r"""}
\\begin{document}
\\maketitle
\\tableofcontents
\\newpage
""")
    parts = [header]
    parts.append(r"\\section*{Document Overview}")
    parts.append(latex_escape_verbatim(overview))
    parts.append("\n\\newpage\n")

    for it in items:
        title = f"Lines {it['line_start']}â€“{it['line_end']} / {it['kind']} {('['+it['env']+']') if it['env'] else ''}"
        parts.append(f"\\section*{{{latex_escape_verbatim(title)}}}")
        parts.append(it["explanation"])
        parts.append("\n")

    parts.append("\\end{document}\n")
    return "\n".join(parts)


# === ë³´ì¡°: ìˆ˜ì‹ ê°œìˆ˜ë§Œ ===
def count_equations_only(input_tex_path: str) -> Dict[str, int]:
    p = Path(input_tex_path)
    if not p.exists():
        raise FileNotFoundError(f"Cannot find TeX file: {input_tex_path}")

    src = p.read_text(encoding="utf-8", errors="ignore")
    offsets = make_line_offsets(src)
    pos_to_line = build_pos_to_line(offsets)
    equations_all = extract_equations(src, pos_to_line)
    equations_advanced = [e for e in equations_all if is_advanced(e["body"])]

    print(f"ì´ ìˆ˜ì‹: {len(equations_all)}", flush=True)
    print(f"ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒ: {len(equations_advanced)} / {len(equations_all)}", flush=True)

    return {"ì´ ìˆ˜ì‹": len(equations_all),
            "ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒ": len(equations_advanced)}


# === ë©”ì¸ íŒŒì´í”„ë¼ì¸ ===
def run_pipeline(input_tex_path: str) -> Dict:
    p = Path(input_tex_path)
    if not p.exists():
        raise FileNotFoundError(f"Cannot find TeX file: {input_tex_path}")

    Path(OUT_DIR).mkdir(parents=True, exist_ok=True)

    src = p.read_text(encoding="utf-8", errors="ignore")

    offsets = make_line_offsets(src)
    pos_to_line = build_pos_to_line(offsets)

    equations_all = extract_equations(src, pos_to_line)
    equations_advanced = [e for e in equations_all if is_advanced(e["body"])]

    print(f"ì´ ìˆ˜ì‹: {len(equations_all)}", flush=True)
    print(f"ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒ: {len(equations_advanced)} / {len(equations_all)}", flush=True)

    head, mid, tail = take_slices(src)
    overview_prompt = textwrap.dedent(f"""
    You will be given three slices of a LaTeX document (head / middle / tail).
    Please produce a concise English overview with:
    - One short paragraph describing the core topic and objective of the paper.
    - A few bullet points listing the main sections or ideas (if discernible).
    - Focus on how mathematical notation and symbols are used to support the overall flow.

    [HEAD]
    {head}

    [MIDDLE]
    {mid}

    [TAIL]
    {tail}
    """).strip()
    doc_overview = chat_overview(overview_prompt)

    explanations: List[Dict] = []
    for idx, item in enumerate(equations_advanced, start=1):
        print(f"[{idx}/{len(equations_advanced)}] ë¼ì¸ {item['line_start']}â€“{item['line_end']}", flush=True)
        exp = explain_equation_with_llm(item["body"])
        explanations.append({
            "index": idx,
            "line_start": item["line_start"],
            "line_end": item["line_end"],
            "kind": item["kind"],
            "env": item["env"],
            "equation": item["body"],
            "explanation": exp
        })

    json_path = os.path.join(OUT_DIR, "equations_explained.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({"overview": doc_overview, "items": explanations}, f, ensure_ascii=False, indent=2)
    print(f"ì €ì¥ëœ JSON: {json_path}", flush=True)

    report_tex_path = os.path.join(OUT_DIR, "yolo_math_report.tex")
    report_tex = build_report(doc_overview, explanations)
    Path(report_tex_path).write_text(report_tex, encoding="utf-8")
    print(f"ì €ì¥ëœ TeX: {report_tex_path}", flush=True)

    return {
        "input": str(p),
        "counts": {
            "ì´ ìˆ˜ì‹": len(equations_all),
            "ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒ": len(equations_advanced)
        },
        "outputs": {
            "json": json_path,
            "report_tex": report_tex_path,
            "out_dir": OUT_DIR
        }
    }


# === FastAPI ì•± ===
app = FastAPI(title="POLO Math Explainer API", version="1.0.0")

class MathRequest(BaseModel):
    path: str

@app.get("/health")
async def health():
    return {
        "status": "ok",
        "python": sys.version.split()[0],
        "torch": torch.__version__,
        "cuda": torch.cuda.is_available(),
        "device": DEVICE,
        "model_loaded": (tokenizer is not None and model is not None)
    }

@app.get("/count/{file_path:path}")
async def count_get(file_path: str):
    try:
        return count_equations_only(file_path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {e}")

@app.post("/count")
async def count_post(req: MathRequest):
    try:
        return count_equations_only(req.path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {e}")

@app.get("/math/{file_path:path}")
async def math_get(file_path: str):
    try:
        return run_pipeline(file_path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {e}")

@app.post("/math")
async def math_post(req: MathRequest):
    try:
        return run_pipeline(req.path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {e}")

# ì§ì ‘ ì‹¤í–‰
if __name__ == "__main__":
    try:
        import uvicorn
        print("ğŸš€ Math Model ì„œë²„ ì‹œì‘ ì¤‘...")
        uvicorn.run("app:app", host="0.0.0.0", port=5004, reload=False)
    except Exception as e:
        print(f"âŒ Math Model ì‹œì‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        input("Press Enter to exit...")
