{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1c5a43",
   "metadata": {},
   "source": [
    "# ì½”ë“œì„¤ëª…\n",
    "Texë¥¼ ë°›ì•„ì™€ì„œ ìˆ˜ì‹ë§Œ ë½‘ì•„ë‚¸ í›„\n",
    "\n",
    "ì¤‘í•™ìƒ ì´ìƒì˜ ìˆ˜ì¤€ì„ ìš”êµ¬í•˜ëŠ” ìˆ˜ì‹ë§Œ í•´ì„¤\n",
    "\n",
    "_build í´ë” ì•ˆì— .texíŒŒì¼ë¡œ ìƒê¸´ë‹¤!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6b2df",
   "metadata": {},
   "source": [
    "### ì…€ 1. í™˜ê²½ ì¤€ë¹„ & ëª¨ë¸ ë¡œë“œ (4-bit, CUDA ìë™ ê°ì§€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4be5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\requirement\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9\n",
      "PyTorch: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Device selected: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "# === ì…€ 1: í™˜ê²½ ì¤€ë¹„ & ëª¨ë¸ ë¡œë“œ ===\n",
    "import os, torch, sys, platform, json, re, textwrap, subprocess, shutil\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# ----- ê¸°ë³¸ ê²½ë¡œ ì„¤ì • -----\n",
    "INPUT_TEX_PATH = r\"C:\\POLO\\polo-system\\models\\math\\iclr2022_conference.tex\"        # <- ë¶„ì„í•  LaTeX íŒŒì¼ ê²½ë¡œ\n",
    "INPUT_TEX_PATH = r\"C:\\POLO\\polo-system\\models\\math\\yolo.tex\"        # <- ë¶„ì„í•  LaTeX íŒŒì¼ ê²½ë¡œ\n",
    "OUT_DIR        = \"C:/POLO/polo-system/models/math./_build\"          # ì‚°ì¶œë¬¼ í´ë” ê²½ë¡œ\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----- ëª¨ë¸/í† í¬ë‚˜ì´ì € ì„¤ì • -----\n",
    "MODEL_ID = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"   # 1.5B ìˆ˜í•™ íŠ¹í™” instruct\n",
    "USE_4BIT = True                                 # 8GB VRAMì„ ê³ ë ¤í•œ 4-bit ì–‘ìí™”\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Device selected: {DEVICE}\")\n",
    "\n",
    "# ----- ëª¨ë¸ ë¡œë“œ -----\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "bnb_config = None\n",
    "if USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ê³µí†µ generate ì„¤ì •\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"Model & tokenizer loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dcc565",
   "metadata": {},
   "source": [
    "### ì…€ 2. LaTeX íŒŒì„œ: ìˆ˜ì‹ ì¶”ì¶œ(ì¸ë¼ì¸/ë””ìŠ¤í”Œë ˆì´/í™˜ê²½) + ë¼ì¸ë²ˆí˜¸ ë§¤í•‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "134f4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ìˆ˜ì‹ ê°œìˆ˜: 465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'kind': 'inline($ $)',\n",
       "  'env': '',\n",
       "  'start': 1370,\n",
       "  'end': 1374,\n",
       "  'line_start': 50,\n",
       "  'line_end': 50,\n",
       "  'body': '^*'},\n",
       " {'kind': 'inline($ $)',\n",
       "  'env': '',\n",
       "  'start': 3564,\n",
       "  'end': 3567,\n",
       "  'line_start': 85,\n",
       "  'line_end': 85,\n",
       "  'body': 'A'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === ì…€ 2: LaTeX ìˆ˜ì‹ ì¶”ì¶œ ìœ í‹¸ ===\n",
    "from pathlib import Path\n",
    "\n",
    "assert Path(INPUT_TEX_PATH).exists(), f\"ì…ë ¥ TeX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_TEX_PATH}\"\n",
    "src = Path(INPUT_TEX_PATH).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "# ë¼ì¸ ì˜¤í”„ì…‹ ì¸ë±ìŠ¤\n",
    "lines = src.splitlines()\n",
    "offsets = []\n",
    "pos = 0\n",
    "for ln in lines:\n",
    "    offsets.append(pos)\n",
    "    pos += len(ln) + 1\n",
    "\n",
    "def pos_to_line(p:int)->int:\n",
    "    lo, hi = 0, len(offsets)-1\n",
    "    while lo <= hi:\n",
    "        mid = (lo+hi)//2\n",
    "        if offsets[mid] <= p:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            hi = mid - 1\n",
    "    return hi + 1  # 1-based\n",
    "\n",
    "def extract_equations(tex:str)->List[Dict]:\n",
    "    matches = []\n",
    "    def add(kind, start, end, body, env=\"\"):\n",
    "        matches.append({\n",
    "            \"kind\": kind, \"env\": env, \"start\": start, \"end\": end,\n",
    "            \"line_start\": pos_to_line(start), \"line_end\": pos_to_line(end),\n",
    "            \"body\": body.strip()\n",
    "        })\n",
    "    # $$ ... $$\n",
    "    for m in re.finditer(r\"\\$\\$(.+?)\\$\\$\", tex, flags=re.DOTALL):\n",
    "        add(\"display($$ $$)\", m.start(), m.end(), m.group(1))\n",
    "    # \\[ ... \\]\n",
    "    for m in re.finditer(r\"\\\\\\[(.+?)\\\\\\]\", tex, flags=re.DOTALL):\n",
    "        add(\"display(\\\\[ \\\\])\", m.start(), m.end(), m.group(1))\n",
    "    # \\( ... \\)\n",
    "    for m in re.finditer(r\"\\\\\\((.+?)\\\\\\)\", tex, flags=re.DOTALL):\n",
    "        add(\"inline(\\\\( \\\\))\", m.start(), m.end(), m.group(1))\n",
    "    # inline $...$ (ë‹¨, $$ ì œì™¸)\n",
    "    for m in re.finditer(r\"(?<!\\$)\\$(?!\\$)(.+?)(?<!\\$)\\$(?!\\$)\", tex, flags=re.DOTALL):\n",
    "        add(\"inline($ $)\", m.start(), m.end(), m.group(1))\n",
    "    # environments\n",
    "    envs = [\"equation\",\"equation*\",\"align\",\"align*\",\"multline\",\"multline*\",\"gather\",\"gather*\",\"flalign\",\"flalign*\",\"eqnarray\",\"eqnarray*\",\"split\"]\n",
    "    for env in envs:\n",
    "        pattern = rf\"\\\\begin{{{re.escape(env)}}}(.+?)\\\\end{{{re.escape(env)}}}\"\n",
    "        for m in re.finditer(pattern, tex, flags=re.DOTALL):\n",
    "            add(f\"env\", m.start(), m.end(), m.group(1), env=env)\n",
    "    # ì¤‘ë³µ(ë™ì¼ ë²”ìœ„) ì œê±° ë° ì •ë ¬\n",
    "    uniq = {}\n",
    "    for it in matches:\n",
    "        key = (it[\"start\"], it[\"end\"])\n",
    "        if key not in uniq:\n",
    "            uniq[key] = it\n",
    "    out = list(uniq.values())\n",
    "    out.sort(key=lambda x: x[\"start\"])\n",
    "    return out\n",
    "\n",
    "equations_all = extract_equations(src)\n",
    "print(f\"ì´ ìˆ˜ì‹ ê°œìˆ˜: {len(equations_all)}\")\n",
    "equations_all[:2]  # ë¯¸ë¦¬ë³´ê¸°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8992e",
   "metadata": {},
   "source": [
    "### ì…€ 3. â€œì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒâ€ íŒë³„ íœ´ë¦¬ìŠ¤í‹±\n",
    "\n",
    "> ê¸°ë³¸ì ìœ¼ë¡œ âˆ‘, âˆ‚, argmax, ë¶„ìˆ˜Â·ì œê³±ê·¼ì˜ ì¤‘ì²©, ì¢Œí‘œ/ë°•ìŠ¤ ì§€ì‹œì ë“± ì¡°ê¸ˆ ë³µí•©ì ì¸ í‘œê¸°ê°€ ìˆìœ¼ë©´ ìƒìœ„ ë‚œì´ë„ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤. í”„ë¡œì íŠ¸ì— ë§ê²Œ ê·œì¹™ì„ ì‰½ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312366e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ ìˆ˜ì‹: 84 / 465\n"
     ]
    }
   ],
   "source": [
    "# === ì…€ 3: ë‚œì´ë„ íœ´ë¦¬ìŠ¤í‹± ì •ì˜ ===\n",
    "ADV_TOKENS = [\n",
    "    r\"\\\\sum\", r\"\\\\prod\", r\"\\\\int\", r\"\\\\lim\", r\"\\\\nabla\", r\"\\\\partial\",\n",
    "    r\"\\\\mathbb\", r\"\\\\mathcal\", r\"\\\\mathbf\", r\"\\\\boldsymbol\",\n",
    "    r\"\\\\argmax\", r\"\\\\argmin\", r\"\\\\operatorname\", r\"\\\\mathrm\\{KL\\}\",\n",
    "    r\"\\\\mathbb\\{E\\}\", r\"\\\\Pr\", r\"\\\\sigma\", r\"\\\\mu\", r\"\\\\Sigma\", r\"\\\\theta\",\n",
    "    r\"\\\\frac\\{[^{}]*\\{[^{}]*\\}[^{}]*\\}\",  # ì¤‘ì²© ë¶„ìˆ˜\n",
    "    r\"\\\\hat\\{\", r\"\\\\tilde\\{\", r\"\\\\bar\\{\", r\"\\\\widehat\\{\", r\"\\\\widetilde\\{\",\n",
    "    r\"\\\\sqrt\\{[^{}]*\\{\",                   # ì¤‘ì²© sqrt\n",
    "    r\"\\\\left\", r\"\\\\right\",\n",
    "    r\"\\\\in\", r\"\\\\subset\", r\"\\\\forall\", r\"\\\\exists\",\n",
    "    r\"\\\\cdot\", r\"\\\\times\", r\"\\\\otimes\",\n",
    "    r\"IoU\", r\"\\\\log\", r\"\\\\exp\",\n",
    "    r\"\\\\mathbb\\{R\\}\", r\"\\\\mathbb\\{N\\}\", r\"\\\\mathbb\\{Z\\}\",\n",
    "    r\"\\\\Delta\", r\"\\\\delta\", r\"\\\\epsilon\", r\"\\\\varepsilon\",\n",
    "]\n",
    "\n",
    "ADV_RE = re.compile(\"|\".join(ADV_TOKENS))\n",
    "\n",
    "def count_subscripts(expr: str) -> int:\n",
    "    return len(re.findall(r\"_[a-zA-Z0-9{\\\\]\", expr))\n",
    "\n",
    "def is_advanced(eq: str) -> bool:\n",
    "    if ADV_RE.search(eq):\n",
    "        return True\n",
    "    if len(eq) > 40 and count_subscripts(eq) >= 2:\n",
    "        return True\n",
    "    if \"\\n\" in eq and len(eq) > 30:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "equations_advanced = [e for e in equations_all if is_advanced(e[\"body\"])]\n",
    "print(f\"ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ ìˆ˜ì‹: {len(equations_advanced)} / {len(equations_all)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea70d70b",
   "metadata": {},
   "source": [
    "### ì…€ 4. â€œë¬¸ì„œ ì „ì²´ ì´í•´â€ ìš”ì•½(ê°œìš”/ì„¹ì…˜ ìš”ì§€) ìƒì„±\n",
    "\n",
    "> .tex ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ ë„£ê¸°ì—” ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í† ë§‰ ìš”ì•½ ë°©ì‹(ì•/ì¤‘/ë’¤ ìƒ˜í”Œë§)ìœ¼ë¡œ ê°œìš”ë¥¼ ë½‘ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ebd4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": [
    "# === ì…€ 4: ë¬¸ì„œ ì „ì²´ ê°œìš” ìƒì„± (ëª¨ë¸ ìš”ì•½) ===\n",
    "def take_slices(text:str, head_chars=4000, mid_chars=2000, tail_chars=4000):\n",
    "    n = len(text)\n",
    "    head = text[:min(head_chars, n)]\n",
    "    mid_start = max((n//2) - (mid_chars//2), 0)\n",
    "    mid = text[mid_start: mid_start + min(mid_chars, n)]\n",
    "    tail = text[max(0, n - tail_chars):]\n",
    "    return head, mid, tail\n",
    "\n",
    "head, mid, tail = take_slices(src)\n",
    "\n",
    "def chat(prompt:str)->str:\n",
    "    # Qwen2.5 Instruct í¬ë§· ê°„ë‹¨ ì‚¬ìš©\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\":\"ë‹¹ì‹ ì€ AI ë…¼ë¬¸/ìˆ˜ì‹ì„ í•œêµ­ì–´ë¡œ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ì„ ìƒë‹˜ì…ë‹ˆë‹¤. í•­ìƒ ì¡´ëŒ“ë§ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\"},\n",
    "        {\"role\":\"user\", \"content\": prompt}\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(input_ids=input_ids, **GEN_KW)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    # chat í…œí”Œë¦¿ ì ‘ë‘ë¶€ ì œê±°\n",
    "    return text.split(messages[-1][\"content\"])[-1].strip()\n",
    "\n",
    "overview_prompt = textwrap.dedent(f\"\"\"\n",
    "ë‹¤ìŒ LaTeX ë¬¸ì„œì˜ ì•/ì¤‘/ë’¤ ì¼ë¶€ë¥¼ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.\n",
    "- í•µì‹¬ ì£¼ì œ/ëª©í‘œë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•˜ì‹œê³ ,\n",
    "- ì£¼ìš” ì„¹ì…˜(ìˆë‹¤ë©´)ì„ ë¶ˆë¦¿ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\n",
    "- ìˆ˜í•™ í‘œê¸°/ê¸°í˜¸ í•´ì„ì— ì´ˆì ì„ ë§ì¶°, ì „ì²´ íë¦„ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
    "- í•­ìƒ í•œêµ­ì–´ ì¡´ëŒ“ë§ë¡œ, ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ.\n",
    "\n",
    "[ì•ë¶€ë¶„]\n",
    "{head}\n",
    "\n",
    "[ì¤‘ê°„]\n",
    "{mid}\n",
    "\n",
    "[ë’·ë¶€ë¶„]\n",
    "{tail}\n",
    "\"\"\").strip()\n",
    "\n",
    "doc_overview = chat(overview_prompt)\n",
    "print(doc_overview[:1200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2a3a4",
   "metadata": {},
   "source": [
    "### ì…€ 5. ìˆ˜ì‹ í•´ì„¤(ì˜ˆì‹œ â†’ ì„¤ëª… â†’ ê²°ë¡ ) ìƒì„± í•¨ìˆ˜ + ë°°ì¹˜ ì‹¤í–‰\n",
    "\n",
    "> ê° ìˆ˜ì‹ì€ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì œì‹œí•˜ê³ , ìƒì§•/ê¸°í˜¸ì˜ ì˜ë¯¸ì™€ YOLO ë§¥ë½(ì¢Œí‘œ, ì§€ì‹œì ğŸ™, âˆšw_i ë“±)ì„ ë°˜ì˜í•´ í•œêµ­ì–´/ì¡´ëŒ“ë§/ë¯¸ê´„ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f923056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/84] ë¼ì¸ 134â€“134\n",
      "[2/84] ë¼ì¸ 136â€“136\n",
      "[3/84] ë¼ì¸ 152â€“152\n",
      "[4/84] ë¼ì¸ 153â€“156\n",
      "[5/84] ë¼ì¸ 157â€“157\n",
      "[6/84] ë¼ì¸ 157â€“157\n",
      "[7/84] ë¼ì¸ 160â€“160\n",
      "[8/84] ë¼ì¸ 161â€“161\n",
      "[9/84] ë¼ì¸ 162â€“165\n",
      "[10/84] ë¼ì¸ 166â€“166\n",
      "[11/84] ë¼ì¸ 233â€“233\n",
      "[12/84] ë¼ì¸ 233â€“233\n",
      "[13/84] ë¼ì¸ 233â€“233\n",
      "[14/84] ë¼ì¸ 235â€“235\n",
      "[15/84] ë¼ì¸ 237â€“240\n",
      "[16/84] ë¼ì¸ 243â€“243\n",
      "[17/84] ë¼ì¸ 244â€“244\n",
      "[18/84] ë¼ì¸ 259â€“259\n",
      "[19/84] ë¼ì¸ 282â€“282\n",
      "[20/84] ë¼ì¸ 297â€“297\n",
      "[21/84] ë¼ì¸ 361â€“361\n",
      "[22/84] ë¼ì¸ 365â€“365\n",
      "[23/84] ë¼ì¸ 366â€“366\n",
      "[24/84] ë¼ì¸ 366â€“366\n",
      "[25/84] ë¼ì¸ 366â€“366\n",
      "[26/84] ë¼ì¸ 397â€“397\n",
      "[27/84] ë¼ì¸ 397â€“397\n",
      "[28/84] ë¼ì¸ 398â€“398\n",
      "[29/84] ë¼ì¸ 430â€“430\n",
      "[30/84] ë¼ì¸ 440â€“443\n",
      "[31/84] ë¼ì¸ 447â€“447\n",
      "[32/84] ë¼ì¸ 453â€“453\n",
      "[33/84] ë¼ì¸ 453â€“453\n",
      "[34/84] ë¼ì¸ 461â€“461\n",
      "[35/84] ë¼ì¸ 461â€“461\n",
      "[36/84] ë¼ì¸ 461â€“461\n",
      "[37/84] ë¼ì¸ 461â€“461\n",
      "[38/84] ë¼ì¸ 471â€“471\n",
      "[39/84] ë¼ì¸ 471â€“471\n",
      "[40/84] ë¼ì¸ 477â€“477\n",
      "[41/84] ë¼ì¸ 477â€“477\n",
      "[42/84] ë¼ì¸ 477â€“477\n",
      "[43/84] ë¼ì¸ 482â€“482\n",
      "[44/84] ë¼ì¸ 484â€“484\n",
      "[45/84] ë¼ì¸ 485â€“485\n",
      "[46/84] ë¼ì¸ 485â€“485\n",
      "[47/84] ë¼ì¸ 485â€“485\n",
      "[48/84] ë¼ì¸ 488â€“488\n",
      "[49/84] ë¼ì¸ 488â€“488\n",
      "[50/84] ë¼ì¸ 499â€“499\n",
      "[51/84] ë¼ì¸ 499â€“499\n",
      "[52/84] ë¼ì¸ 503â€“503\n",
      "[53/84] ë¼ì¸ 504â€“504\n",
      "[54/84] ë¼ì¸ 508â€“508\n",
      "[55/84] ë¼ì¸ 514â€“514\n",
      "[56/84] ë¼ì¸ 514â€“514\n",
      "[57/84] ë¼ì¸ 515â€“515\n",
      "[58/84] ë¼ì¸ 534â€“534\n",
      "[59/84] ë¼ì¸ 1042â€“1042\n",
      "[60/84] ë¼ì¸ 1042â€“1042\n",
      "[61/84] ë¼ì¸ 1042â€“1042\n",
      "[62/84] ë¼ì¸ 1045â€“1045\n",
      "[63/84] ë¼ì¸ 1046â€“1046\n",
      "[64/84] ë¼ì¸ 1049â€“1049\n",
      "[65/84] ë¼ì¸ 1052â€“1052\n",
      "[66/84] ë¼ì¸ 1064â€“1064\n",
      "[67/84] ë¼ì¸ 1064â€“1064\n",
      "[68/84] ë¼ì¸ 1071â€“1071\n",
      "[69/84] ë¼ì¸ 1071â€“1071\n",
      "[70/84] ë¼ì¸ 1107â€“1107\n",
      "[71/84] ë¼ì¸ 1110â€“1110\n",
      "[72/84] ë¼ì¸ 1112â€“1112\n",
      "[73/84] ë¼ì¸ 1112â€“1112\n",
      "[74/84] ë¼ì¸ 1112â€“1112\n",
      "[75/84] ë¼ì¸ 1119â€“1119\n",
      "[76/84] ë¼ì¸ 1119â€“1119\n",
      "[77/84] ë¼ì¸ 1119â€“1119\n",
      "[78/84] ë¼ì¸ 1126â€“1126\n",
      "[79/84] ë¼ì¸ 1126â€“1126\n",
      "[80/84] ë¼ì¸ 1126â€“1126\n",
      "[81/84] ë¼ì¸ 1128â€“1128\n",
      "[82/84] ë¼ì¸ 1128â€“1128\n",
      "[83/84] ë¼ì¸ 1131â€“1131\n",
      "[84/84] ë¼ì¸ 1133â€“1133\n",
      "ì €ì¥ ì™„ë£Œ: ./_build\\equations_explained.json\n"
     ]
    }
   ],
   "source": [
    "# === ì…€ 5: ìˆ˜ì‹ í•´ì„¤ ìƒì„± ===\n",
    "EXPLAIN_SYSTEM = \"You are a teacher who explains math/AI research equations in clear, simple English. Always answer politely and understandably.\"\n",
    "\n",
    "EXPLAIN_TEMPLATE = \"\"\"Please explain the following equation so that it can be understood by someone at least at a middle school level.\n",
    "Follow this exact order in your output: Example â†’ Explanation â†’ Conclusion\n",
    "\n",
    "- Example: Show the equation exactly as LaTeX in a single block (do not modify or add anything).\n",
    "- Explanation: Provide bullet points explaining the meaning of symbols (âˆ‘, ğŸ™, ^, _, âˆš, \\\\, etc.) and the role of each term, in a clear and concise way.\n",
    "- Conclusion: Summarize in one sentence the core purpose of this equation in the context of the paper (e.g., loss composition, normalization, coordinate error, probability/log-likelihood, etc.).\n",
    "- (Important) Do not change the symbols or the order of the equation, and do not invent new symbols.\n",
    "- (Important) Write only in English.\n",
    "\n",
    "[Equation]\n",
    "{EQUATION}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def explain_equation_with_llm(eq_latex:str)->str:\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\": EXPLAIN_SYSTEM},\n",
    "        {\"role\":\"user\", \"content\": EXPLAIN_TEMPLATE.format(EQUATION=eq_latex)}\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(input_ids=input_ids, **GEN_KW)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return text.split(messages[-1][\"content\"])[-1].strip()\n",
    "\n",
    "# ë°°ì¹˜ ì‹¤í–‰ (ë„ˆë¬´ ë§ìœ¼ë©´ ì—¬ëŸ¬ ë²ˆì— ë‚˜ëˆ„ì„¸ìš”)\n",
    "explanations = []\n",
    "for idx, item in enumerate(equations_advanced, start=1):\n",
    "    print(f\"[{idx}/{len(equations_advanced)}] ë¼ì¸ {item['line_start']}â€“{item['line_end']}\")\n",
    "    exp = explain_equation_with_llm(item[\"body\"])\n",
    "    explanations.append({\n",
    "        \"index\": idx,\n",
    "        \"line_start\": item[\"line_start\"],\n",
    "        \"line_end\": item[\"line_end\"],\n",
    "        \"kind\": item[\"kind\"],\n",
    "        \"env\": item[\"env\"],\n",
    "        \"equation\": item[\"body\"],\n",
    "        \"explanation\": exp\n",
    "    })\n",
    "\n",
    "# ì¤‘ê°„ ì €ì¥\n",
    "json_path = os.path.join(OUT_DIR, \"equations_explained.json\")\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"overview\": doc_overview,\n",
    "        \"items\": explanations\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "print(f\"ì €ì¥ ì™„ë£Œ: {json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d30a1c",
   "metadata": {},
   "source": [
    "### ì…€ 6. LaTeX ë¦¬í¬íŠ¸ ìƒì„±(.tex) â€” ê°œìš” + ìˆ˜ì‹ë³„ í•´ì„¤\n",
    "\n",
    "> MiKTeX(Windows) ë˜ëŠ” TeX Liveê°€ ìˆì–´ì•¼ PDF ì»´íŒŒì¼ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì—†ìœ¼ë©´ ë‹¤ìŒ ì…€ì—ì„œ ì„¤ì¹˜ ì•ˆë‚´/ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë“œë¦½ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ì…€ 6: LaTeX ë¦¬í¬íŠ¸(.tex) ë§Œë“¤ê¸° ===\n",
    "import datetime\n",
    "\n",
    "report_tex_path = os.path.join(OUT_DIR, \"yolo_math_report.tex\")\n",
    "\n",
    "def latex_escape_verbatim(s:str)->str:\n",
    "    # ì„¤ëª… í…ìŠ¤íŠ¸ ë‚´ LaTeX íŠ¹ìˆ˜ë¬¸ì ê°„ë‹¨ ì´ìŠ¤ì¼€ì´í”„\n",
    "    s = s.replace(\"\\\\\", r\"\\\\\")\n",
    "    s = s.replace(\"#\", r\"\\#\").replace(\"$\", r\"\\$\")\n",
    "    s = s.replace(\"%\", r\"\\%\").replace(\"&\", r\"\\&\")\n",
    "    s = s.replace(\"_\", r\"\\_\").replace(\"{\", r\"\\{\").replace(\"}\", r\"\\}\")\n",
    "    s = s.replace(\"^\", r\"\\^{}\").replace(\"~\", r\"\\~{}\")\n",
    "    return s\n",
    "\n",
    "def build_report(overview:str, items:List[Dict])->str:\n",
    "    header = r\"\"\"\\documentclass[11pt]{article}\n",
    "\\usepackage[margin=1in]{geometry}\n",
    "\\usepackage{amsmath, amssymb, amsfonts}\n",
    "\\usepackage{hyperref}\n",
    "\\usepackage{kotex} % Windows MiKTeXì— ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨ (ì—†ìœ¼ë©´ xelatex/xeCJKë¡œ ì»´íŒŒì¼)\n",
    "\\setlength{\\parskip}{6pt}\n",
    "\\setlength{\\parindent}{0pt}\n",
    "\\title{LaTeX ë¬¸ì„œ ìˆ˜ì‹ í•´ì„¤ ë¦¬í¬íŠ¸ (ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒ)}\n",
    "\\author{ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸}\n",
    "\\date{\"\"\" + datetime.date.today().isoformat() + r\"\"\"}\n",
    "\\begin{document}\n",
    "\\maketitle\n",
    "\\tableofcontents\n",
    "\\newpage\n",
    "\"\"\"\n",
    "    parts = [header]\n",
    "    # ê°œìš”\n",
    "    parts.append(r\"\\section*{ë¬¸ì„œ ê°œìš”}\")\n",
    "    parts.append(latex_escape_verbatim(overview))\n",
    "    parts.append(\"\\n\\\\newpage\\n\")\n",
    "\n",
    "    # ê° ìˆ˜ì‹\n",
    "    for it in items:\n",
    "        title = f\"ë¼ì¸ {it['line_start']}â€“{it['line_end']} / {it['kind']} {('['+it['env']+']') if it['env'] else ''}\"\n",
    "        parts.append(f\"\\\\section*{{{latex_escape_verbatim(title)}}}\")\n",
    "        # ì˜ˆì‹œ â†’ ì„¤ëª… â†’ ê²°ë¡  (LLM ì¶œë ¥ ê·¸ëŒ€ë¡œ ì‚½ì…, ë‹¨ ìˆ˜ì‹ ë¸”ë¡ì€ ìœ ì§€)\n",
    "        # ëª¨ë¸ ì¶œë ¥ ì¤‘ ì½”ë“œë¸”ë¡ì´ ìˆë‹¤ë©´ ì œê±°í•˜ê³  LaTeX ìˆ˜ì‹ë§Œ ë‚¨ê¸°ëŠ” ê²Œ ì•ˆì „\n",
    "        exp_txt = it[\"explanation\"]\n",
    "        parts.append(exp_txt)\n",
    "        parts.append(\"\\n\")\n",
    "\n",
    "    parts.append(\"\\\\end{document}\\n\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "report_tex = build_report(doc_overview, explanations)\n",
    "Path(report_tex_path).write_text(report_tex, encoding=\"utf-8\")\n",
    "print(f\"LaTeX ë¦¬í¬íŠ¸ ìƒì„±: {report_tex_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fc0d23",
   "metadata": {},
   "source": [
    "ë”± ì—¬ê¸° ì…€6ê¹Œì§€ë§Œ ì‹¤í–‰í•˜ê³  íŒŒì¼ í™•ì¸í•˜ì‹œë©´ ë©ë‹ˆë‹¤! 7ë²ˆë¶€í„°ëŠ” ì´ëŸ°ì €ëŸ°ê±° ê¹”ì•„ì•¼í•˜ê³  ë°©í–¥ì„±ì„ ë‹¤ì‹œ ì¡ëŠ” ì¤‘ì´ë¼..\n",
    "\n",
    "sectionë³„ë¡œ ë‚˜ë‰˜ì–´ ìˆìŠµë‹ˆë‹¤! ë“œë˜ê·¸í•´ì„œ ì§€í”¼í‹°í•œí…Œ í•œê¸€ë¡œ ë²ˆì—­. í‹€ë ¤ë„ ê·¸ëŒ€ë¡œ ì´ë ‡ê²Œ ì£¼ë¬¸í•˜ì‹œë©´ ë©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc0e2b",
   "metadata": {},
   "source": [
    "### ì…€ 7. PDF ì»´íŒŒì¼ ì‹œë„ (pdflatex â†’ xelatex ìˆœì„œ)\n",
    "\n",
    "> ë¯¸ì„¤ì¹˜ ì‹œ MiKTeX ì„¤ì¹˜ í›„ PATH ë°˜ì˜ì´ í•„ìš”í•©ë‹ˆë‹¤. ì‹¤íŒ¨í•´ë„ .texëŠ” ìƒì„±ë˜ì–´ ìˆìœ¼ë‹ˆ, ë¡œì»¬ì—ì„œ GUI(MiKTeX Console)ë¡œ ì»´íŒŒì¼í•˜ì…”ë„ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e220663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Trying: pdflatex -interaction=nonstopmode -halt-on-error yolo_math_report.tex\n",
      "ì‹¤íŒ¨ ì½”ë“œ: 127\n",
      "\n",
      "Command not found: pdflatex\n",
      ">> Trying: xelatex -interaction=nonstopmode -halt-on-error yolo_math_report.tex\n",
      "ì‹¤íŒ¨ ì½”ë“œ: 127\n",
      "\n",
      "Command not found: xelatex\n",
      "PDF ì»´íŒŒì¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. MiKTeX ë˜ëŠ” TeX Live ì„¤ì¹˜/íŒ¨í‚¤ì§€(kotex) í™•ì¸ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "# === ì…€ 7: PDF ì»´íŒŒì¼ ===\n",
    "def run_cmd(cmd:List[str])->Tuple[int,str,str]:\n",
    "    try:\n",
    "        proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, cwd=OUT_DIR, shell=False)\n",
    "        return proc.returncode, proc.stdout, proc.stderr\n",
    "    except FileNotFoundError:\n",
    "        return 127, \"\", f\"Command not found: {cmd[0]}\"\n",
    "\n",
    "def try_compile(tex_filename:str)->Optional[str]:\n",
    "    base = os.path.splitext(tex_filename)[0]\n",
    "    pdf_path = os.path.join(OUT_DIR, base + \".pdf\")\n",
    "    for engine in [[\"pdflatex\",\"-interaction=nonstopmode\",\"-halt-on-error\",tex_filename],\n",
    "                   [\"xelatex\",\"-interaction=nonstopmode\",\"-halt-on-error\",tex_filename]]:\n",
    "        print(\">> Trying:\", \" \".join(engine))\n",
    "        code,out,err = run_cmd(engine)\n",
    "        if code == 0 and Path(pdf_path).exists():\n",
    "            print(\"PDF ìƒì„± ì„±ê³µ:\", pdf_path)\n",
    "            return pdf_path\n",
    "        else:\n",
    "            print(\"ì‹¤íŒ¨ ì½”ë“œ:\", code)\n",
    "            print(out[:800])\n",
    "            print(err[:800])\n",
    "    return None\n",
    "\n",
    "pdf_path = try_compile(\"yolo_math_report.tex\")\n",
    "if pdf_path:\n",
    "    print(\"ì™„ë£Œ:\", pdf_path)\n",
    "else:\n",
    "    print(\"PDF ì»´íŒŒì¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. MiKTeX ë˜ëŠ” TeX Live ì„¤ì¹˜/íŒ¨í‚¤ì§€(kotex) í™•ì¸ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f628b",
   "metadata": {},
   "source": [
    "### ì…€ 8. ì¸ë±ìŠ¤/ë¡œê·¸ ì €ì¥ (CSV/JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ì…€ 8: ì¸ë±ìŠ¤/ë¡œê·¸ ì €ì¥ ===\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(equations_all)\n",
    "df_adv = pd.DataFrame(equations_advanced)\n",
    "\n",
    "df_path = os.path.join(OUT_DIR, \"equations_all.csv\")\n",
    "df_adv_path = os.path.join(OUT_DIR, \"equations_advanced.csv\")\n",
    "df.to_csv(df_path, index=False, encoding=\"utf-8-sig\")\n",
    "df_adv.to_csv(df_adv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"ì €ì¥ ì™„ë£Œ:\")\n",
    "print(\" - ì „ì²´ ìˆ˜ì‹ CSV :\", df_path)\n",
    "print(\" - ê³ ë‚œë„ ìˆ˜ì‹ CSV:\", df_adv_path)\n",
    "\n",
    "# ê°œìš”/ì„¤ëª… ìš”ì•½ë³¸\n",
    "summary_md = os.path.join(OUT_DIR, \"README_report_summary.md\")\n",
    "with open(summary_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# ë¬¸ì„œ ê°œìš”\\n\\n\")\n",
    "    f.write(doc_overview + \"\\n\\n\")\n",
    "    f.write(\"## ìˆ˜ì‹ í†µê³„\\n\")\n",
    "    f.write(f\"- ì „ì²´ ìˆ˜ì‹: {len(equations_all)}ê°œ\\n\")\n",
    "    f.write(f\"- ê³ ë‚œë„ ìˆ˜ì‹: {len(equations_advanced)}ê°œ\\n\")\n",
    "print(\" - ìš”ì•½ MD       :\", summary_md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
