{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1c5a43",
   "metadata": {},
   "source": [
    "# ì½”ë“œì„¤ëª…\n",
    "Texë¥¼ ë°›ì•„ì™€ì„œ ìˆ˜ì‹ë§Œ ë½‘ì•„ë‚¸ í›„\n",
    "\n",
    "ì¤‘í•™ìƒ ì´ìƒì˜ ìˆ˜ì¤€ì„ ìš”êµ¬í•˜ëŠ” ìˆ˜ì‹ë§Œ í•´ì„¤\n",
    "\n",
    "_build í´ë” ì•ˆì— .texíŒŒì¼ë¡œ ìƒê¸´ë‹¤!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6b2df",
   "metadata": {},
   "source": [
    "### ì…€ 1. í™˜ê²½ ì¤€ë¹„ & ëª¨ë¸ ë¡œë“œ (4-bit, CUDA ìë™ ê°ì§€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c65b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch file: c:\\POLO\\venv\\Lib\\site-packages\\torch\\__init__.py\n",
      "torch version: 2.5.1+cu121\n",
      "pip sees torch: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch, importlib.metadata as im\n",
    "print(\"torch file:\", torch.__file__)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"pip sees torch:\", im.version(\"torch\"))\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd4be5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9\n",
      "PyTorch: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Device selected: cuda\n",
      "Model & tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "# === ì…€ 1: í™˜ê²½ ì¤€ë¹„ & ëª¨ë¸ ë¡œë“œ ===\n",
    "import os, torch, sys, platform, json, re, textwrap, subprocess, shutil\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# ----- ê¸°ë³¸ ê²½ë¡œ ì„¤ì • -----\n",
    "# INPUT_TEX_PATH = r\"C:\\POLO\\polo-system\\models\\math\\iclr2022_conference.tex\"        # <- ë¶„ì„í•  LaTeX íŒŒì¼ ê²½ë¡œ\n",
    "INPUT_TEX_PATH = r\"C:\\POLO\\polo-system\\models\\math\\yolo.tex\"        # <- ë¶„ì„í•  LaTeX íŒŒì¼ ê²½ë¡œ\n",
    "OUT_DIR        = \"C:/POLO/polo-system/models/math./_build\"          # ì‚°ì¶œë¬¼ í´ë” ê²½ë¡œ\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----- ëª¨ë¸/í† í¬ë‚˜ì´ì € ì„¤ì • -----\n",
    "MODEL_ID = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"   # 1.5B ìˆ˜í•™ íŠ¹í™” instruct\n",
    "USE_4BIT = True                                 # 8GB VRAMì„ ê³ ë ¤í•œ 4-bit ì–‘ìí™”\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Device selected: {DEVICE}\")\n",
    "\n",
    "# ----- ëª¨ë¸ ë¡œë“œ -----\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "bnb_config = None\n",
    "if USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True \n",
    ")\n",
    "\n",
    "# ê³µí†µ generate ì„¤ì •\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"Model & tokenizer loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dcc565",
   "metadata": {},
   "source": [
    "### ì…€ 2. LaTeX íŒŒì„œ: ìˆ˜ì‹ ì¶”ì¶œ(ì¸ë¼ì¸/ë””ìŠ¤í”Œë ˆì´/í™˜ê²½) + ë¼ì¸ë²ˆí˜¸ ë§¤í•‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134f4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ìˆ˜ì‹ ê°œìˆ˜: 66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'kind': 'inline($ $)',\n",
       "  'env': '',\n",
       "  'start': 1801,\n",
       "  'end': 1805,\n",
       "  'line_start': 63,\n",
       "  'line_end': 63,\n",
       "  'body': '^*'},\n",
       " {'kind': 'inline($ $)',\n",
       "  'env': '',\n",
       "  'start': 1822,\n",
       "  'end': 1833,\n",
       "  'line_start': 63,\n",
       "  'line_end': 63,\n",
       "  'body': '^{* \\\\dag}'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === ì…€ 2: LaTeX ìˆ˜ì‹ ì¶”ì¶œ ìœ í‹¸ ===\n",
    "from pathlib import Path\n",
    "\n",
    "assert Path(INPUT_TEX_PATH).exists(), f\"ì…ë ¥ TeX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_TEX_PATH}\"\n",
    "src = Path(INPUT_TEX_PATH).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "# ë¼ì¸ ì˜¤í”„ì…‹ ì¸ë±ìŠ¤\n",
    "lines = src.splitlines()\n",
    "offsets = []\n",
    "pos = 0\n",
    "for ln in lines:\n",
    "    offsets.append(pos)\n",
    "    pos += len(ln) + 1\n",
    "\n",
    "def pos_to_line(p:int)->int:\n",
    "    lo, hi = 0, len(offsets)-1\n",
    "    while lo <= hi:\n",
    "        mid = (lo+hi)//2\n",
    "        if offsets[mid] <= p:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            hi = mid - 1\n",
    "    return hi + 1  # 1-based\n",
    "\n",
    "def extract_equations(tex:str)->List[Dict]:\n",
    "    matches = []\n",
    "    def add(kind, start, end, body, env=\"\"):\n",
    "        matches.append({\n",
    "            \"kind\": kind, \"env\": env, \"start\": start, \"end\": end,\n",
    "            \"line_start\": pos_to_line(start), \"line_end\": pos_to_line(end),\n",
    "            \"body\": body.strip()\n",
    "        })\n",
    "    # $$ ... $$\n",
    "    for m in re.finditer(r\"\\$\\$(.+?)\\$\\$\", tex, flags=re.DOTALL):\n",
    "        add(\"display($$ $$)\", m.start(), m.end(), m.group(1))\n",
    "    # \\[ ... \\]\n",
    "    for m in re.finditer(r\"\\\\\\[(.+?)\\\\\\]\", tex, flags=re.DOTALL):\n",
    "        add(\"display(\\\\[ \\\\])\", m.start(), m.end(), m.group(1))\n",
    "    # \\( ... \\)\n",
    "    for m in re.finditer(r\"\\\\\\((.+?)\\\\\\)\", tex, flags=re.DOTALL):\n",
    "        add(\"inline(\\\\( \\\\))\", m.start(), m.end(), m.group(1))\n",
    "    # inline $...$ (ë‹¨, $$ ì œì™¸)\n",
    "    for m in re.finditer(r\"(?<!\\$)\\$(?!\\$)(.+?)(?<!\\$)\\$(?!\\$)\", tex, flags=re.DOTALL):\n",
    "        add(\"inline($ $)\", m.start(), m.end(), m.group(1))\n",
    "    # environments\n",
    "    envs = [\"equation\",\"equation*\",\"align\",\"align*\",\"multline\",\"multline*\",\"gather\",\"gather*\",\"flalign\",\"flalign*\",\"eqnarray\",\"eqnarray*\",\"split\"]\n",
    "    for env in envs:\n",
    "        pattern = rf\"\\\\begin{{{re.escape(env)}}}(.+?)\\\\end{{{re.escape(env)}}}\"\n",
    "        for m in re.finditer(pattern, tex, flags=re.DOTALL):\n",
    "            add(f\"env\", m.start(), m.end(), m.group(1), env=env)\n",
    "    # ì¤‘ë³µ(ë™ì¼ ë²”ìœ„) ì œê±° ë° ì •ë ¬\n",
    "    uniq = {}\n",
    "    for it in matches:\n",
    "        key = (it[\"start\"], it[\"end\"])\n",
    "        if key not in uniq:\n",
    "            uniq[key] = it\n",
    "    out = list(uniq.values())\n",
    "    out.sort(key=lambda x: x[\"start\"])\n",
    "    return out\n",
    "\n",
    "equations_all = extract_equations(src)\n",
    "print(f\"ì´ ìˆ˜ì‹ ê°œìˆ˜: {len(equations_all)}\")\n",
    "equations_all[:2]  # ë¯¸ë¦¬ë³´ê¸°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8992e",
   "metadata": {},
   "source": [
    "### ì…€ 3. â€œì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒâ€ íŒë³„ íœ´ë¦¬ìŠ¤í‹±\n",
    "\n",
    "> ê¸°ë³¸ì ìœ¼ë¡œ âˆ‘, âˆ‚, argmax, ë¶„ìˆ˜Â·ì œê³±ê·¼ì˜ ì¤‘ì²©, ì¢Œí‘œ/ë°•ìŠ¤ ì§€ì‹œì ë“± ì¡°ê¸ˆ ë³µí•©ì ì¸ í‘œê¸°ê°€ ìˆìœ¼ë©´ ìƒìœ„ ë‚œì´ë„ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤. í”„ë¡œì íŠ¸ì— ë§ê²Œ ê·œì¹™ì„ ì‰½ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312366e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ ìˆ˜ì‹: 19 / 66\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "LaTeX ìˆ˜ì‹ í•´ì„¤ API (FastAPI)\n",
    "- uvicorn --reload app:app ë¡œ ì‹¤í–‰\n",
    "- GET  /health                         : ìƒíƒœ ì²´í¬\n",
    "- GET  /count/{file_path:path}         : ìˆ˜ì‹ ê°œìˆ˜ë§Œ ì„¸ê¸°(ê°„ë‹¨ í™•ì¸ìš©)\n",
    "- POST /count                          : JSON {\"path\": \"C:\\\\...\\\\yolo.tex\"}\n",
    "- GET  /math/{file_path:path}          : íŒŒì¼ ê²½ë¡œë¥¼ URL pathë¡œ ë„˜ê²¨ ì‹¤í–‰\n",
    "- POST /math                           : JSON {\"path\": \"C:\\\\...\\\\yolo.tex\"}\n",
    "\n",
    "ì°¸ê³  ì‚¬í•­\n",
    "- ì½˜ì†” ì¶œë ¥ì´ ë°”ë¡œ ë³´ì´ë„ë¡ stdout ë¼ì¸ ë²„í¼ë§ + flush=True ì ìš©\n",
    "- pad==eos ê²½ê³  ë°©ì§€ë¥¼ ìœ„í•´ pad í† í° ì¶”ê°€ + attention_mask ëª…ì‹œ\n",
    "\"\"\"\n",
    "\n",
    "# === ì…€ 1: í™˜ê²½ ì¤€ë¹„ & ëª¨ë¸ ë¡œë“œ ===\n",
    "import os, torch, sys, json, re, textwrap, datetime\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# stdout ë¼ì¸ ë²„í¼ë§ (print ì¦‰ì‹œ í‘œì‹œ)\n",
    "try:\n",
    "    sys.stdout.reconfigure(line_buffering=True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "VERSION = \"POLO-Math-API v3 (flush+mask+pad)\"; print(VERSION, flush=True)\n",
    "\n",
    "# ----- ê¸°ë³¸ ê²½ë¡œ ì„¤ì • -----\n",
    "INPUT_TEX_PATH = r\"C:\\\\POLO\\\\polo-system\\\\models\\\\math\\\\yolo.tex\"  # ì˜ˆì‹œ ê²½ë¡œ\n",
    "OUT_DIR        = \"C:/POLO/polo-system/models/math/_build\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----- ëª¨ë¸/í† í¬ë‚˜ì´ì € ì„¤ì • -----\n",
    "MODEL_ID = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "USE_4BIT = False\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\", flush=True)\n",
    "print(f\"PyTorch: {torch.__version__}\", flush=True)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\", flush=True)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\", flush=True)\n",
    "print(f\"Device selected: {DEVICE}\", flush=True)\n",
    "\n",
    "# ----- ëª¨ë¸ ë¡œë“œ -----\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "    # pad_token ê²½ê³  ì™„í™”: pad ì—†ê±°ë‚˜ eosì™€ ê°™ìœ¼ë©´ [PAD] ì¶”ê°€\n",
    "    PAD_ADDED = False\n",
    "    if tokenizer.pad_token_id is None or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        PAD_ADDED = True\n",
    "\n",
    "    bnb_config = None\n",
    "    if USE_4BIT:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "        quantization_config=bnb_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    if PAD_ADDED:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # ê³µí†µ generate ì„¤ì •\n",
    "    GEN_KW = dict(\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    print(\"Model & tokenizer loaded.\", flush=True)\n",
    "except Exception as e:\n",
    "    tokenizer = None\n",
    "    model = None\n",
    "    GEN_KW = {}\n",
    "    print(\"[Model Load Error]\", e, flush=True)\n",
    "\n",
    "\n",
    "# === ê³µìš© ìœ í‹¸: ë¼ì¸ ìœ„ì¹˜ ê³„ì‚° ===\n",
    "def make_line_offsets(text: str) -> List[int]:\n",
    "    lines = text.splitlines()\n",
    "    offsets, pos = [], 0\n",
    "    for ln in lines:\n",
    "        offsets.append(pos)\n",
    "        pos += len(ln) + 1  # '\\n'\n",
    "    return offsets\n",
    "\n",
    "def build_pos_to_line(offsets: List[int]):\n",
    "    def pos_to_line(p: int) -> int:\n",
    "        lo, hi = 0, len(offsets)-1\n",
    "        while lo <= hi:\n",
    "            mid = (lo+hi)//2\n",
    "            if offsets[mid] <= p:\n",
    "                lo = mid + 1\n",
    "            else:\n",
    "                hi = mid - 1\n",
    "        return hi + 1  # 1-based\n",
    "    return pos_to_line\n",
    "\n",
    "\n",
    "# === ì…€ 2: LaTeX ìˆ˜ì‹ ì¶”ì¶œ ìœ í‹¸ ===\n",
    "def extract_equations(tex: str, pos_to_line) -> List[Dict]:\n",
    "    matches: List[Dict] = []\n",
    "\n",
    "    def add(kind, start, end, body, env=\"\"):\n",
    "        matches.append({\n",
    "            \"kind\": kind, \"env\": env, \"start\": start, \"end\": end,\n",
    "            \"line_start\": pos_to_line(start), \"line_end\": pos_to_line(end),\n",
    "            \"body\": body.strip()\n",
    "        })\n",
    "\n",
    "    # $$ ... $$\n",
    "    for m in re.finditer(r\"\\$\\$(.+?)\\$\\$\", tex, flags=re.DOTALL):\n",
    "        add(\"display($$ $$)\", m.start(), m.end(), m.group(1))\n",
    "    # \\[ ... \\]\n",
    "    for m in re.finditer(r\"\\\\\\[(.+?)\\\\\\]\", tex, flags=re.DOTALL):\n",
    "        add(\"display(\\\\[ \\\\])\", m.start(), m.end(), m.group(1))\n",
    "    # \\( ... \\)\n",
    "    for m in re.finditer(r\"\\\\\\((.+?)\\\\\\)\", tex, flags=re.DOTALL):\n",
    "        add(\"inline(\\\\( \\\\))\", m.start(), m.end(), m.group(1))\n",
    "    # inline $...$ (ë‹¨, $$ ì œì™¸)\n",
    "    for m in re.finditer(r\"(?<!\\$)\\$(?!\\$)(.+?)(?<!\\$)\\$(?!\\$)\", tex, flags=re.DOTALL):\n",
    "        add(\"inline($ $)\", m.start(), m.end(), m.group(1))\n",
    "    # environments\n",
    "    envs = [\"equation\",\"equation*\",\"align\",\"align*\",\"multline\",\"multline*\",\n",
    "            \"gather\",\"gather*\",\"flalign\",\"flalign*\",\"eqnarray\",\"eqnarray*\",\"split\"]\n",
    "    for env in envs:\n",
    "        pattern = rf\"\\\\begin{{{re.escape(env)}}}(.+?)\\\\end{{{re.escape(env)}}}\"\n",
    "        for m in re.finditer(pattern, tex, flags=re.DOTALL):\n",
    "            add(\"env\", m.start(), m.end(), m.group(1), env=env)\n",
    "\n",
    "    # ì¤‘ë³µ(ë™ì¼ ë²”ìœ„) ì œê±° ë° ì •ë ¬\n",
    "    uniq = {}\n",
    "    for it in matches:\n",
    "        key = (it[\"start\"], it[\"end\"])\n",
    "        if key not in uniq:\n",
    "            uniq[key] = it\n",
    "    out = list(uniq.values())\n",
    "    out.sort(key=lambda x: x[\"start\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "# === ì…€ 3: ë‚œì´ë„ íœ´ë¦¬ìŠ¤í‹± ì •ì˜ (ìš”ì²­í•˜ì‹  ë²„ì „ ë°˜ì˜) ===\n",
    "ADV_TOKENS = [\n",
    "    r\"\\\\sum\", r\"\\\\prod\", r\"\\\\int\", r\"\\\\lim\", r\"\\\\nabla\", r\"\\\\partial\",\n",
    "    r\"\\\\mathbb\", r\"\\\\mathcal\", r\"\\\\mathbf\", r\"\\\\boldsymbol\",\n",
    "    r\"\\\\argmax\", r\"\\\\argmin\", r\"\\\\operatorname\", r\"\\\\mathrm\\{KL\\}\",\n",
    "    r\"\\\\mathbb\\{E\\}\", r\"\\\\Pr\", r\"\\\\sigma\", r\"\\\\mu\", r\"\\\\Sigma\", r\"\\\\theta\",\n",
    "    r\"\\\\frac\\{[^{}]*\\{[^{}]*\\}[^{}]*\\}\",  # ì¤‘ì²© ë¶„ìˆ˜\n",
    "    r\"\\\\hat\\{\", r\"\\\\tilde\\{\", r\"\\\\bar\\{\", r\"\\\\widehat\\{\", r\"\\\\widetilde\\{\",\n",
    "    r\"\\\\sqrt\\{[^{}]*\\{\",                   # ì¤‘ì²© sqrt\n",
    "    r\"\\\\left\", r\"\\\\right\",\n",
    "    r\"\\\\in\", r\"\\\\subset\", r\"\\\\forall\", r\"\\\\exists\",\n",
    "    r\"\\\\cdot\", r\"\\\\times\", r\"\\\\otimes\",\n",
    "    r\"IoU\", r\"\\\\log\", r\"\\\\exp\",\n",
    "    r\"\\\\mathbb\\{R\\}\", r\"\\\\mathbb\\{N\\}\", r\"\\\\mathbb\\{Z\\}\",\n",
    "    r\"\\\\Delta\", r\"\\\\delta\", r\"\\\\epsilon\", r\"\\\\varepsilon\",\n",
    "]\n",
    "ADV_RE = re.compile(\"|\".join(ADV_TOKENS))\n",
    "\n",
    "def count_subscripts(expr: str) -> int:\n",
    "    return len(re.findall(r\"_[a-zA-Z0-9{\\\\]\", expr))\n",
    "\n",
    "def is_advanced(eq: str) -> bool:\n",
    "    if ADV_RE.search(eq):\n",
    "        return True\n",
    "    if len(eq) > 40 and count_subscripts(eq) >= 2:\n",
    "        return True\n",
    "    if \"\\n\" in eq and len(eq) > 30:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# === ì…€ 4: ë¬¸ì„œ ì „ì²´ ê°œìš” ìƒì„± (ëª¨ë¸ ìš”ì•½) ===\n",
    "def take_slices(text: str, head_chars=4000, mid_chars=2000, tail_chars=4000):\n",
    "    n = len(text)\n",
    "    head = text[:min(head_chars, n)]\n",
    "    mid_start = max((n // 2) - (mid_chars // 2), 0)\n",
    "    mid = text[mid_start: mid_start + min(mid_chars, n)]\n",
    "    tail = text[max(0, n - tail_chars):]\n",
    "    return head, mid, tail\n",
    "\n",
    "def _generate_with_mask_from_messages(messages: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    attention_maskë¥¼ ëª…ì‹œí•´ ê²½ê³  ë°©ì§€ ë° ì¼ê´€ ë™ì‘ ë³´ì¥.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    # pad_token_idê°€ ì •ì˜ë˜ì–´ ìˆìœ¼ë¯€ë¡œ maskë¥¼ ì •í™•íˆ ìƒì„±\n",
    "    attention_mask = (inputs != tokenizer.pad_token_id).long()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids=inputs.to(model.device),\n",
    "            attention_mask=attention_mask.to(model.device),\n",
    "            **GEN_KW\n",
    "        )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def chat(prompt: str) -> str:\n",
    "    if tokenizer is None or model is None:\n",
    "        raise RuntimeError(\"Model is not loaded.\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ AI ë…¼ë¬¸/ìˆ˜ì‹ì„ í•œêµ­ì–´ë¡œ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ì„ ìƒë‹˜ì…ë‹ˆë‹¤. í•­ìƒ ì¡´ëŒ“ë§ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\"},\n",
    "        {\"role\": \"user\",   \"content\": prompt}\n",
    "    ]\n",
    "    text = _generate_with_mask_from_messages(messages)\n",
    "    return text.split(messages[-1][\"content\"])[-1].strip()\n",
    "\n",
    "\n",
    "# === ì…€ 5: ìˆ˜ì‹ í•´ì„¤ ìƒì„± ===\n",
    "EXPLAIN_SYSTEM = \"You are a teacher who explains math/AI research equations in clear, simple English. Always answer politely and understandably.\"\n",
    "EXPLAIN_TEMPLATE = (\n",
    "    \"\"\"Please explain the following equation so that it can be understood by someone at least at a middle school level.\n",
    "Follow this exact order in your output: Example â†’ Explanation â†’ Conclusion\n",
    "\n",
    "- Example: Show the equation exactly as LaTeX in a single block (do not modify or add anything).\n",
    "- Explanation: Provide bullet points explaining the meaning of symbols (âˆ‘, ğŸ™, ^, _, âˆš, \\\\, etc.) and the role of each term, in a clear and concise way.\n",
    "- Conclusion: Summarize in one sentence the core purpose of this equation in the context of the paper (e.g., loss composition, normalization, coordinate error, probability/log-likelihood, etc.).\n",
    "- (Important) Do not change the symbols or the order of the equation, and do not invent new symbols.\n",
    "- (Important) Write only in English.\n",
    "\n",
    "[Equation]\n",
    "{EQUATION}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def explain_equation_with_llm(eq_latex: str) -> str:\n",
    "    if tokenizer is None or model is None:\n",
    "        raise RuntimeError(\"Model is not loaded.\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": EXPLAIN_SYSTEM},\n",
    "        {\"role\": \"user\",   \"content\": EXPLAIN_TEMPLATE.format(EQUATION=eq_latex)}\n",
    "    ]\n",
    "    text = _generate_with_mask_from_messages(messages)\n",
    "    return text.split(messages[-1][\"content\"])[:-1][-1].strip() if messages[-1][\"content\"] in text else text\n",
    "\n",
    "\n",
    "# === ì…€ 6: LaTeX ë¦¬í¬íŠ¸(.tex) ë§Œë“¤ê¸° ===\n",
    "def latex_escape_verbatim(s: str) -> str:\n",
    "    s = s.replace(\"\\\\\", r\"\\\\\")\n",
    "    s = s.replace(\"#\", r\"\\#\").replace(\"$\", r\"\\$\")\n",
    "    s = s.replace(\"%\", r\"\\%\").replace(\"&\", r\"\\&\")\n",
    "    s = s.replace(\"_\", r\"\\_\").replace(\"{\", r\"\\{\").replace(\"}\", r\"\\}\")\n",
    "    s = s.replace(\"^\", r\"\\^{}\").replace(\"~\", r\"\\~{}\")\n",
    "    return s\n",
    "\n",
    "def build_report(overview: str, items: List[Dict]) -> str:\n",
    "    header = (r\"\"\"\\\\documentclass[11pt]{article}\n",
    "\\\\usepackage[margin=1in]{geometry}\n",
    "\\\\usepackage{amsmath, amssymb, amsfonts}\n",
    "\\\\usepackage{hyperref}\n",
    "\\\\usepackage{kotex} % Windows MiKTeXì— ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨ (ì—†ìœ¼ë©´ xelatex/xeCJKë¡œ ì»´íŒŒì¼)\n",
    "\\\\setlength{\\\\parskip}{6pt}\n",
    "\\\\setlength{\\\\parindent}{0pt}\n",
    "\\\\title{LaTeX ë¬¸ì„œ ìˆ˜ì‹ í•´ì„¤ ë¦¬í¬íŠ¸ (ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒ)}\n",
    "\\\\author{ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸}\n",
    "\\\\date{\"\"\" + datetime.date.today().isoformat() + r\"\"\"}\n",
    "\\\\begin{document}\n",
    "\\\\maketitle\n",
    "\\\\tableofcontents\n",
    "\\\\newpage\n",
    "\"\"\")\n",
    "    parts = [header]\n",
    "    parts.append(r\"\\\\section*{ë¬¸ì„œ ê°œìš”}\")\n",
    "    parts.append(latex_escape_verbatim(overview))\n",
    "    parts.append(\"\\n\\\\newpage\\n\")\n",
    "\n",
    "    for it in items:\n",
    "        title = f\"ë¼ì¸ {it['line_start']}â€“{it['line_end']} / {it['kind']} {('['+it['env']+']') if it['env'] else ''}\"\n",
    "        parts.append(f\"\\\\section*{{{latex_escape_verbatim(title)}}}\")\n",
    "        parts.append(it[\"explanation\"])  # LLM ì¶œë ¥ ê·¸ëŒ€ë¡œ ì‚½ì…\n",
    "        parts.append(\"\\n\")\n",
    "\n",
    "    parts.append(\"\\\\end{document}\\n\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "# === ìƒˆë¡œ ì¶”ê°€: ìˆ˜ì‹ ê°œìˆ˜ë§Œ ì„¸ê¸° ===\n",
    "def count_equations_only(input_tex_path: str) -> Dict[str, int]:\n",
    "    p = Path(input_tex_path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"ì…ë ¥ TeX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_tex_path}\")\n",
    "\n",
    "    src = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    offsets = make_line_offsets(src)\n",
    "    pos_to_line = build_pos_to_line(offsets)\n",
    "\n",
    "    equations_all = extract_equations(src, pos_to_line)\n",
    "    equations_advanced = [e for e in equations_all if is_advanced(e[\"body\"])]\n",
    "\n",
    "    # ì½˜ì†” ì¶œë ¥ (ì¦‰ì‹œ í‘œì‹œ)\n",
    "    print(f\"ì´ ìˆ˜ì‹ ê°œìˆ˜: {len(equations_all)}\", flush=True)\n",
    "    print(f\"ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ ìˆ˜ì‹: {len(equations_advanced)} / {len(equations_all)}\", flush=True)\n",
    "\n",
    "    return {\n",
    "        \"equations_total\": len(equations_all),\n",
    "        \"equations_advanced\": len(equations_advanced)\n",
    "    }\n",
    "\n",
    "\n",
    "# === íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ===\n",
    "def run_pipeline(input_tex_path: str) -> Dict:\n",
    "    p = Path(input_tex_path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"ì…ë ¥ TeX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_tex_path}\")\n",
    "\n",
    "    Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    src = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    offsets = make_line_offsets(src)\n",
    "    pos_to_line = build_pos_to_line(offsets)\n",
    "\n",
    "    # 1) ìˆ˜ì‹ ì¶”ì¶œ & ê³ ë‚œë„ ë¶„ë¥˜\n",
    "    equations_all = extract_equations(src, pos_to_line)\n",
    "    equations_advanced = [e for e in equations_all if is_advanced(e[\"body\"])]\n",
    "\n",
    "    # ì½˜ì†” ìš”ì•½ ì¶œë ¥ (ì¦‰ì‹œ í‘œì‹œ)\n",
    "    print(f\"ì´ ìˆ˜ì‹ ê°œìˆ˜: {len(equations_all)}\", flush=True)\n",
    "    print(f\"ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ ìˆ˜ì‹: {len(equations_advanced)} / {len(equations_all)}\", flush=True)\n",
    "\n",
    "    # 2) ë¬¸ì„œ ê°œìš” ìš”ì•½\n",
    "    head, mid, tail = take_slices(src)\n",
    "    overview_prompt = textwrap.dedent(f\"\"\"\n",
    "    ë‹¤ìŒ LaTeX ë¬¸ì„œì˜ ì•/ì¤‘/ë’¤ ì¼ë¶€ë¥¼ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.\n",
    "    - í•µì‹¬ ì£¼ì œ/ëª©í‘œë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•˜ì‹œê³ ,\n",
    "    - ì£¼ìš” ì„¹ì…˜(ìˆë‹¤ë©´)ì„ ë¶ˆë¦¿ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\n",
    "    - ìˆ˜í•™ í‘œê¸°/ê¸°í˜¸ í•´ì„ì— ì´ˆì ì„ ë§ì¶°, ì „ì²´ íë¦„ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
    "    - í•­ìƒ í•œêµ­ì–´ ì¡´ëŒ“ë§ë¡œ, ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ.\n",
    "\n",
    "    [ì•ë¶€ë¶„]\n",
    "    {head}\n",
    "\n",
    "    [ì¤‘ê°„]\n",
    "    {mid}\n",
    "\n",
    "    [ë’·ë¶€ë¶„]\n",
    "    {tail}\n",
    "    \"\"\").strip()\n",
    "    doc_overview = chat(overview_prompt)\n",
    "\n",
    "    # 3) ìˆ˜ì‹ë³„ í•´ì„¤ ìƒì„±\n",
    "    explanations: List[Dict] = []\n",
    "    for idx, item in enumerate(equations_advanced, start=1):\n",
    "        print(f\"[{idx}/{len(equations_advanced)}] ë¼ì¸ {item['line_start']}â€“{item['line_end']}\", flush=True)\n",
    "        exp = explain_equation_with_llm(item[\"body\"])\n",
    "        explanations.append({\n",
    "            \"index\": idx,\n",
    "            \"line_start\": item[\"line_start\"],\n",
    "            \"line_end\": item[\"line_end\"],\n",
    "            \"kind\": item[\"kind\"],\n",
    "            \"env\": item[\"env\"],\n",
    "            \"equation\": item[\"body\"],\n",
    "            \"explanation\": exp\n",
    "        })\n",
    "\n",
    "    # 4) JSON ì €ì¥\n",
    "    json_path = os.path.join(OUT_DIR, \"equations_explained.json\")\n",
    "    Path(json_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"overview\": doc_overview, \"items\": explanations}, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"ì €ì¥ ì™„ë£Œ(JSON): {json_path}\", flush=True)\n",
    "\n",
    "    # 5) LaTeX ë¦¬í¬íŠ¸(.tex) ìƒì„±\n",
    "    report_tex_path = os.path.join(OUT_DIR, \"yolo_math_report.tex\")\n",
    "    report_tex = build_report(doc_overview, explanations)\n",
    "    Path(report_tex_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(report_tex_path).write_text(report_tex, encoding=\"utf-8\")\n",
    "    print(f\"ì €ì¥ ì™„ë£Œ(TeX): {report_tex_path}\", flush=True)\n",
    "\n",
    "    return {\n",
    "        \"input\": str(p),\n",
    "        \"counts\": {\n",
    "            \"equations_total\": len(equations_all),\n",
    "            \"equations_advanced\": len(equations_advanced)\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"json\": json_path,\n",
    "            \"report_tex\": report_tex_path,\n",
    "            \"out_dir\": OUT_DIR\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# === FastAPI ì•± ì •ì˜ ===\n",
    "app = FastAPI(title=\"POLO Math Explainer API\", version=\"1.0.0\")\n",
    "\n",
    "class MathRequest(BaseModel):\n",
    "    path: str\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"torch\": torch.__version__,\n",
    "        \"cuda\": torch.cuda.is_available(),\n",
    "        \"device\": DEVICE,\n",
    "        \"model_loaded\": (tokenizer is not None and model is not None)\n",
    "    }\n",
    "\n",
    "# ìˆ˜ì‹ ê°œìˆ˜ë§Œ ì„¸ëŠ” ì—”ë“œí¬ì¸íŠ¸ (GET/POST)\n",
    "@app.get(\"/count/{file_path:path}\")\n",
    "async def count_get(file_path: str):\n",
    "    try:\n",
    "        return count_equations_only(file_path)\n",
    "    except FileNotFoundError as e:\n",
    "        raise HTTPException(status_code=404, detail=str(e))\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "@app.post(\"/count\")\n",
    "async def count_post(req: MathRequest):\n",
    "    try:\n",
    "        return count_equations_only(req.path)\n",
    "    except FileNotFoundError as e:\n",
    "        raise HTTPException(status_code=404, detail=str(e))\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "@app.post(\"/math\")\n",
    "async def math_post(req: MathRequest):\n",
    "    try:\n",
    "        return run_pipeline(req.path)\n",
    "    except FileNotFoundError as e:\n",
    "        raise HTTPException(status_code=404, detail=str(e))\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "@app.get(\"/math/{file_path:path}\")\n",
    "async def math_get(file_path: str):\n",
    "    try:\n",
    "        return run_pipeline(file_path)\n",
    "    except FileNotFoundError as e:\n",
    "        raise HTTPException(status_code=404, detail=str(e))\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# ë¡œì»¬ì—ì„œ python app.py ë¡œ ì‹¤í–‰í•  ë•Œ í¸ì˜ë¥¼ ìœ„í•œ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(\"app:app\", host=\"127.0.0.1\", port=8000, reload=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea70d70b",
   "metadata": {},
   "source": [
    "### ì…€ 4. â€œë¬¸ì„œ ì „ì²´ ì´í•´â€ ìš”ì•½(ê°œìš”/ì„¹ì…˜ ìš”ì§€) ìƒì„±\n",
    "\n",
    "> .tex ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ ë„£ê¸°ì—” ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í† ë§‰ ìš”ì•½ ë°©ì‹(ì•/ì¤‘/ë’¤ ìƒ˜í”Œë§)ìœ¼ë¡œ ê°œìš”ë¥¼ ë½‘ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ebd4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": [
    "# === ì…€ 4: ë¬¸ì„œ ì „ì²´ ê°œìš” ìƒì„± (ëª¨ë¸ ìš”ì•½) ===\n",
    "def take_slices(text:str, head_chars=4000, mid_chars=2000, tail_chars=4000):\n",
    "    n = len(text)\n",
    "    head = text[:min(head_chars, n)]\n",
    "    mid_start = max((n//2) - (mid_chars//2), 0)\n",
    "    mid = text[mid_start: mid_start + min(mid_chars, n)]\n",
    "    tail = text[max(0, n - tail_chars):]\n",
    "    return head, mid, tail\n",
    "\n",
    "head, mid, tail = take_slices(src)\n",
    "\n",
    "def chat(prompt:str)->str:\n",
    "    # Qwen2.5 Instruct í¬ë§· ê°„ë‹¨ ì‚¬ìš©\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\":\"ë‹¹ì‹ ì€ AI ë…¼ë¬¸/ìˆ˜ì‹ì„ í•œêµ­ì–´ë¡œ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ì„ ìƒë‹˜ì…ë‹ˆë‹¤. í•­ìƒ ì¡´ëŒ“ë§ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\"},\n",
    "        {\"role\":\"user\", \"content\": prompt}\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(input_ids=input_ids, **GEN_KW)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    # chat í…œí”Œë¦¿ ì ‘ë‘ë¶€ ì œê±°\n",
    "    return text.split(messages[-1][\"content\"])[-1].strip()\n",
    "\n",
    "overview_prompt = textwrap.dedent(f\"\"\"\n",
    "ë‹¤ìŒ LaTeX ë¬¸ì„œì˜ ì•/ì¤‘/ë’¤ ì¼ë¶€ë¥¼ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.\n",
    "- í•µì‹¬ ì£¼ì œ/ëª©í‘œë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•˜ì‹œê³ ,\n",
    "- ì£¼ìš” ì„¹ì…˜(ìˆë‹¤ë©´)ì„ ë¶ˆë¦¿ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\n",
    "- ìˆ˜í•™ í‘œê¸°/ê¸°í˜¸ í•´ì„ì— ì´ˆì ì„ ë§ì¶°, ì „ì²´ íë¦„ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
    "- í•­ìƒ í•œêµ­ì–´ ì¡´ëŒ“ë§ë¡œ, ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ.\n",
    "\n",
    "[ì•ë¶€ë¶„]\n",
    "{head}\n",
    "\n",
    "[ì¤‘ê°„]\n",
    "{mid}\n",
    "\n",
    "[ë’·ë¶€ë¶„]\n",
    "{tail}\n",
    "\"\"\").strip()\n",
    "\n",
    "doc_overview = chat(overview_prompt)\n",
    "print(doc_overview[:1200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2a3a4",
   "metadata": {},
   "source": [
    "### ì…€ 5. ìˆ˜ì‹ í•´ì„¤(ì˜ˆì‹œ â†’ ì„¤ëª… â†’ ê²°ë¡ ) ìƒì„± í•¨ìˆ˜ + ë°°ì¹˜ ì‹¤í–‰\n",
    "\n",
    "> ê° ìˆ˜ì‹ì€ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì œì‹œí•˜ê³ , ìƒì§•/ê¸°í˜¸ì˜ ì˜ë¯¸ì™€ YOLO ë§¥ë½(ì¢Œí‘œ, ì§€ì‹œì ğŸ™, âˆšw_i ë“±)ì„ ë°˜ì˜í•´ í•œêµ­ì–´/ì¡´ëŒ“ë§/ë¯¸ê´„ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f923056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/19] ë¼ì¸ 91â€“91\n",
      "[2/19] ë¼ì¸ 118â€“118\n",
      "[3/19] ë¼ì¸ 120â€“120\n",
      "[4/19] ë¼ì¸ 124â€“124\n",
      "[5/19] ë¼ì¸ 127â€“130\n",
      "[6/19] ë¼ì¸ 138â€“138\n",
      "[7/19] ë¼ì¸ 138â€“138\n",
      "[8/19] ë¼ì¸ 142â€“142\n",
      "[9/19] ë¼ì¸ 150â€“150\n",
      "[10/19] ë¼ì¸ 150â€“150\n",
      "[11/19] ë¼ì¸ 156â€“156\n",
      "[12/19] ë¼ì¸ 156â€“156\n",
      "[13/19] ë¼ì¸ 160â€“160\n",
      "[14/19] ë¼ì¸ 173â€“173\n",
      "[15/19] ë¼ì¸ 173â€“173\n",
      "[16/19] ë¼ì¸ 179â€“185\n",
      "[17/19] ë¼ì¸ 198â€“246\n",
      "[18/19] ë¼ì¸ 248â€“248\n",
      "[19/19] ë¼ì¸ 248â€“248\n",
      "ì €ì¥ ì™„ë£Œ: C:/POLO/polo-system/models/math./_build\\equations_explained.json\n"
     ]
    }
   ],
   "source": [
    "# === ì…€ 5: ìˆ˜ì‹ í•´ì„¤ ìƒì„± ===\n",
    "EXPLAIN_SYSTEM = \"You are a teacher who explains math/AI research equations in clear, simple English. Always answer politely and understandably.\"\n",
    "\n",
    "EXPLAIN_TEMPLATE = \"\"\"Please explain the following equation so that it can be understood by someone at least at a middle school level.\n",
    "Follow this exact order in your output: Example â†’ Explanation â†’ Conclusion\n",
    "\n",
    "- Example: Show the equation exactly as LaTeX in a single block (do not modify or add anything).\n",
    "- Explanation: Provide bullet points explaining the meaning of symbols (âˆ‘, ğŸ™, ^, _, âˆš, \\\\, etc.) and the role of each term, in a clear and concise way.\n",
    "- Conclusion: Summarize in one sentence the core purpose of this equation in the context of the paper (e.g., loss composition, normalization, coordinate error, probability/log-likelihood, etc.).\n",
    "- (Important) Do not change the symbols or the order of the equation, and do not invent new symbols.\n",
    "- (Important) Write only in English.\n",
    "\n",
    "[Equation]\n",
    "{EQUATION}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def explain_equation_with_llm(eq_latex:str)->str:\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\": EXPLAIN_SYSTEM},\n",
    "        {\"role\":\"user\", \"content\": EXPLAIN_TEMPLATE.format(EQUATION=eq_latex)}\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(input_ids=input_ids, **GEN_KW)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return text.split(messages[-1][\"content\"])[-1].strip()\n",
    "\n",
    "# ë°°ì¹˜ ì‹¤í–‰ (ë„ˆë¬´ ë§ìœ¼ë©´ ì—¬ëŸ¬ ë²ˆì— ë‚˜ëˆ„ì„¸ìš”)\n",
    "explanations = []\n",
    "for idx, item in enumerate(equations_advanced, start=1):\n",
    "    print(f\"[{idx}/{len(equations_advanced)}] ë¼ì¸ {item['line_start']}â€“{item['line_end']}\")\n",
    "    exp = explain_equation_with_llm(item[\"body\"])\n",
    "    explanations.append({\n",
    "        \"index\": idx,\n",
    "        \"line_start\": item[\"line_start\"],\n",
    "        \"line_end\": item[\"line_end\"],\n",
    "        \"kind\": item[\"kind\"],\n",
    "        \"env\": item[\"env\"],\n",
    "        \"equation\": item[\"body\"],\n",
    "        \"explanation\": exp\n",
    "    })\n",
    "\n",
    "# ì¤‘ê°„ ì €ì¥\n",
    "json_path = os.path.join(OUT_DIR, \"equations_explained.json\")\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"overview\": doc_overview,\n",
    "        \"items\": explanations\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "print(f\"ì €ì¥ ì™„ë£Œ: {json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d30a1c",
   "metadata": {},
   "source": [
    "### ì…€ 6. LaTeX ë¦¬í¬íŠ¸ ìƒì„±(.tex) â€” ê°œìš” + ìˆ˜ì‹ë³„ í•´ì„¤\n",
    "\n",
    "> MiKTeX(Windows) ë˜ëŠ” TeX Liveê°€ ìˆì–´ì•¼ PDF ì»´íŒŒì¼ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì—†ìœ¼ë©´ ë‹¤ìŒ ì…€ì—ì„œ ì„¤ì¹˜ ì•ˆë‚´/ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë“œë¦½ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf7a9464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX ë¦¬í¬íŠ¸ ìƒì„±: C:/POLO/polo-system/models/math./_build\\yolo_math_report.tex\n"
     ]
    }
   ],
   "source": [
    "# === ì…€ 6: LaTeX ë¦¬í¬íŠ¸(.tex) ë§Œë“¤ê¸° ===\n",
    "import datetime\n",
    "\n",
    "report_tex_path = os.path.join(OUT_DIR, \"yolo_math_report.tex\")\n",
    "\n",
    "def latex_escape_verbatim(s:str)->str:\n",
    "    # ì„¤ëª… í…ìŠ¤íŠ¸ ë‚´ LaTeX íŠ¹ìˆ˜ë¬¸ì ê°„ë‹¨ ì´ìŠ¤ì¼€ì´í”„\n",
    "    s = s.replace(\"\\\\\", r\"\\\\\")\n",
    "    s = s.replace(\"#\", r\"\\#\").replace(\"$\", r\"\\$\")\n",
    "    s = s.replace(\"%\", r\"\\%\").replace(\"&\", r\"\\&\")\n",
    "    s = s.replace(\"_\", r\"\\_\").replace(\"{\", r\"\\{\").replace(\"}\", r\"\\}\")\n",
    "    s = s.replace(\"^\", r\"\\^{}\").replace(\"~\", r\"\\~{}\")\n",
    "    return s\n",
    "\n",
    "def build_report(overview:str, items:List[Dict])->str:\n",
    "    header = r\"\"\"\\documentclass[11pt]{article}\n",
    "\\usepackage[margin=1in]{geometry}\n",
    "\\usepackage{amsmath, amssymb, amsfonts}\n",
    "\\usepackage{hyperref}\n",
    "\\usepackage{kotex} % Windows MiKTeXì— ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨ (ì—†ìœ¼ë©´ xelatex/xeCJKë¡œ ì»´íŒŒì¼)\n",
    "\\setlength{\\parskip}{6pt}\n",
    "\\setlength{\\parindent}{0pt}\n",
    "\\title{LaTeX ë¬¸ì„œ ìˆ˜ì‹ í•´ì„¤ ë¦¬í¬íŠ¸ (ì¤‘í•™ìƒ ìˆ˜ì¤€ ì´ìƒ)}\n",
    "\\author{ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸}\n",
    "\\date{\"\"\" + datetime.date.today().isoformat() + r\"\"\"}\n",
    "\\begin{document}\n",
    "\\maketitle\n",
    "\\tableofcontents\n",
    "\\newpage\n",
    "\"\"\"\n",
    "    parts = [header]\n",
    "    # ê°œìš”\n",
    "    parts.append(r\"\\section*{ë¬¸ì„œ ê°œìš”}\")\n",
    "    parts.append(latex_escape_verbatim(overview))\n",
    "    parts.append(\"\\n\\\\newpage\\n\")\n",
    "\n",
    "    # ê° ìˆ˜ì‹\n",
    "    for it in items:\n",
    "        title = f\"ë¼ì¸ {it['line_start']}â€“{it['line_end']} / {it['kind']} {('['+it['env']+']') if it['env'] else ''}\"\n",
    "        parts.append(f\"\\\\section*{{{latex_escape_verbatim(title)}}}\")\n",
    "        # ì˜ˆì‹œ â†’ ì„¤ëª… â†’ ê²°ë¡  (LLM ì¶œë ¥ ê·¸ëŒ€ë¡œ ì‚½ì…, ë‹¨ ìˆ˜ì‹ ë¸”ë¡ì€ ìœ ì§€)\n",
    "        # ëª¨ë¸ ì¶œë ¥ ì¤‘ ì½”ë“œë¸”ë¡ì´ ìˆë‹¤ë©´ ì œê±°í•˜ê³  LaTeX ìˆ˜ì‹ë§Œ ë‚¨ê¸°ëŠ” ê²Œ ì•ˆì „\n",
    "        exp_txt = it[\"explanation\"]\n",
    "        parts.append(exp_txt)\n",
    "        parts.append(\"\\n\")\n",
    "\n",
    "    parts.append(\"\\\\end{document}\\n\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "report_tex = build_report(doc_overview, explanations)\n",
    "Path(report_tex_path).write_text(report_tex, encoding=\"utf-8\")\n",
    "print(f\"LaTeX ë¦¬í¬íŠ¸ ìƒì„±: {report_tex_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fc0d23",
   "metadata": {},
   "source": [
    "ë”± ì—¬ê¸° ì…€6ê¹Œì§€ë§Œ ì‹¤í–‰í•˜ê³  íŒŒì¼ í™•ì¸í•˜ì‹œë©´ ë©ë‹ˆë‹¤! 7ë²ˆë¶€í„°ëŠ” ì´ëŸ°ì €ëŸ°ê±° ê¹”ì•„ì•¼í•˜ê³  ë°©í–¥ì„±ì„ ë‹¤ì‹œ ì¡ëŠ” ì¤‘ì´ë¼..\n",
    "\n",
    "sectionë³„ë¡œ ë‚˜ë‰˜ì–´ ìˆìŠµë‹ˆë‹¤! ë“œë˜ê·¸í•´ì„œ ì§€í”¼í‹°í•œí…Œ í•œê¸€ë¡œ ë²ˆì—­. í‹€ë ¤ë„ ê·¸ëŒ€ë¡œ ì´ë ‡ê²Œ ì£¼ë¬¸í•˜ì‹œë©´ ë©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc0e2b",
   "metadata": {},
   "source": [
    "### ì…€ 7. PDF ì»´íŒŒì¼ ì‹œë„ (pdflatex â†’ xelatex ìˆœì„œ)\n",
    "\n",
    "> ë¯¸ì„¤ì¹˜ ì‹œ MiKTeX ì„¤ì¹˜ í›„ PATH ë°˜ì˜ì´ í•„ìš”í•©ë‹ˆë‹¤. ì‹¤íŒ¨í•´ë„ .texëŠ” ìƒì„±ë˜ì–´ ìˆìœ¼ë‹ˆ, ë¡œì»¬ì—ì„œ GUI(MiKTeX Console)ë¡œ ì»´íŒŒì¼í•˜ì…”ë„ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e220663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Trying: pdflatex -interaction=nonstopmode -halt-on-error yolo_math_report.tex\n",
      "ì‹¤íŒ¨ ì½”ë“œ: 127\n",
      "\n",
      "Command not found: pdflatex\n",
      ">> Trying: xelatex -interaction=nonstopmode -halt-on-error yolo_math_report.tex\n",
      "ì‹¤íŒ¨ ì½”ë“œ: 127\n",
      "\n",
      "Command not found: xelatex\n",
      "PDF ì»´íŒŒì¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. MiKTeX ë˜ëŠ” TeX Live ì„¤ì¹˜/íŒ¨í‚¤ì§€(kotex) í™•ì¸ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "# === ì…€ 7: PDF ì»´íŒŒì¼ ===\n",
    "def run_cmd(cmd:List[str])->Tuple[int,str,str]:\n",
    "    try:\n",
    "        proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, cwd=OUT_DIR, shell=False)\n",
    "        return proc.returncode, proc.stdout, proc.stderr\n",
    "    except FileNotFoundError:\n",
    "        return 127, \"\", f\"Command not found: {cmd[0]}\"\n",
    "\n",
    "def try_compile(tex_filename:str)->Optional[str]:\n",
    "    base = os.path.splitext(tex_filename)[0]\n",
    "    pdf_path = os.path.join(OUT_DIR, base + \".pdf\")\n",
    "    for engine in [[\"pdflatex\",\"-interaction=nonstopmode\",\"-halt-on-error\",tex_filename],\n",
    "                   [\"xelatex\",\"-interaction=nonstopmode\",\"-halt-on-error\",tex_filename]]:\n",
    "        print(\">> Trying:\", \" \".join(engine))\n",
    "        code,out,err = run_cmd(engine)\n",
    "        if code == 0 and Path(pdf_path).exists():\n",
    "            print(\"PDF ìƒì„± ì„±ê³µ:\", pdf_path)\n",
    "            return pdf_path\n",
    "        else:\n",
    "            print(\"ì‹¤íŒ¨ ì½”ë“œ:\", code)\n",
    "            print(out[:800])\n",
    "            print(err[:800])\n",
    "    return None\n",
    "\n",
    "pdf_path = try_compile(\"yolo_math_report.tex\")\n",
    "if pdf_path:\n",
    "    print(\"ì™„ë£Œ:\", pdf_path)\n",
    "else:\n",
    "    print(\"PDF ì»´íŒŒì¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. MiKTeX ë˜ëŠ” TeX Live ì„¤ì¹˜/íŒ¨í‚¤ì§€(kotex) í™•ì¸ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f628b",
   "metadata": {},
   "source": [
    "### ì…€ 8. ì¸ë±ìŠ¤/ë¡œê·¸ ì €ì¥ (CSV/JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ì…€ 8: ì¸ë±ìŠ¤/ë¡œê·¸ ì €ì¥ ===\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(equations_all)\n",
    "df_adv = pd.DataFrame(equations_advanced)\n",
    "\n",
    "df_path = os.path.join(OUT_DIR, \"equations_all.csv\")\n",
    "df_adv_path = os.path.join(OUT_DIR, \"equations_advanced.csv\")\n",
    "df.to_csv(df_path, index=False, encoding=\"utf-8-sig\")\n",
    "df_adv.to_csv(df_adv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"ì €ì¥ ì™„ë£Œ:\")\n",
    "print(\" - ì „ì²´ ìˆ˜ì‹ CSV :\", df_path)\n",
    "print(\" - ê³ ë‚œë„ ìˆ˜ì‹ CSV:\", df_adv_path)\n",
    "\n",
    "# ê°œìš”/ì„¤ëª… ìš”ì•½ë³¸\n",
    "summary_md = os.path.join(OUT_DIR, \"README_report_summary.md\")\n",
    "with open(summary_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# ë¬¸ì„œ ê°œìš”\\n\\n\")\n",
    "    f.write(doc_overview + \"\\n\\n\")\n",
    "    f.write(\"## ìˆ˜ì‹ í†µê³„\\n\")\n",
    "    f.write(f\"- ì „ì²´ ìˆ˜ì‹: {len(equations_all)}ê°œ\\n\")\n",
    "    f.write(f\"- ê³ ë‚œë„ ìˆ˜ì‹: {len(equations_advanced)}ê°œ\\n\")\n",
    "print(\" - ìš”ì•½ MD       :\", summary_md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
