# app.py
# -*- coding: utf-8 -*-
"""
raw í´ë” â†’ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ â†’ ì „ë‹¬ìš© payload êµ¬ì„±(+ ë¡œì»¬ ì €ì¥)
- auto_merge í¬í•¨ëœ pipelineì„ í˜¸ì¶œ
- transport payload: meta + merged_tex + counts + íŒŒì¼ ê²½ë¡œ + (ì˜µì…˜) ì¼ë¶€ ì¸ë¼ì¸ ìƒ˜í”Œ
- ë™ì‹œì— JSONL(.gz) ì‚°ì¶œë¬¼ì€ ê¸°ì¡´ì²˜ëŸ¼ out_dirì— ì €ì¥
- FastAPI ì„œë²„ë¡œ ë³€ê²½í•˜ì—¬ HTTP ìš”ì²­ ì²˜ë¦¬
"""
from __future__ import annotations
from pathlib import Path
import argparse, json, gzip, time, sys
from typing import Any, Optional
import uvicorn
import httpx
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

# ëª¨ë“ˆ ê²½ë¡œ ì„¤ì •
current_dir = Path(__file__).parent
src_dir = current_dir / "src"
sys.path.insert(0, str(src_dir))

# ìš°ë¦¬ ëª¨ë“ˆ
from texprep.utils.cfg import load_cfg  # ë„¤ê°€ ë§Œë“  cfg ë¡œë”
from texprep.pipeline import run_pipeline

# FastAPI ì•± ìƒì„±
app = FastAPI(title="POLO Preprocessing Service", version="1.0.0")

# í™˜ê²½ ë³€ìˆ˜ (Easy ëª¨ë¸ë§Œ ì‚¬ìš©)
EASY_URL = "http://localhost:5003"

# Pydantic ëª¨ë¸
class ProcessRequest(BaseModel):
    paper_id: str
    source_dir: str
    callback: str

class ProcessResponse(BaseModel):
    ok: bool
    paper_id: str
    out_dir: str
    transport_path: str
    counts: dict

@app.get("/health")
def health():
    return {"ok": True}

@app.post("/process", response_model=ProcessResponse)
async def process_paper(request: ProcessRequest):
    """
    ë…¼ë¬¸ ì „ì²˜ë¦¬ ì‹¤í–‰
    1. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    2. transport payload ìƒì„±
    3. math/easy ëª¨ë¸ë¡œ ê²°ê³¼ ì „ì†¡
    4. ì½œë°± í˜¸ì¶œ
    """
    try:
        # 1) ì„¤ì • ë¡œë“œ
        config_path = Path(__file__).parent / "configs" / "default.yaml"
        cfg = load_cfg(str(config_path))
        
        # ì ˆëŒ€ ê²½ë¡œë¡œ out_dir ì„¤ì •
        current_file = Path(__file__).resolve()
        polo_system_dir = current_file.parent.parent.parent  # polo-system
        server_dir = polo_system_dir / "server"  # polo-system/server
        default_out = server_dir / "data" / "out"
        cfg["out_dir"] = str(default_out)
        
        # 2) source_dirì—ì„œ ë©”ì¸ tex ì°¾ê¸°
        source_path = Path(request.source_dir)
        if not source_path.exists():
            raise HTTPException(status_code=400, detail=f"Source directory not found: {source_path}")
        
        # ë©”ì¸ tex íŒŒì¼ ì°¾ê¸°
        tex_files = list(source_path.rglob("*.tex"))
        if not tex_files:
            raise HTTPException(status_code=400, detail="No .tex files found in source directory")
        
        # ë©”ì¸ tex íŒŒì¼ ì°¾ê¸° (ê°„ë‹¨í•œ ì¶”ì •)
        main_tex = None
        priority_names = ["main.tex", "ms.tex", "paper.tex", "arxiv.tex", "root.tex"]
        for priority_name in priority_names:
            for tex_file in tex_files:
                if tex_file.name.lower() == priority_name:
                    main_tex = tex_file
                    break
            if main_tex:
                break
        
        if not main_tex:
            main_tex = tex_files[0]  # ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©
        
        # 3) íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        run = run_pipeline(cfg, main_tex=str(main_tex), sink="json")
        
        # 4) transport payload êµ¬ì„±
        payload = build_transport_payload(run, inline=True, head_n=3, body_chars=20000)
        
        # 5) ì €ì¥
        out_dir = Path(run["out_dir"])
        out_dir.mkdir(parents=True, exist_ok=True)
        transport_path = out_dir / "transport.json"
        transport_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
        
        # 6) math/easy ëª¨ë¸ë¡œ ê²°ê³¼ ì „ì†¡
        await send_to_models(request.paper_id, payload, out_dir)
        
        # 7) ì½œë°± í˜¸ì¶œ
        await send_callback(request.callback, request.paper_id, str(transport_path))
        
        return ProcessResponse(
            ok=True,
            paper_id=request.paper_id,
            out_dir=str(out_dir),
            transport_path=str(transport_path),
            counts=payload["meta"]["counts"]
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")

async def send_to_models(paper_id: str, payload: dict, out_dir: Path):
    """easy ëª¨ë¸ë¡œ ê²°ê³¼ ì „ì†¡ (Math ëª¨ë¸ ì œì™¸)"""
    try:
        # easy ëª¨ë¸ë¡œ JSONL íŒŒì¼ ê²½ë¡œ ì „ì†¡ (ë°°ì¹˜ ì²˜ë¦¬)
        chunks_path = out_dir / "chunks.jsonl"
        if not chunks_path.exists():
            chunks_path = out_dir / "chunks.jsonl.gz"
        
        if chunks_path.exists():
            print(f"ğŸ“¤ Easy ëª¨ë¸ë¡œ ì „ì†¡: {chunks_path}")
            try:
                async with httpx.AsyncClient(timeout=60) as client:
                    response = await client.post(f"{EASY_URL}/batch", json={
                        "paper_id": paper_id,
                        "chunks_jsonl": str(chunks_path),
                        "output_dir": str(out_dir / "easy_outputs")
                    })
                    print(f"âœ… Easy ëª¨ë¸ ì‘ë‹µ: {response.status_code}")
            except Exception as e:
                print(f"Warning: Failed to send to models: {e}")
        else:
            print(f"âš ï¸ chunks.jsonl íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {out_dir}")
                
    except Exception as e:
        print(f"Warning: Failed to send to models: {e}")

async def send_callback(callback_url: str, paper_id: str, transport_path: str):
    """ì½œë°± í˜¸ì¶œ"""
    try:
        async with httpx.AsyncClient(timeout=30) as client:
            await client.post(callback_url, json={
                "paper_id": paper_id,
                "transport_path": transport_path,
                "status": "completed"
            })
    except Exception as e:
        print(f"Warning: Failed to send callback: {e}")

def read_chunks_as_text(chunks_path: Path) -> str:
    """chunks íŒŒì¼ì„ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    chunks = []
    if chunks_path.suffix == ".gz":
        with gzip.open(chunks_path, "rt", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        chunk = json.loads(line)
                        chunks.append(chunk.get("text", ""))
                    except:
                        continue
    else:
        with open(chunks_path, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        chunk = json.loads(line)
                        chunks.append(chunk.get("text", ""))
                    except:
                        continue
    return "\n\n".join(chunks)

def _now() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

def _read_text(p: Path, limit_chars: int | None = None) -> str:
    s = p.read_text(encoding="utf-8", errors="ignore")
    return s if not limit_chars else s[:limit_chars]

def _head_jsonl(path: Path, n: int = 3) -> list[dict[str, Any]]:
    """jsonl ë˜ëŠ” jsonl.gz ì• Nì¤„ë§Œ íŒŒì‹±."""
    items: list[dict[str, Any]] = []
    if path.suffix == ".gz":
        f = gzip.open(path, "rt", encoding="utf-8")
    else:
        f = open(path, "r", encoding="utf-8")
    with f:
        for i, line in enumerate(f):
            if i >= n: break
            line = line.strip()
            if not line: continue
            try:
                items.append(json.loads(line))
            except Exception:
                # ìƒ˜í”Œì´ë‹ˆ ì¡°ìš©íˆ ìŠ¤í‚µ
                pass
    return items

def _count_lines(path: Path) -> int:
    if path.suffix == ".gz":
        with gzip.open(path, "rt", encoding="utf-8") as f:
            return sum(1 for _ in f)
    with open(path, "r", encoding="utf-8") as f:
        return sum(1 for _ in f)

def build_transport_payload(run: dict[str, Any], inline: bool = True, head_n: int = 3, body_chars: int = 20000) -> dict[str, Any]:
    """
    ì „ë‹¬ìš© êµ¬ì¡°í™”. ë¬´ì‹í•˜ê²Œ ì „ë¶€ ì‹¤ì–´ ë‚˜ë¥´ì§€ ë§ê³ , ìƒ˜í”ŒÂ·ì¹´ìš´íŠ¸Â·ê²½ë¡œ ìœ„ì£¼.
    í•„ìš”í•˜ë©´ inline=Trueë¡œ ì¼ë¶€ë§Œ ë™ë´‰.
    """
    files = {k: Path(v) for k, v in (run.get("files") or {}).items()}
    out_dir = Path(run["out_dir"])

    merged_tex_path = out_dir / "merged_body.tex"
    # 
    payload: dict[str, Any] = {
        "meta": {
            "doc_id": run["doc_id"],
            "mode": run.get("mode", "one"),
            "ts": _now(),
            "main": run.get("main"),
            "merged_roots": run.get("merged_roots", []),
            "counts": {
                "chunks": run["chunks"],
                "equations": run["equations"],
                "inline_equations": run["inline_equations"],
                "assets": run["assets"],
            },
        },
        "artifacts": {
            "merged_body_tex": {
                "path": str(merged_tex_path),
                **({"preview": _read_text(merged_tex_path, body_chars)} if inline and merged_tex_path.exists() else {}),
            },
            "chunks": {
                "path": str(files.get("chunks", "")),
                "count": _count_lines(files["chunks"]) if "chunks" in files else 0,
                **({"head": _head_jsonl(files["chunks"], head_n)} if inline and "chunks" in files else {}),
            },
            "equations": {
                "path": str(files.get("equations", "")),
                "count": _count_lines(files["equations"]) if "equations" in files else 0,
                **({"head": _head_jsonl(files["equations"], head_n)} if inline and "equations" in files else {}),
            },
            "assets": {
                "path": str(files.get("assets", "")),
                "count": _count_lines(files["assets"]) if "assets" in files else 0,
                **({"head": _head_jsonl(files["assets"], head_n)} if inline and "assets" in files else {}),
            },
            "xref_mentions": {
                "path": str(files.get("xref_mentions", "")),
                "count": _count_lines(files["xref_mentions"]) if "xref_mentions" in files else 0,
                **({"head": _head_jsonl(files["xref_mentions"], head_n)} if inline and "xref_mentions" in files else {}),
            },
            "xref_edges": {
                "path": str(files.get("xref_edges", "")),
                "count": _count_lines(files["xref_edges"]) if "xref_edges" in files else 0,
                **({"head": _head_jsonl(files["xref_edges"], head_n)} if inline and "xref_edges" in files else {}),
            },
        },
    }
    return payload

def main():
    ap = argparse.ArgumentParser(description="texprep app: raw â†’ pipeline â†’ transport payload + files")
    ap.add_argument("--config", required=True, help="configs/default.yaml")
    # ë‘˜ ì¤‘ í•˜ë‚˜: --main(ì•µì»¤ tex) ë˜ëŠ” --root(í´ë”) ì œê³µ
    ap.add_argument("--main", help="anchor .tex path")
    ap.add_argument("--root", help="folder containing raw .tex")
    ap.add_argument("--inline", action="store_true", help="transportì— ë¯¸ë¦¬ë³´ê¸°/ìƒ˜í”Œ í¬í•¨")
    ap.add_argument("--head", type=int, default=3, help="ê° jsonl í—¤ë“œ ìƒ˜í”Œ ê°œìˆ˜")
    ap.add_argument("--body-chars", type=int, default=20000, help="merged_body.tex ë¯¸ë¦¬ë³´ê¸° ê¸€ììˆ˜")
    ap.add_argument("--save-transport", help="ê²°ê³¼ transport.json ì €ì¥ ê²½ë¡œ(ê¸°ë³¸: out_dir/<doc_id>/transport.json)")
    args = ap.parse_args()

    cfg = load_cfg(args.config)
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ out_dir ì„¤ì •
    current_file = Path(__file__).resolve()
    polo_system_dir = current_file.parent.parent.parent  # polo-system
    server_dir = polo_system_dir / "server"  # polo-system/server
    default_out = server_dir / "data" / "out"
    cfg["out_dir"] = str(default_out)

    # ì•µì»¤ ê²°ì •
    anchor: str | None = None
    if args.main:
        anchor = args.main
    elif args.root:
        # rootë§Œ ì™”ìœ¼ë©´ ëŒ€ì¶© ê·¸ í´ë” ì•„ë˜ ì•„ë¬´ .tex í•˜ë‚˜ë¥¼ ì•µì»¤ë¡œ ì‚¬ìš©
        # (pipelineì€ auto_mergeë©´ í´ë” ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•¨)
        root = Path(args.root)
        any_tex = next(root.rglob("*.tex"), None)
        if not any_tex:
            print("*.tex ì—†ìŒ: --root í™•ì¸í•´ë¼.", file=sys.stderr); sys.exit(2)
        anchor = str(any_tex)
    else:
        print("--main ë˜ëŠ” --rootë¥¼ ì¤˜.", file=sys.stderr); sys.exit(2)

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    run = run_pipeline(cfg, main_tex=anchor, sink="json")

    # transport payload êµ¬ì„±
    payload = build_transport_payload(run, inline=args.inline, head_n=args.head, body_chars=args.body_chars)

    # ì €ì¥ ìœ„ì¹˜
    out_dir = Path(run["out_dir"])
    out_dir.mkdir(parents=True, exist_ok=True)
    save_path = Path(args.save_transport) if args.save_transport else (out_dir / "transport.json")
    save_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")

    # ìš”ì•½ ì¶œë ¥
    result = {
        "doc_id": run["doc_id"],
        "mode": run.get("mode"),
        "out_dir": run["out_dir"],
        "transport": str(save_path),
        "counts": payload["meta"]["counts"],
        "files": run.get("files", {}),
    }
    print(json.dumps(result, ensure_ascii=False))
    return result

# FastAPI ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
async def health():
    return {"status": "ok", "service": "preprocess"}

@app.post("/preprocess", response_model=ProcessResponse)
async def preprocess_endpoint(request: ProcessResponse):
    """ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    try:
        # ì„ì‹œ args ê°ì²´ ìƒì„±
        class Args:
            def __init__(self, input_path, output_dir, config_path):
                self.main = input_path
                self.root = None
                self.out_dir = output_dir
                self.config = config_path
                self.inline = True
                self.head = 3
                self.body_chars = 1000
                self.save_transport = None
        
        args = Args(request.input_path, request.output_dir, request.config_path)
        
        # ì „ì²˜ë¦¬ ì‹¤í–‰
        result = main_with_args(args)
        
        return ProcessResponse(
            success=True,
            message="ì „ì²˜ë¦¬ ì™„ë£Œ",
            output_path=result.get("out_dir"),
            file_count=result.get("counts", {}).get("total", 0)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

def main_with_args(args):
    """main í•¨ìˆ˜ë¥¼ args ê°ì²´ë¡œ ì‹¤í–‰"""
    # ê¸°ì¡´ main í•¨ìˆ˜ ë¡œì§ì„ ì—¬ê¸°ì— êµ¬í˜„
    cfg = load_cfg(args.config)
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ out_dir ì„¤ì •
    current_file = Path(__file__).resolve()
    polo_system_dir = current_file.parent.parent.parent  # polo-system
    server_dir = polo_system_dir / "server"  # polo-system/server
    default_out = server_dir / "data" / "out"
    cfg["out_dir"] = str(default_out)
    
    # main ë˜ëŠ” root ê²°ì •
    if args.main:
        anchor = str(Path(args.main).resolve())
    elif args.root:
        root = Path(args.root)
        any_tex = next(root.glob("**/*.tex"), None)
        if not any_tex:
            raise FileNotFoundError("*.tex ì—†ìŒ: --root í™•ì¸í•´ë¼.")
        anchor = str(any_tex)
    else:
        raise ValueError("--main ë˜ëŠ” --rootë¥¼ ì¤˜.")

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    run = run_pipeline(cfg, main_tex=anchor, sink="json")

    # transport payload êµ¬ì„±
    payload = build_transport_payload(run, inline=args.inline, head_n=args.head, body_chars=args.body_chars)

    # ì €ì¥ ìœ„ì¹˜
    out_dir = Path(run["out_dir"])
    out_dir.mkdir(parents=True, exist_ok=True)
    save_path = Path(args.save_transport) if args.save_transport else (out_dir / "transport.json")
    save_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")

    # ê²°ê³¼ ë°˜í™˜
    return {
        "doc_id": run["doc_id"],
        "mode": run.get("mode"),
        "out_dir": run["out_dir"],
        "transport": str(save_path),
        "counts": payload["meta"]["counts"],
        "files": run.get("files", {}),
    }

if __name__ == "__main__":
    # CLI ëª¨ë“œì™€ ì„œë²„ ëª¨ë“œ êµ¬ë¶„
    if len(sys.argv) > 1 and sys.argv[1] == "cli":
        # CLI ëª¨ë“œ (ê¸°ì¡´ ë™ì‘)
        main()
    else:
        # FastAPI ì„œë²„ ëª¨ë“œ
        uvicorn.run(app, host="0.0.0.0", port=5002, reload=False)
