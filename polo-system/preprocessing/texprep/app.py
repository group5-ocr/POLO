# app.py
# -*- coding: utf-8 -*-
"""
raw í´ë” â†’ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ â†’ ì „ë‹¬ìš© payload êµ¬ì„±(+ ë¡œì»¬ ì €ì¥)
- auto_merge í¬í•¨ëœ pipelineì„ í˜¸ì¶œ
- transport payload: meta + merged_tex + counts + íŒŒì¼ ê²½ë¡œ + (ì˜µì…˜) ì¼ë¶€ ì¸ë¼ì¸ ìƒ˜í”Œ
- ë™ì‹œì— JSONL(.gz) ì‚°ì¶œë¬¼ì€ ê¸°ì¡´ì²˜ëŸ¼ out_dirì— ì €ì¥
"""
from __future__ import annotations
from pathlib import Path
import argparse, json, gzip, time, sys
from typing import Any
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

# ìš°ë¦¬ ëª¨ë“ˆ
from src.texprep.utils.cfg import load_cfg  # ë„¤ê°€ ë§Œë“  cfg ë¡œë”
from src.texprep.pipeline import run_pipeline

# FastAPI ì•± ìƒì„±
app = FastAPI(title="POLO Preprocess Service", version="1.0.0")

class PreprocessRequest(BaseModel):
    input_path: str
    output_dir: str = "data/outputs"
    config_path: str = "configs/default.yaml"

class PreprocessResponse(BaseModel):
    success: bool
    message: str
    output_path: str = None
    file_count: int = 0

def _now() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

def _read_text(p: Path, limit_chars: int | None = None) -> str:
    s = p.read_text(encoding="utf-8", errors="ignore")
    return s if not limit_chars else s[:limit_chars]

def _head_jsonl(path: Path, n: int = 3) -> list[dict[str, Any]]:
    """jsonl ë˜ëŠ” jsonl.gz ì• Nì¤„ë§Œ íŒŒì‹±."""
    items: list[dict[str, Any]] = []
    if path.suffix == ".gz":
        f = gzip.open(path, "rt", encoding="utf-8")
    else:
        f = open(path, "r", encoding="utf-8")
    with f:
        for i, line in enumerate(f):
            if i >= n: break
            line = line.strip()
            if not line: continue
            try:
                items.append(json.loads(line))
            except Exception:
                # ìƒ˜í”Œì´ë‹ˆ ì¡°ìš©íˆ ìŠ¤í‚µ
                pass
    return items

def _count_lines(path: Path) -> int:
    if path.suffix == ".gz":
        with gzip.open(path, "rt", encoding="utf-8") as f:
            return sum(1 for _ in f)
    with open(path, "r", encoding="utf-8") as f:
        return sum(1 for _ in f)

def build_transport_payload(run: dict[str, Any], inline: bool = True, head_n: int = 3, body_chars: int = 20000) -> dict[str, Any]:
    """
    ì „ë‹¬ìš© êµ¬ì¡°í™”. ë¬´ì‹í•˜ê²Œ ì „ë¶€ ì‹¤ì–´ ë‚˜ë¥´ì§€ ë§ê³ , ìƒ˜í”ŒÂ·ì¹´ìš´íŠ¸Â·ê²½ë¡œ ìœ„ì£¼.
    í•„ìš”í•˜ë©´ inline=Trueë¡œ ì¼ë¶€ë§Œ ë™ë´‰.
    """
    files = {k: Path(v) for k, v in (run.get("files") or {}).items()}
    out_dir = Path(run["out_dir"])

    merged_tex_path = out_dir / "merged_body.tex"
    # 
    payload: dict[str, Any] = {
        "meta": {
            "doc_id": run["doc_id"],
            "mode": run.get("mode", "one"),
            "ts": _now(),
            "main": run.get("main"),
            "merged_roots": run.get("merged_roots", []),
            "counts": {
                "chunks": run["chunks"],
                "equations": run["equations"],
                "inline_equations": run["inline_equations"],
                "assets": run["assets"],
            },
        },
        "artifacts": {
            "merged_body_tex": {
                "path": str(merged_tex_path),
                **({"preview": _read_text(merged_tex_path, body_chars)} if inline and merged_tex_path.exists() else {}),
            },
            "chunks": {
                "path": str(files.get("chunks", "")),
                "count": _count_lines(files["chunks"]) if "chunks" in files else 0,
                **({"head": _head_jsonl(files["chunks"], head_n)} if inline and "chunks" in files else {}),
            },
            "equations": {
                "path": str(files.get("equations", "")),
                "count": _count_lines(files["equations"]) if "equations" in files else 0,
                **({"head": _head_jsonl(files["equations"], head_n)} if inline and "equations" in files else {}),
            },
            "assets": {
                "path": str(files.get("assets", "")),
                "count": _count_lines(files["assets"]) if "assets" in files else 0,
                **({"head": _head_jsonl(files["assets"], head_n)} if inline and "assets" in files else {}),
            },
            "xref_mentions": {
                "path": str(files.get("xref_mentions", "")),
                "count": _count_lines(files["xref_mentions"]) if "xref_mentions" in files else 0,
                **({"head": _head_jsonl(files["xref_mentions"], head_n)} if inline and "xref_mentions" in files else {}),
            },
            "xref_edges": {
                "path": str(files.get("xref_edges", "")),
                "count": _count_lines(files["xref_edges"]) if "xref_edges" in files else 0,
                **({"head": _head_jsonl(files["xref_edges"], head_n)} if inline and "xref_edges" in files else {}),
            },
        },
    }
    return payload

def main():
    ap = argparse.ArgumentParser(description="texprep app: raw â†’ pipeline â†’ transport payload + files")
    ap.add_argument("--config", required=True, help="configs/default.yaml")
    # ë‘˜ ì¤‘ í•˜ë‚˜: --main(ì•µì»¤ tex) ë˜ëŠ” --root(í´ë”) ì œê³µ
    ap.add_argument("--main", help="anchor .tex path")
    ap.add_argument("--root", help="folder containing raw .tex")
    ap.add_argument("--inline", action="store_true", help="transportì— ë¯¸ë¦¬ë³´ê¸°/ìƒ˜í”Œ í¬í•¨")
    ap.add_argument("--head", type=int, default=3, help="ê° jsonl í—¤ë“œ ìƒ˜í”Œ ê°œìˆ˜")
    ap.add_argument("--body-chars", type=int, default=20000, help="merged_body.tex ë¯¸ë¦¬ë³´ê¸° ê¸€ììˆ˜")
    ap.add_argument("--save-transport", help="ê²°ê³¼ transport.json ì €ì¥ ê²½ë¡œ(ê¸°ë³¸: out_dir/<doc_id>/transport.json)")
    args = ap.parse_args()

    cfg = load_cfg(args.config)

    # ì•µì»¤ ê²°ì •
    anchor: str | None = None
    if args.main:
        anchor = args.main
    elif args.root:
        # rootë§Œ ì™”ìœ¼ë©´ ëŒ€ì¶© ê·¸ í´ë” ì•„ë˜ ì•„ë¬´ .tex í•˜ë‚˜ë¥¼ ì•µì»¤ë¡œ ì‚¬ìš©
        # (pipelineì€ auto_mergeë©´ í´ë” ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•¨)
        root = Path(args.root)
        any_tex = next(root.rglob("*.tex"), None)
        if not any_tex:
            print("*.tex ì—†ìŒ: --root í™•ì¸í•´ë¼.", file=sys.stderr); sys.exit(2)
        anchor = str(any_tex)
    else:
        print("--main ë˜ëŠ” --rootë¥¼ ì¤˜.", file=sys.stderr); sys.exit(2)

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    run = run_pipeline(cfg, main_tex=anchor, sink="json")

    # transport payload êµ¬ì„±
    payload = build_transport_payload(run, inline=args.inline, head_n=args.head, body_chars=args.body_chars)

    # ì €ì¥ ìœ„ì¹˜
    out_dir = Path(run["out_dir"])
    out_dir.mkdir(parents=True, exist_ok=True)
    save_path = Path(args.save_transport) if args.save_transport else (out_dir / "transport.json")
    save_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")

    # ìš”ì•½ ì¶œë ¥
    result = {
        "doc_id": run["doc_id"],
        "mode": run.get("mode"),
        "out_dir": run["out_dir"],
        "transport": str(save_path),
        "counts": payload["meta"]["counts"],
        "files": run.get("files", {}),
    }
    print(json.dumps(result, ensure_ascii=False))
    return result

# FastAPI ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
async def health():
    return {"status": "ok", "service": "preprocess"}

@app.post("/preprocess", response_model=PreprocessResponse)
async def preprocess_endpoint(request: PreprocessRequest):
    """ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    try:
        # ì„ì‹œ args ê°ì²´ ìƒì„±
        class Args:
            def __init__(self, input_path, output_dir, config_path):
                self.main = input_path
                self.root = None
                self.out_dir = output_dir
                self.config = config_path
                self.inline = True
                self.head = 3
                self.body_chars = 1000
                self.save_transport = None
        
        args = Args(request.input_path, request.output_dir, request.config_path)
        
        # ì „ì²˜ë¦¬ ì‹¤í–‰
        result = main_with_args(args)
        
        return PreprocessResponse(
            success=True,
            message="ì „ì²˜ë¦¬ ì™„ë£Œ",
            output_path=result.get("out_dir"),
            file_count=result.get("counts", {}).get("total", 0)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

def main_with_args(args):
    """main í•¨ìˆ˜ë¥¼ args ê°ì²´ë¡œ ì‹¤í–‰"""
    # ê¸°ì¡´ main í•¨ìˆ˜ ë¡œì§ì„ ì—¬ê¸°ì— êµ¬í˜„
    cfg = load_cfg(args.config)
    
    # main ë˜ëŠ” root ê²°ì •
    if args.main:
        anchor = str(Path(args.main).resolve())
    elif args.root:
        root = Path(args.root)
        any_tex = next(root.glob("**/*.tex"), None)
        if not any_tex:
            raise FileNotFoundError("*.tex ì—†ìŒ: --root í™•ì¸í•´ë¼.")
        anchor = str(any_tex)
    else:
        raise ValueError("--main ë˜ëŠ” --rootë¥¼ ì¤˜.")

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    run = run_pipeline(cfg, main_tex=anchor, sink="json")

    # transport payload êµ¬ì„±
    payload = build_transport_payload(run, inline=args.inline, head_n=args.head, body_chars=args.body_chars)

    # ì €ì¥ ìœ„ì¹˜
    out_dir = Path(run["out_dir"])
    out_dir.mkdir(parents=True, exist_ok=True)
    save_path = Path(args.save_transport) if args.save_transport else (out_dir / "transport.json")
    save_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")

    # ê²°ê³¼ ë°˜í™˜
    return {
        "doc_id": run["doc_id"],
        "mode": run.get("mode"),
        "out_dir": run["out_dir"],
        "transport": str(save_path),
        "counts": payload["meta"]["counts"],
        "files": run.get("files", {}),
    }

if __name__ == "__main__":
    import os
    port = int(os.getenv("PREPROCESS_PORT", "5002"))
    print(f"ğŸ”§ Preprocess Service ì‹œì‘ (í¬íŠ¸: {port})")
    uvicorn.run(app, host="0.0.0.0", port=port)
