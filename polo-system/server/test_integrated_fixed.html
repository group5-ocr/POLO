
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>POLO - Integrated Results for 30</title>
    <style>
        body {
            font-family: 'Pretendard', 'Spoqa Han Sans Neo', 'Noto Sans KR', 'Apple SD Gothic Neo', 'Inter', 'Segoe UI', system-ui, -apple-system, BlinkMacSystemFont, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            background: #f8f9fa;
            color: #333;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        .header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
            padding: 20px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .section {
            background: white;
            margin: 20px 0;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .section h2 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        .paragraph {
            margin: 15px 0;
            padding: 10px;
            background: #f8f9fa;
            border-left: 4px solid #3498db;
        }
        .equation {
            margin: 20px 0;
            padding: 15px;
            background: #f0f8ff;
            border: 1px solid #b0d4f1;
            border-radius: 5px;
            text-align: center;
        }
        .equation-header {
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 10px;
        }
        .equation-content {
            font-size: 1.2em;
            margin: 10px 0;
        }
        .equation-explanation {
            margin-top: 10px;
            font-style: italic;
            color: #666;
        }
        .image-container {
            text-align: center;
            margin: 20px 0;
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        .image-caption {
            margin-top: 10px;
            font-style: italic;
            color: #666;
        }
        .mathjax {
            font-size: 1.1em;
        }
    </style>
    <script>
        window.MathJax = {
            loader: { load: ['[tex]/ams', '[tex]/mathtools', '[tex]/physics'] },
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                packages: { '[+]': ['ams', 'mathtools', 'physics'] },
                processEscapes: true,
                tags: 'none',
                macros: {
                    mathlarger: ['{\\large #1}', 1],
                    mathbbm: ['{\\mathbb{#1}}', 1],
                    wt: ['{\\widetilde{#1}}', 1],
                    wh: ['{\\widehat{#1}}', 1],
                    dfn: '{\\triangleq}',
                    dB: '{\\mathrm{dB}}',
                    snr: '{\\mathrm{SNR}}',
                    bsnr: '{\\mathrm{S}\\widetilde{\\mathrm{N}}\\mathrm{R}}'
                }
            },
            options: {
                ignoreHtmlClass: 'no-mathjax',
                processHtmlClass: 'mathjax'
            },
            svg: { 
                fontCache: 'global', 
                scale: 1,
                minScale: 0.5,
                mtextInheritFont: true,
                merrorInheritFont: true,
                mathmlSpacing: false,
                skipAttributes: {},
                exFactor: 0.5,
                displayAlign: 'center',
                displayIndent: '0'
            }
        };
    </script>
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>POLO - 통합 결과 미리보기</h1>
            <div>Paper ID: 30</div>
        </div>
<div class="section">
<h2>Abstract</h2>
<div class="paragraph">
<p>우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)에 대한 새로운 접근 방식인 YOLO를 제시합니다. 기존의 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업) 연구들은 분류기를 재활용하여 탐지를 수행했습니다. 하지만 우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)을 공간적으로 분리된 bounding box(물체를 둘러싸는 네모 상자)와 관련 class probability(각 물체가 특정 클래스일 확률)에 대한 regression(연속적인 값을 예측하는 문제)으로 재구성합니다.</p>
</div>
<div class="paragraph">
<p>단일 neural network(뇌 구조를 본뜬 인공 신경망)가 전체 이미지에서 한 번의 평가로 bounding box(물체를 둘러싸는 네모 상자)와 class probability(각 물체가 특정 클래스일 확률)를 직접 예측합니다. 전체 탐지 pipeline(탐지를 위해 단계적으로 처리되는 일련의 과정)이 단일 네트워크이므로 탐지 성능에 대해 end-to-end(입력부터 출력까지 한 번에 처리하는 방식)로 직접 최적화할 수 있습니다.</p>
</div>
<div class="paragraph">
<p>우리의 통합 아키텍처는 극도로 빠릅니다. 기본 YOLO 모델은 real-time(실시간으로 물체를 탐지하는 능력)으로 초당 45프레임으로 이미지를 처리합니다. 더 작은 버전인 Fast YOLO는 놀랍게도 초당 155프레임을 처리하면서도 다른 real-time detector(실시간으로 물체를 탐지하는 능력)의 두 배 mAP(탐지 모델의 평균 정확도 지표)를 달성합니다. 최신 탐지 시스템들과 비교했을 때, YOLO는 더 많은 localization error(물체 위치를 잘못 예측한 오류)를 만들지만 배경에서 false positive(잘못된 양성 예측)를 예측할 가능성은 더 낮습니다. 마지막으로, YOLO는 물체의 매우 일반적인 표현을 학습합니다. 자연 이미지에서 예술 작품과 같은 다른 도메인으로 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력)할 때 DPM(변형 가능한 부품 모델)과 R-CNN(영역 기반 합성곱 신경망)을 포함한 다른 탐지 방법들을 능가합니다.</p>
</div>
</div>
<div class="section">
<h2>Introduction</h2>
<div class="paragraph">
<p>인간은 이미지를 잠깐 보기만 해도 장면 속 물체가 무엇이고 어디 있는지, 서로 어떻게 상호작용하는지 금방 파악합니다. 인간의 시각 시스템은 빠르고 정확하여 운전과 같은 복잡한 작업도 거의 의식적 노력 없이 수행할 수 있습니다. 빠르고 정확한 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업) 알고리즘이 있다면 컴퓨터가 특수 센서 없이도 자동차를 운전할 수 있고, 보조 장치가 인간 사용자에게 real-time(실시간으로 물체를 탐지하는 능력) 장면 정보를 전달할 수 있으며, 범용적이고 반응성 있는 로봇 시스템의 잠재력을 열어줄 것입니다.</p>
</div>
<div class="paragraph">
<p>현재 탐지 시스템들은 분류기를 재활용하여 탐지를 수행합니다. 물체를 탐지하기 위해 이런 시스템들은 해당 물체에 대한 분류기를 가져와서 테스트 이미지의 다양한 위치와 크기에서 평가합니다. DPM(변형 가능한 부품 모델)과 같은 시스템들은 sliding window(창을 일정 간격으로 옮기며 물체를 탐색하는 기법) 접근법을 사용하여 분류기를 전체 이미지에서 균등한 간격으로 실행합니다.</p>
</div>
<div class="paragraph">
<p>R-CNN(영역 기반 합성곱 신경망)과 같은 최신 접근법들은 region proposal(이미지 속 물체 후보 영역을 생성하는 단계) 방법을 사용하여 먼저 이미지에서 잠재적 bounding box(물체를 둘러싸는 네모 상자)를 생성한 다음 이 제안된 박스들에 대해 분류기를 실행합니다. 분류 후에는 후처리를 사용하여 bounding box(물체를 둘러싸는 네모 상자)를 정제하고, 중복 탐지를 제거하며, 장면의 다른 물체들을 기반으로 박스들의 점수를 재계산합니다. 이런 복잡한 pipeline(탐지를 위해 단계적으로 처리되는 일련의 과정)들은 각각의 개별 구성 요소가 따로 훈련되어야 하기 때문에 느리고 최적화하기 어렵습니다.</p>
<div class="image-container">
<img src="/static/viz/figures/system/system_p1.png" alt="easy_paragraph_2_3" />
<div class="image-caption">원본 이미지: easy_paragraph_2_3</div>
</div>
</div>
<div class="paragraph">
<p>우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)을 이미지 픽셀에서 bounding box(물체를 둘러싸는 네모 상자) 좌표와 class probability(각 물체가 특정 클래스일 확률)까지의 단일 regression(연속적인 값을 예측하는 문제) 문제로 재구성합니다. 우리 시스템을 사용하면, 이미지를 단 한 번만 보는 것(YOLO)으로 어떤 물체들이 있고 어디에 있는지 예측할 수 있습니다.</p>
</div>
<div class="paragraph">
<p>YOLO는 단일 convolutional network(이미지 처리에 특화된 신경망)가 동시에 여러 bounding box(물체를 둘러싸는 네모 상자)와 해당 박스들의 class probability(각 물체가 특정 클래스일 확률)를 예측합니다. YOLO는 전체 이미지에서 훈련하고 탐지 성능을 직접 최적화합니다. 이 통합 모델은 전통적인 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업) 방법들에 비해 여러 장점이 있습니다.</p>
</div>
<div class="equation">
<div class="equation-header">수식 (1)</div>
<div class="equation-content mathjax">$$\Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}$$</div>
<div class="equation-explanation">조수
제공된 LaTeX 논문의 맥락 내에서 주어진 방정식을 설명하려면 방정식과 제공된 맥락을 신중하게 분석해야 합니다. 방정식은 다음과 같습니다.

⟦수학0⟧

이 방정식은 예측된 객체의 신뢰도 점수를 나타냅니다. 단계별로 분석해 보겠습니다.

1. **\(\Pr(\textrm{Object})\)**: 이는 해당 객체가 격자 셀에 존재할 확률을 나타냅니다. 객체가 존재하지 않으면 이 확률은 0입니다.

2. **\(\textrm{IOU}_{\textrm{Pred}}^{\textrm{truth}}\)**: 예측된 상자와 실제 값 사이의 Intersection over Union (IOU) 점수를 나타냅니다. IOU 점수는 예측된 상자가 실제 값 상자와 얼마나 잘 일치하는지를 나타내는 척도입니다. 예측된 상자와 실제 값 상자의 교점 면적을 예측된 상자와 실제 값 상자의 합집합 면적으로 나누어 계산합니다.

방정식 전체는 이 두 항의 곱입니다. 즉, 예측된 객체의 신뢰도 점수는 해당 객체가 그리드 셀에 존재할 확률에 예측된 상자와 실제 상자 사이의 유사도 점수 (IOU)를 곱한 값입니다.

### 설명

이 방정식은 예측된 객체의 신뢰도 점수를 계산합니다. 신뢰도 점수는 해당 객체가 그리드 셀에 존재할 확률과 예측된 상자가 실제 상자와 얼마나 일치하는지를 나타내는 척도입니다. 객체가 존재하지 않으면 확률 항은 0이므로 신뢰도 점수도 0입니다. 객체가 존재하면 확률 항은 0과 1 사이의 값이 되고, 신뢰도 점수는 예측된 상자와 실제 상자 사이의 유사도 점수입니다.

### 변수 목록

- **\(\Pr(\textrm{Object})\)**: 객체가 그리드 셀에 존재할 확률입니다.
- **\(\textrm{IOU}_{\textrm{Pred}}^{\textrm{truth}}\)**: 예측된 상자와 실제 상자 사이의 합집합 점수에 대한 교집합입니다.

### 결론

이 방정식은 예측된 객체의 신뢰도 점수를 계산합니다. 객체가 존재하지 않으면 신뢰도 점수는 0입니다. 객체가 존재하는 경우, 신뢰도 점수는 객체가 존재할 확률과 예측된 상자와 실제 상자 사이의 유사도 점수의 곱입니다.

\[
\boxed{\Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{Pred}}^{\textrm</div>
</div>
<div class="paragraph">
<p>첫째, YOLO는 극도로 빠릅니다. 탐지를 regression(연속적인 값을 예측하는 문제) 문제로 구성했기 때문에 복잡한 pipeline(탐지를 위해 단계적으로 처리되는 일련의 과정)이 필요하지 않습니다. 테스트 시에는 새로운 이미지에서 neural network(뇌 구조를 본뜬 인공 신경망)를 실행하여 탐지를 예측하기만 하면 됩니다. 기본 네트워크는 Titan X(NVIDIA의 고성능 그래픽 카드) GPU(그래픽 처리 장치)에서 배치 처리 없이 초당 45프레임으로 실행되고, 빠른 버전은 150 FPS(초당 처리 가능한 프레임 수) 이상으로 실행됩니다. 이는 25밀리초 미만의 지연시간으로 스트리밍 비디오를 real-time(실시간으로 물체를 탐지하는 능력)으로 처리할 수 있음을 의미합니다. 게다가 YOLO는 다른 real-time(실시간으로 물체를 탐지하는 능력) 시스템들의 두 배 이상의 평균 정밀도를 달성합니다.</p>
</div>
<div class="paragraph">
<p>둘째, YOLO는 예측할 때 이미지에 대해 전역적으로 추론합니다. sliding window(창을 일정 간격으로 옮기며 물체를 탐색하는 기법)와 region proposal(이미지 속 물체 후보 영역을 생성하는 단계) 기반 기법들과 달리, YOLO는 훈련과 테스트 시에 전체 이미지를 보기 때문에 클래스들의 외관뿐만 아니라 contextual reasoning(주변 맥락을 고려해 더 정확한 예측을 하는 과정) 정보도 암묵적으로 인코딩합니다. 최고 탐지 방법 중 하나인 Fast R-CNN(빠른 영역 기반 물체 탐지 모델)은 더 큰 맥락을 볼 수 없기 때문에 이미지의 배경 패치를 물체로 잘못 인식합니다. YOLO는 Fast R-CNN(빠른 영역 기반 물체 탐지 모델)에 비해 절반 미만의 배경 오류를 만듭니다.</p>
</div>
<div class="paragraph">
<p>셋째, YOLO는 물체의 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력) 가능한 표현을 학습합니다. 자연 이미지에서 훈련하고 예술 작품에서 테스트했을 때, YOLO는 DPM(변형 가능한 부품 모델)과 R-CNN(영역 기반 합성곱 신경망)과 같은 최고 탐지 방법들을 큰 차이로 능가합니다. YOLO는 높은 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력) 능력을 가지고 있어 새로운 도메인이나 예상치 못한 입력에 적용될 때 실패할 가능성이 적습니다.</p>
</div>
<div class="paragraph">
<p>YOLO는 여전히 정확도 면에서 최신 탐지 시스템들에 뒤처집니다. 이미지에서 물체를 빠르게 식별할 수 있지만 일부 물체들, 특히 작은 물체들을 정확히 위치 파악하는 데 어려움을 겪습니다. 우리는 실험에서 이런 trade-off들을 더 자세히 살펴봅니다.</p>
</div>
<div class="paragraph">
<p>우리의 모든 훈련 및 테스트 코드는 오픈소스입니다. 다양한 사전 훈련된 모델들도 다운로드할 수 있습니다.</p>
</div>
</div>
<div class="section">
<h2>Unified Detection</h2>
<div class="paragraph">
<p>우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)의 개별 구성 요소들을 단일 neural network(뇌 구조를 본뜬 인공 신경망)로 통합합니다. 우리 네트워크는 전체 이미지의 feature(이미지에서 추출된 숫자 표현, 물체를 구분하는 단서)를 사용하여 각 bounding box(물체를 둘러싸는 네모 상자)를 예측합니다. 또한 이미지의 모든 클래스에 대한 모든 bounding box(물체를 둘러싸는 네모 상자)를 동시에 예측합니다. 이는 우리 네트워크가 전체 이미지와 이미지 속 모든 물체들에 대해 전역적으로 추론한다는 의미입니다. YOLO 설계는 높은 평균 정밀도를 유지하면서 end-to-end(입력부터 출력까지 한 번에 처리하는 방식) 훈련과 real-time(실시간으로 물체를 탐지하는 능력) 속도를 가능하게 합니다.</p>
</div>
<div class="paragraph">
<p>우리 시스템은 입력 이미지를 S×S 그리드로 나눕니다. 물체의 중심이 grid cell(이미지를 나누는 격자의 칸)에 속하면 해당 grid cell(이미지를 나누는 격자의 칸)이 그 물체를 탐지할 책임을 집니다.</p>
<div class="image-container">
<img src="/static/viz/figures/model/model_p1.png" alt="easy_paragraph_3_2" />
<div class="image-caption">원본 이미지: easy_paragraph_3_2</div>
</div>
</div>
<div class="paragraph">
<p>각 grid cell(이미지를 나누는 격자의 칸)은 B개의 bounding box(물체를 둘러싸는 네모 상자)와 해당 박스들의 confidence score(예측한 상자가 물체를 제대로 포함할 확률)를 예측합니다. 이 confidence score(예측한 상자가 물체를 제대로 포함할 확률)는 모델이 박스에 물체가 포함되어 있다고 얼마나 확신하는지와 예측한 박스가 얼마나 정확하다고 생각하는지를 반영합니다. 공식적으로 우리는 confidence를 Pr(Object) * IOU(예측 박스와 실제 박스의 겹치는 비율)로 정의합니다. 해당 셀에 물체가 없으면 confidence score(예측한 상자가 물체를 제대로 포함할 확률)는 0이 되어야 합니다. 그렇지 않으면 confidence score(예측한 상자가 물체를 제대로 포함할 확률)가 예측된 박스와 실제 정답 박스 간의 IOU(예측 박스와 실제 박스의 겹치는 비율)와 같기를 원합니다.</p>
</div>
<div class="equation">
<div class="equation-header">수식 (2)</div>
<div class="equation-content mathjax">$$\Pr(\textrm{Class}_i | \textrm{Object})$$</div>
<div class="equation-explanation">조수
논문의 맥락에서 주어진 방정식을 설명하려면 기호와 그 의미를 이해해야 합니다. 방정식은 다음과 같습니다.

⟦수학0⟧

이는 객체가 객체일 때, 객체가 \(i\) 클래스에 속할 확률을 나타냅니다. 논문의 맥락은 다음과 같은 정보를 제공합니다.

1. 이 논문은 이미지에서의 객체 검출에 관한 것입니다.
2. 이 방정식은 객체가 객체일 때, 특정 클래스 \(i\) 객체가 존재할 확률을 나타냅니다.

논문의 맥락은 기호 \(\Pr(\textrm{Class}_i | \textrm{Object})\) 자체를 정의하지는 않지만, 방정식을 이해하는 데 필요한 맥락을 제공합니다. 논문에서는 이 모델이 각 객체에 대해 각 클래스 \(i\)의 확률을 예측한다고 명시합니다. \(\Pr(\textrm{Class}_i | \textrm{Object})\) 표기법은 해당 객체가 객체일 때 확률이 조건부임을 나타냅니다.

이 방정식을 설명하자면 다음과 같습니다.

방정식 \(\Pr(\textrm{Class}_i | \textrm{Object})\)는 객체가 객체일 때 클래스 \(i\)에 속할 확률을 나타냅니다. 이는 논문의 맥락에서 제공되는 조건부 확률입니다. 논문에서는 이 모델이 각 객체에 대해 각 클래스 \(i\)에 속할 확률을 예측하며, 이 확률은 객체가 객체라는 조건 하에 존재한다고 명시합니다.

논문의 맥락은 기호 \(\Pr(\textrm{Class}_i | \textrm{Object})\) 자체를 정의하지는 않지만, 방정식을 이해하는 데 필요한 맥락을 제공합니다. 논문에서는 이 모델이 각 객체에 대해 각 클래스 \(i\)의 확률을 예측한다고 명시합니다. \(\Pr(\textrm{Class}_i | \textrm{Object})\) 표기법은 해당 객체가 객체일 때 확률이 조건부임을 나타냅니다.

따라서 이 방정식에 대한 설명은 다음과 같습니다.

⟦수학0⟧

보조
논문의 맥락은 방정식을 이해하는 데 필요한 맥락을 제공합니다. 논문은 모델이 각 객체에 대해 각 클래스 \(i\)의 확률을 예측한다고 명시합니다. \(\Pr(\textrm{Class}_i | \textrm{Object})\) 표기법은 해당 객체가 객체일 때 확률이 조건임을 나타냅니다.

따라서 이 방정식에 대한 설명은 다음과 같습니다.

⟦수학0⟧</div>
</div>
<div class="paragraph">
<p>각 bounding box(물체를 둘러싸는 네모 상자)는 5개의 예측으로 구성됩니다: x, y, w, h, 그리고 confidence입니다. (x,y) 좌표는 grid cell(이미지를 나누는 격자의 칸)의 경계에 대한 박스 중심의 상대적 위치를 나타냅니다. 너비와 높이는 전체 이미지에 대해 상대적으로 예측됩니다. 마지막으로 confidence 예측은 예측된 박스와 임의의 실제 정답 박스 간의 IOU(예측 박스와 실제 박스의 겹치는 비율)를 나타냅니다.</p>
</div>
<div class="paragraph">
<p>각 grid cell(이미지를 나누는 격자의 칸)은 또한 C개의 조건부 class probability(각 물체가 특정 클래스일 확률), Pr(Class_i | Object)를 예측합니다. 이 확률들은 grid cell(이미지를 나누는 격자의 칸)에 물체가 포함되어 있다는 조건 하에서 계산됩니다. 박스 B의 개수에 관계없이 grid cell(이미지를 나누는 격자의 칸)당 하나의 class probability(각 물체가 특정 클래스일 확률) 집합만 예측합니다.</p>
</div>
<div class="paragraph">
<p>테스트 시에는 조건부 class probability(각 물체가 특정 클래스일 확률)와 개별 박스 confidence 예측을 곱합니다: Pr(Class_i | Object) * Pr(Object) * IOU = Pr(Class_i) * IOU. 이는 각 박스에 대한 클래스별 confidence score(예측한 상자가 물체를 제대로 포함할 확률)를 제공합니다. 이 점수들은 해당 클래스가 박스에 나타날 확률과 예측된 박스가 물체에 얼마나 잘 맞는지를 모두 인코딩합니다.</p>
<div class="image-container">
<img src="/static/viz/figures/net/net_p1.png" alt="easy_paragraph_3_6" />
<div class="image-caption">원본 이미지: easy_paragraph_3_6</div>
</div>
</div>
<div class="paragraph">
<p>PASCAL VOC(물체 탐지 성능을 측정하는 표준 데이터셋)에서 YOLO를 평가하기 위해 S=7, B=2를 사용합니다. PASCAL VOC(물체 탐지 성능을 측정하는 표준 데이터셋)는 20개의 라벨된 클래스를 가지므로 C=20입니다. 최종 예측은 7×7×30 텐서입니다.</p>
</div>
</div>
<div class="section">
<h2>Network Design</h2>
<div class="paragraph">
<p>우리는 이 모델을 convolutional neural network(이미지 처리에 특화된 신경망)로 구현하고 PASCAL VOC(물체 탐지 성능을 측정하는 표준 데이터셋) 탐지 데이터셋에서 평가합니다. 네트워크의 초기 convolutional layer들은 이미지에서 feature(이미지에서 추출된 숫자 표현, 물체를 구분하는 단서)를 추출하고 fully connected layer들은 출력 확률과 좌표를 예측합니다.</p>
</div>
<div class="equation">
<div class="equation-header">수식 (3)</div>
<div class="equation-content mathjax">$$\scriptsize
\Pr(\textrm{Class}_i | \textrm{Object}) * \Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}} = \Pr(\textrm{Class}_i)*\textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}$$</div>
<div class="equation-explanation">조수
제공된 LaTeX 논문의 맥락에서 주어진 방정식을 설명하기 위해 문제에 제시된 단계를 따릅니다. 방정식이 계산하거나 나타내는 내용, 가정 또는 조건을 설명하고, 기호와 그 의미를 나열합니다.

### 설명

주어진 방정식은 다음과 같습니다.

이 방정식은 주어진 객체에 대해 특정 클래스 (Class \(i\))의 확률을 계산하고, 여기에 객체 자체의 확률을 곱한 후, 예측된 상자의 신뢰도 점수 (IOU_{推}^{truth})를 곱한 값을 나타냅니다. 방정식의 우변은 클래스의 확률과 예측된 상자의 신뢰도 점수의 곱입니다.

### 변수 목록

- \(\Pr(\textrm{Class}_i | \textrm{Object})\): 객체가 있을 때 클래스 \(i\)의 확률.
- \(\Pr(\textrm{Object})\): 객체 자체의 확률.
- : 예측된 상자 (IOU_{推}^{truth})의 신뢰도 점수.
- \(\Pr(\textrm{Class}_i)\): 클래스 \(i\)의 확률.
- : 예측된 상자 ( same as on the left-hand side)의 신뢰도 점수.

### 가정 및 조건

1. 객체의 확률은 클래스 확률과 무관합니다.
2. 예측된 상자의 신뢰도 점수는 상자가 객체에 얼마나 잘 맞는지를 나타내는 척도입니다.

### 결론

이 방정식은 주어진 객체에 대한 특정 클래스의 확률, 객체 자체의 확률, 그리고 예측된 상자의 신뢰도 점수를 합산하여 계산합니다. 이 합산 확률은 클래스의 확률과 예측된 상자의 신뢰도 점수를 곱한 값과 같습니다. 이 방정식은 객체 감지에서 예측된 상자의 점수를 매기는 데 사용됩니다.</div>
</div>
<div class="paragraph">
<p>우리 네트워크 아키텍처는 이미지 분류용 GoogLeNet(구글에서 개발한 깊은 신경망 구조) 모델에서 영감을 받았습니다. 우리 네트워크는 24개의 convolutional layer와 그 뒤에 2개의 fully connected layer를 가지고 있습니다. GoogLeNet(구글에서 개발한 깊은 신경망 구조)에서 사용하는 inception 모듈 대신, 우리는 단순히 1×1 reduction layer들과 그 뒤에 3×3 convolutional layer들을 사용합니다.</p>
<div class="image-container">
<img src="/static/viz/figures/pie_compare/pie_compare_p1.png" alt="easy_paragraph_4_2" />
<div class="image-caption">원본 이미지: easy_paragraph_4_2</div>
</div>
</div>
<div class="paragraph">
<p>우리는 또한 빠른 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)의 경계를 넓히기 위해 설계된 빠른 버전의 YOLO를 훈련시킵니다. Fast YOLO는 더 적은 convolutional layer(24개 대신 9개)와 해당 layer들에서 더 적은 filter를 사용하는 neural network(뇌 구조를 본뜬 인공 신경망)를 사용합니다. 네트워크 크기를 제외하고는 YOLO와 Fast YOLO 간의 모든 훈련 및 테스트 매개변수는 동일합니다.</p>
</div>
<div class="paragraph">
<p>우리 네트워크의 최종 출력은 7×7×30 예측 텐서입니다.</p>
</div>
</div>
<div class="section">
<h2>Training</h2>
<div class="paragraph">
<p>우리는 ImageNet(대규모 이미지 분류 데이터셋) 1000-클래스 경쟁 데이터셋에서 convolutional layer들을 사전 훈련합니다. 사전 훈련을 위해 우리는 처음 20개의 convolutional layer와 그 뒤에 average-pooling layer와 fully connected layer를 사용합니다. 이 네트워크를 약 일주일 동안 훈련시키고 ImageNet(대규모 이미지 분류 데이터셋) 2012 validation set(모델 성능을 검증하기 위한 데이터셋)에서 single crop(이미지의 일부분을 잘라내는 것) top-5 정확도 88%를 달성합니다. 이는 Caffe's Model Zoo의 GoogLeNet(구글에서 개발한 깊은 신경망 구조) 모델들과 비교할 만합니다. 우리는 모든 훈련과 추론에 Darknet(YOLO 모델을 구현한 딥러닝 프레임워크) 프레임워크를 사용합니다.</p>
</div>
<div class="paragraph">
<p>그 다음 탐지를 수행하기 위해 모델을 변환합니다. 사전 훈련된 네트워크에 convolutional layer와 connected layer를 모두 추가하면 성능을 향상시킬 수 있다는 연구 결과에 따라, 우리는 무작위로 초기화된 가중치를 가진 4개의 convolutional layer와 2개의 fully connected layer를 추가합니다. 탐지는 종종 세밀한 시각적 정보를 요구하므로 네트워크의 입력 해상도를 224×224에서 448×448로 증가시킵니다.</p>
</div>
<div class="paragraph">
<p>최종 layer는 class probability(각 물체가 특정 클래스일 확률)와 bounding box(물체를 둘러싸는 네모 상자) 좌표를 모두 예측합니다. 우리는 bounding box(물체를 둘러싸는 네모 상자)의 너비와 높이를 이미지 너비와 높이로 정규화하여 0과 1 사이에 오도록 합니다. bounding box(물체를 둘러싸는 네모 상자)의 x와 y 좌표는 특정 grid cell(이미지를 나누는 격자의 칸) 위치의 오프셋으로 매개화하므로 역시 0과 1 사이에 제한됩니다.</p>
</div>
<div class="paragraph">
<p>최종 layer에는 선형 활성화 함수를 사용하고 다른 모든 layer들은 leaky ReLU(음수 값에서도 작은 기울기를 갖는 활성화 함수)를 사용합니다: x > 0이면 x, 그렇지 않으면 0.1x입니다.</p>
</div>
<div class="paragraph">
<p>우리는 모델 출력에서 sum-squared error를 최적화합니다. sum-squared error는 최적화하기 쉽기 때문에 사용하지만, 평균 정밀도 최대화라는 우리 목표와 완벽하게 일치하지는 않습니다. 이는 localization error(물체 위치를 잘못 예측한 오류)와 classification error(물체의 종류를 잘못 분류한 오류)를 동등하게 가중치를 두는데, 이는 이상적이지 않을 수 있습니다. 또한 모든 이미지에서 많은 grid cell(이미지를 나누는 격자의 칸)들이 물체를 포함하지 않습니다. 이는 해당 셀들의 confidence score(예측한 상자가 물체를 제대로 포함할 확률)를 0으로 밀어내며, 종종 물체를 포함하는 셀들의 gradient(함수의 기울기, 학습 방향을 결정)를 압도합니다. 이는 모델 불안정성을 야기하여 훈련 초기에 발산을 일으킬 수 있습니다.</p>
</div>
<div class="paragraph">
<p>이를 해결하기 위해 우리는 bounding box(물체를 둘러싸는 네모 상자) 좌표 예측에서의 loss function(모델이 학습 과정에서 최소화하려는 함수, 오차를 측정)를 증가시키고 물체를 포함하지 않는 박스들의 confidence 예측에서의 loss function(모델이 학습 과정에서 최소화하려는 함수, 오차를 측정)를 감소시킵니다. 이를 위해 두 매개변수 λ_coord = 5와 λ_noobj = 0.5를 사용합니다.</p>
</div>
<div class="paragraph">
<p>sum-squared error는 또한 큰 박스와 작은 박스의 오류에 동등한 가중치를 둡니다. 우리의 오류 메트릭은 큰 박스에서의 작은 편차가 작은 박스에서보다 덜 중요하다는 점을 반영해야 합니다. 이를 부분적으로 해결하기 위해 너비와 높이를 직접 예측하는 대신 bounding box(물체를 둘러싸는 네모 상자) 너비와 높이의 제곱근을 예측합니다.</p>
</div>
<div class="paragraph">
<p>YOLO는 grid cell(이미지를 나누는 격자의 칸)당 여러 bounding box(물체를 둘러싸는 네모 상자)를 예측합니다. 훈련 시에는 각 물체에 대해 하나의 bounding box(물체를 둘러싸는 네모 상자) 예측기만 책임지기를 원합니다. 현재 ground truth(실제 정답 데이터)와 가장 높은 IOU(예측 박스와 실제 박스의 겹치는 비율)를 가진 예측에 기반하여 하나의 예측기를 물체 예측에 '책임'지도록 할당합니다. 이는 bounding box(물체를 둘러싸는 네모 상자) 예측기들 간의 전문화를 이끕니다. 각 예측기는 특정 크기, aspect ratio(가로세로 비율), 또는 물체 클래스를 예측하는 데 더 나아지며, 전체 recall(실제 물체 중 올바르게 찾아낸 비율)을 향상시킵니다.</p>
</div>
</div>
<div class="section">
<h2>Experiments</h2>
<div class="paragraph">
<p>먼저 PASCAL VOC(시각 물체 클래스 데이터셋) 2007에서 YOLO를 다른 real-time detection(실시간으로 물체를 탐지하는 능력) 시스템들과 비교합니다. YOLO와 R-CNN(영역 기반 합성곱 신경망) 변형들의 차이점을 이해하기 위해 VOC(시각 물체 클래스 데이터셋) 2007에서 YOLO와 Fast R-CNN(빠른 영역 기반 물체 탐지 모델)이 만드는 오류들을 탐구합니다.</p>
</div>
<div class="paragraph">
<p>또한 VOC(시각 물체 클래스 데이터셋) 2012 결과를 제시하고 현재 최신 방법들과 mAP(탐지 모델의 평균 정확도 지표)를 비교합니다. 마지막으로 YOLO가 두 개의 예술 작품 데이터셋에서 다른 탐지기들보다 새로운 도메인에 더 잘 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력)됨을 보여줍니다.</p>
<div class="image-container">
<img src="/static/viz/figures/cubist/cubist_p1.png" alt="easy_paragraph_6_2" />
<div class="image-caption">원본 이미지: easy_paragraph_6_2</div>
</div>
</div>
<div class="equation">
<div class="equation-header">수식 (4)</div>
<div class="equation-content mathjax">$$\phi(x) =
\begin{cases}
 x, & \text{if } x > 0\\
 0.1x, & \text{otherwise}
 \end{cases}$$</div>
<div class="equation-explanation">보조
주어진 LaTeX 방정식을 분석하고 제공된 맥락을 기반으로 단계별로 설명해 보겠습니다.

방정식은 다음과 같습니다.
\[
\phi(x) =
\begin{cases}
 x, & \text{if } x > 0\\
 0.1x, & \text{otherwise}
 \end{cases}
\]

### 설명

1. **함수와 그 동작을 식별하세요.**
- 함수 \(\phi(x)\)는 조각 함수입니다. 즉, \(x\)의 값에 따라 정의가 달라집니다.

2. **사례 1: \(x > 0\)**
- \(x\)이 0보다 크면 \(\phi(x)\) 함수는 단순히 \(x\)의 값을 반환합니다. 즉, \(x\)의 값이 양수이면 함수는 \(x\)의 값을 변경하지 않습니다.

3. **사례 2: \(x \leq 0\)**
- \(x\)가 0보다 작거나 같으면 \(\phi(x)\) 함수는 \(0.1x\)를 반환합니다. 즉, \(x\)가 양수가 아닌 값인 경우, 함수는 \(x\)를 0.1배로 조정합니다.

### 변수 및 기호 목록

- \(x\): 이것은 함수 \(\phi\)에 대한 입력입니다.

### 결론

함수 \(\phi(x)\)는 \(x\)가 양수가 아니면 입력 \(x\)의 크기를 0.1배로 조정하고, \(x\)가 양수이면 \(x\)의 값을 변경하지 않고 반환하는 구간 선형 함수입니다. 이 함수는 출력이 항상 음수가 아니도록 하고 음수 입력의 크기를 줄여야 하는 경우에 유용합니다.

최종 답은 다음과 같습니다.
\[
\boxed{\phi(x) =
\begin{cases}
 x, & \text{if } x > 0\\
 0.1x, & \text{otherwise}
 \end{cases}}
\]</div>
</div>
<div class="paragraph">
<p>우리는 PASCAL VOC(시각 물체 클래스 데이터셋) 2007과 2012의 training data(모델 학습에 사용되는 데이터) 및 validation set(모델 성능을 검증하기 위한 데이터셋)에서 네트워크를 약 135 epoch 동안 훈련시킵니다. 2012에서 테스트할 때는 훈련을 위해 VOC(시각 물체 클래스 데이터셋) 2007 테스트 데이터도 포함시킵니다. 훈련 전반에 걸쳐 batch size 64, momentum 0.9, decay 0.0005를 사용합니다.</p>
</div>
<div class="paragraph">
<p>학습률 스케줄은 다음과 같습니다: 처음 epoch들에서는 learning rate(학습 속도를 결정하는 하이퍼파라미터)를 10^-3에서 10^-2로 천천히 올립니다. 높은 learning rate(학습 속도를 결정하는 하이퍼파라미터)로 시작하면 불안정한 gradient(함수의 기울기, 학습 방향을 결정) 때문에 모델이 종종 발산합니다. 75 epoch 동안 10^-2로 계속 훈련한 다음, 30 epoch 동안 10^-3, 마지막으로 30 epoch 동안 10^-4로 훈련합니다.</p>
</div>
<div class="paragraph">
<p>overfitting을 피하기 위해 dropout(과적합을 막기 위해 일부 뉴런을 무작위로 꺼버리는 기법)과 광범위한 data augmentation(데이터를 변형해 다양성을 높이는 기법)을 사용합니다. 첫 번째 connected layer 후의 dropout(과적합을 막기 위해 일부 뉴런을 무작위로 꺼버리는 기법) layer(rate = 0.5)는 layer 간의 공동 적응을 방지합니다. data augmentation(데이터를 변형해 다양성을 높이는 기법)을 위해 원본 이미지 크기의 최대 20%까지 무작위 스케일링과 이동을 도입합니다. 또한 HSV 색 공간에서 이미지의 노출과 채도를 최대 1.5배까지 무작위로 조정합니다.</p>
</div>
</div>
<div class="section">
<h2>Comparison to Other Real-Time Systems</h2>
<div class="paragraph">
<p>object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)의 많은 연구 노력들이 표준 탐지 pipeline(탐지를 위해 단계적으로 처리되는 일련의 과정)을 빠르게 만드는 데 초점을 맞추고 있습니다. 그러나 Sadeghi 등만이 실제로 real-time detection(실시간으로 물체를 탐지하는 능력)(초당 30프레임 이상)으로 실행되는 탐지 시스템을 제작했습니다.</p>
</div>
</div>
<div class="section">
<h2>VOC 2007 Error Analysis</h2>
<div class="paragraph">
<p>YOLO와 최신 탐지기들 간의 차이점을 더 자세히 검토하기 위해 VOC 2007에서 결과의 상세한 분석을 살펴봅니다. Fast R-CNN이 PASCAL에서 가장 높은 성능을 보이는 탐지기 중 하나이므로 YOLO와 Fast R-CNN을 비교합니다.</p>
</div>
<div class="equation">
<div class="equation-header">수식 (5)</div>
<div class="equation-content mathjax">$$x, & \text{if } x > 0\\
 0.1x, & \text{otherwise}$$</div>
<div class="equation-explanation">보조
주어진 LaTeX 방정식을 분석하고 제공된 맥락을 기반으로 단계별로 설명해 보겠습니다.

방정식은 다음과 같습니다.
\[
\phi(x) = \begin{cases} 
x, & \text{if } x > 0\\
0.1x, & \text{otherwise}
\end{cases}
\]

### 설명

1. **방정식과 그 구성 요소를 식별하세요.**
- 방정식은 조각 함수입니다.
- \( x \)의 값에 따라 두 가지 경우가 있습니다.

2. **첫 번째 경우: \( x > 0 \)**
- \( x \)가 0보다 크면 함수는 \( x \) 자체를 반환합니다.
- 이 경우는 간단하며 양수 값에 대한 항등 함수를 나타냅니다.

3. **두 번째 경우: \( x \leq 0 \)**
- \( x \)가 0보다 작거나 같으면 함수는 \( 0.1x \)를 반환합니다.
- 이 경우 음수 값을 0.1씩 조정하여 출력이 원하는 범위 내에 있도록 합니다.

### 변수 목록

- ** 기호: \( x \)**
- 이 기호는 함수의 입력 값을 나타냅니다.
- 문맥에 따라 0과 1 사이의 값으로 제한됩니다.

### 결론

방정식 \(\phi(x)\)는 출력이 항상 [0, 1] 범위 내에 있도록 설계되었습니다. 이 방정식은 양수 및 양수가 아닌 입력을 모두 처리하기 위해 조각별 함수를 사용합니다. 양수 입력의 경우 입력 값 자체를 반환합니다. 양수가 아닌 입력의 경우, 출력이 음수가 아닌 원하는 범위 내에 있도록 입력 값을 0.1만큼 조정합니다.

⟦수학0⟧</div>
</div>
</div>
<div class="section">
<h2>Combining Fast R-CNN and YOLO</h2>
<div class="paragraph">
<p>YOLO는 Fast R-CNN보다 배경 실수를 훨씬 적게 만듭니다. YOLO를 사용하여 Fast R-CNN의 배경 탐지를 제거함으로써 성능에서 상당한 향상을 얻습니다.</p>
</div>
</div>
<div class="section">
<h2>VOC 2012 Results</h2>
<div class="paragraph">
<p>VOC 2012 테스트 세트에서 YOLO는 57.9% mAP를 기록합니다. 이는 현재 최신 기술보다 낮으며, VGG-16을 사용하는 원래 R-CNN에 더 가깝습니다.</p>
</div>
</div>
<div class="section">
<h2>Generalizability: Person Detection in Artwork</h2>
<div class="paragraph">
<p>YOLO의 일반화 능력을 테스트하기 위해 자연 이미지에서 훈련하고 예술 작품에서 사람 탐지를 테스트합니다. 이 실험은 YOLO가 새로운 도메인으로 얼마나 잘 일반화되는지를 보여줍니다.</p>
</div>
<div class="equation">
<div class="equation-header">수식 (6)</div>
<div class="equation-content mathjax">$$\lambda_\textbf{coord}
\sum_{i = 0}^{S^2}
 \sum_{j = 0}^{B}
 \mathlarger{\mathbbm{1}}_{ij}^{\text{obj}}
 \left[
 \left(
 x_i - \hat{x}_i
 \right)^2 +
 \left(
 y_i - \hat{y}_i
 \right)^2
 \right]
\\
+ \lambda_\textbf{coord} 
\sum_{i = 0}^{S^2}
 \sum_{j = 0}^{B}
 \mathlarger{\mathbbm{1}}_{ij}^{\text{obj}}
 \left[
 \left(
 \sqrt{w_i} - \sqrt{\hat{w}_i}
 \right)^2 +
 \left(
 \sqrt{h_i} - \sqrt{\hat{h}_i}
 \right)^2
 \right]
\\
+ \sum_{i = 0}^{S^2}
 \sum_{j = 0}^{B}
 \mathlarger{\mathbbm{1}}_{ij}^{\text{obj}}
 \left(
 C_i - \hat{C}_i
 \right)^2
\\
+ \lambda_\textrm{noobj}
\sum_{i = 0}^{S^2}
 \sum_{j = 0}^{B}
 \mathlarger{\mathbbm{1}}_{ij}^{\text{noobj}}
 \left(
 C_i - \hat{C}_i
 \right)^2
\\
+ \sum_{i = 0}^{S^2}
\mathlarger{\mathbbm{1}}_i^{\text{obj}}
 \sum_{c \in \textrm{classes}}
 \left(
 p_i(c) - \hat{p}_i(c)
 \right)^2$$</div>
<div class="equation-explanation">보조
주어진 LaTeX 방정식을 설명하려면 방정식을 단계별로 분석하고 각 항을 이해해야 합니다. 방정식은 다음과 같습니다.

⟦수학0⟧

자세히 살펴보겠습니다.

1. **\(\lambda_\textbf{coord}\) 및 \(\lambda_\textrm{no objects}\)**: 이는 각각 좌표 및 객체 손실에 대한 손실 함수의 가중치를 제어하는 ​​하이퍼 매개변수입니다.

2. **첫 번째 항**: 이 항은 경계 상자 좌표에 대한 평균 제곱 오차 (MSE)를 계산합니다. 이미지의 모든 경계 상자 (from \(i = 0\) to \(S^2\))와 각 경계 상자 (from \(j = 0\) to \(B\))에 대한 합을 구합니다. \(\mathlarger{\mathbbm{1}}_{ij}^{\text{ obj}}\) 항은 경계 상자가 "객체"이면 1, 그렇지 않으면 0인 지시 함수입니다. 합산 안의 항은 기준 좌표 \((x_i, y_i)\)와 예측 좌표 \((\hat{x}_i, \hat{y}_i)\)의 차이 제곱입니다.

3. **두 번째 항**: 이 항은 경계 상자의 종횡비에 대한 평균 제곱 오차 (MSE)를 계산합니다. 첫 번째 항과 유사하지만 경계 상자의 너비와 높이의 제곱근을 사용합니다. \(\mathlarger{\mathbbm{1}}_{ij}^{\text{ obj}}\) 항은 경계 상자가 "객체"이면 1, 그렇지 않으면 0인 지시 함수입니다. 합산 내의 항은 기준 종횡비 \((\sqrt{w_i}, \sqrt{h_i})\)와 예측 종횡비 \((\sqrt{\hat{w}_i}, \sqrt{\hat{h}_i})\)의 제곱 차이입니다.

4. **세 번째 항**: 이 항은 경계 상자 클래스에 대한 평균 제곱 오차 (MSE)를 계산합니다. 이미지의 모든 경계 상자 (from \(i = 0\) to \(S^2\))와 각 경계 상자 (from \(j = 0\) to \(B\))에 대한 합을 구합니다. \(\mathlarger{\mathbbm{1}}_{ij}^{\text{ objects}}\) 항은 경계 상자가 "객체"이면 1, 그렇지 않으면 0인 지시 함수입니다. 합산 내의 항은 기준 클래스 확률 \((C_i)\)와 예측 클래스 확률 \((\hat{C}_i)\)의 차이 제곱입니다.

5. **네 번째 항**: 이 항은 세 번째 항과 유사하지만 "객체 없음" 경계 상자에 대한 항입니다. 이미지의 모든 경계 상자 (from \(i = 0\) to \(S^2\))와 각 경계 상자 (from \(j = 0\) to \(B\))에 대한 합을 구합니다. \(\mathlarger{\mathbbm{1}}_{ij}^{\text{no objects}}\) 항은 경계 상자에 "객체 없음"이 있으면 1, 그렇지 않으면 0인 지시 함수입니다. 합산 내의 항은 기준 클래스 확률 \((C_i)\)와 예측 클래스 확률 \((\hat{C}_i)\)의 차이 제곱입니다.

6. **다섯 번째 항**: 이 항은 바운딩 박스 클래스에 대한 교차 KL 손실을 계산합니다. 이미지의 모든 바운딩 박스 (from \(i = 0\) to \(S^2\))와 각 바운딩 박스 (from \(j = 0\) to \(B\))에 대한 합을 구합니다. \(\mathlarger{\mathbbm{1}}_i^{\text{ objects}}\) 항은 바운딩 박스가 "객체"이면 1, 그렇지 않으면 0인 지시 함수입니다. 합산 내의 항은 각 클래스 \(c\)에 대한 기준 클래스 확률 \((p_i(c))\)와 예측 클래스 확률 \((\hat{p}_i(c))\)의 제곱 차이입니다.

요약하자면, 이 방정식은 이미지의 경계 상자에 대한 총 손실을 계산하는데, 여기에는 좌표, 종횡비, 클래스, 그리고 교차 KL 손실이 포함됩니다. 손실 함수는 각 항의 기여도를 제어하기 위해 하이퍼파라미터 \(\lambda_\textbf{coord}\)와 \(\lambda_\textrm{no objects}\)의 가중치를 적용합니다.

조수
최종 답은 \(\boxed{1}\)입니다.</div>
</div>
</div>
<div class="section">
<h2>Real-Time Detection In The Wild</h2>
<div class="paragraph">
<p>YOLO는 빠르고 정확한 물체 탐지기로서 컴퓨터 비전 애플리케이션에 이상적입니다. YOLO를 웹캠에 연결하여 카메라에서 이미지를 가져오고 탐지를 표시하는 시간을 포함하여 실시간 성능을 유지함을 확인합니다.</p>
<div class="image-container">
<img src="/static/viz/figures/art/art.jpg" alt="easy_paragraph_12_1" />
<div class="image-caption">원본 이미지: easy_paragraph_12_1</div>
</div>
</div>
</div>
<div class="section">
<h2>Conclusion</h2>
<div class="paragraph">
<p>우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)을 위한 통합 모델인 YOLO를 소개합니다. 우리 모델은 구성하기 간단하고 전체 이미지에서 직접 훈련할 수 있습니다. 분류기 기반 접근법과 달리 YOLO는 탐지 성능에 직접 대응하는 loss function(모델이 학습 과정에서 최소화하려는 함수, 오차를 측정)에서 훈련되며 전체 모델이 함께 훈련됩니다.</p>
</div>
<div class="paragraph">
<p>Fast YOLO는 문헌에서 가장 빠른 범용 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)기이며 YOLO는 real-time detection(실시간으로 물체를 탐지하는 능력)에서 최신 기술을 발전시킵니다. YOLO는 또한 새로운 도메인에 잘 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력)되어 빠르고 견고한 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)에 의존하는 애플리케이션에 이상적입니다.</p>
</div>
</div>

    </div>
    <script>
        // MathJax 렌더링
        if (window.MathJax) {
            window.MathJax.typesetPromise();
        }
    </script>
</body>
</html>
