{
  "paper_info": {
    "paper_id": "yolo_v1_analysis",
    "paper_title": "You Only Look Once: Unified, Real-Time Object Detection",
    "paper_authors": "Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi",
    "paper_venue": "CVPR 2016",
    "paper_date": "2016",
    "total_sections": 13
  },
  "easy_sections": [
    {
      "easy_section_id": "easy_section_1",
      "easy_section_title": "Abstract",
      "easy_section_type": "section",
      "easy_section_order": 1,
      "easy_section_level": 1,
      "easy_content": "우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)에 대한 새로운 접근 방식인 YOLO를 제시합니다. 기존의 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업) 연구들은 분류기를 재활용하여 탐지를 수행했습니다. 하지만 우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)을 공간적으로 분리된 bounding box(물체를 둘러싸는 네모 상자)와 관련 class probability(각 물체가 특정 클래스일 확률)에 대한 regression(연속적인 값을 예측하는 문제)으로 재구성합니다.\n\n단일 neural network(뇌 구조를 본뜬 인공 신경망)가 전체 이미지에서 한 번의 평가로 bounding box(물체를 둘러싸는 네모 상자)와 class probability(각 물체가 특정 클래스일 확률)를 직접 예측합니다. 전체 탐지 pipeline(탐지를 위해 단계적으로 처리되는 일련의 과정)이 단일 네트워크이므로 탐지 성능에 대해 end-to-end(입력부터 출력까지 한 번에 처리하는 방식)로 직접 최적화할 수 있습니다.\n\n우리의 통합 아키텍처는 극도로 빠릅니다. 기본 YOLO 모델은 real-time(실시간으로 물체를 탐지하는 능력)으로 초당 45프레임으로 이미지를 처리합니다. 더 작은 버전인 Fast YOLO는 놀랍게도 초당 155프레임을 처리하면서도 다른 real-time detector(실시간으로 물체를 탐지하는 능력)의 두 배 mAP(탐지 모델의 평균 정확도 지표)를 달성합니다. 최신 탐지 시스템들과 비교했을 때, YOLO는 더 많은 localization error(물체 위치를 잘못 예측한 오류)를 만들지만 배경에서 false positive(잘못된 양성 예측)를 예측할 가능성은 더 낮습니다. 마지막으로, YOLO는 물체의 매우 일반적인 표현을 학습합니다. 자연 이미지에서 예술 작품과 같은 다른 도메인으로 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력)할 때 DPM(변형 가능한 부품 모델)과 R-CNN(영역 기반 합성곱 신경망)을 포함한 다른 탐지 방법들을 능가합니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_1_1",
          "easy_paragraph_text": "우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)에 대한 새로운 접근 방식인 YOLO를 제시합니다. 기존의 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업) 연구들은 분류기를 재활용하여 탐지를 수행했습니다. 하지만 우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)을 공간적으로 분리된 bounding box(물체를 둘러싸는 네모 상자)와 관련 class probability(각 물체가 특정 클래스일 확률)에 대한 regression(연속적인 값을 예측하는 문제)으로 재구성합니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_1_2",
          "easy_paragraph_text": "단일 neural network(뇌 구조를 본뜬 인공 신경망)가 전체 이미지에서 한 번의 평가로 bounding box(물체를 둘러싸는 네모 상자)와 class probability(각 물체가 특정 클래스일 확률)를 직접 예측합니다. 전체 탐지 pipeline(탐지를 위해 단계적으로 처리되는 일련의 과정)이 단일 네트워크이므로 탐지 성능에 대해 end-to-end(입력부터 출력까지 한 번에 처리하는 방식)로 직접 최적화할 수 있습니다.",
          "easy_paragraph_order": 2,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_1_3",
          "easy_paragraph_text": "우리의 통합 아키텍처는 극도로 빠릅니다. 기본 YOLO 모델은 real-time(실시간으로 물체를 탐지하는 능력)으로 초당 45프레임으로 이미지를 처리합니다. 더 작은 버전인 Fast YOLO는 놀랍게도 초당 155프레임을 처리하면서도 다른 real-time detector(실시간으로 물체를 탐지하는 능력)의 두 배 mAP(탐지 모델의 평균 정확도 지표)를 달성합니다. 최신 탐지 시스템들과 비교했을 때, YOLO는 더 많은 localization error(물체 위치를 잘못 예측한 오류)를 만들지만 배경에서 false positive(잘못된 양성 예측)를 예측할 가능성은 더 낮습니다. 마지막으로, YOLO는 물체의 매우 일반적인 표현을 학습합니다. 자연 이미지에서 예술 작품과 같은 다른 도메인으로 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력)할 때 DPM(변형 가능한 부품 모델)과 R-CNN(영역 기반 합성곱 신경망)을 포함한 다른 탐지 방법들을 능가합니다.",
          "easy_paragraph_order": 3,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_2",
      "easy_section_title": "Introduction",
      "easy_section_type": "section",
      "easy_section_order": 2,
      "easy_section_level": 1,
      "easy_content": "인간은 이미지를 잠깐 보기만 해도 장면 속 물체가 무엇이고 어디 있는지, 서로 어떻게 상호작용하는지 금방 파악합니다. 인간의 시각 시스템은 빠르고 정확하여 운전과 같은 복잡한 작업도 거의 의식적 노력 없이 수행할 수 있습니다. 빠르고 정확한 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업) 알고리즘이 있다면 컴퓨터가 특수 센서 없이도 자동차를 운전할 수 있고, 보조 장치가 인간 사용자에게 real-time(실시간으로 물체를 탐지하는 능력) 장면 정보를 전달할 수 있으며, 범용적이고 반응성 있는 로봇 시스템의 잠재력을 열어줄 것입니다.\n\n현재 탐지 시스템들은 분류기를 재활용하여 탐지를 수행합니다. 물체를 탐지하기 위해 이런 시스템들은 해당 물체에 대한 분류기를 가져와서 테스트 이미지의 다양한 위치와 크기에서 평가합니다. DPM(변형 가능한 부품 모델)과 같은 시스템들은 sliding window(창을 일정 간격으로 옮기며 물체를 탐색하는 기법) 접근법을 사용하여 분류기를 전체 이미지에서 균등한 간격으로 실행합니다.\n\nR-CNN(영역 기반 합성곱 신경망)과 같은 최신 접근법들은 region proposal(이미지 속 물체 후보 영역을 생성하는 단계) 방법을 사용하여 먼저 이미지에서 잠재적 bounding box(물체를 둘러싸는 네모 상자)를 생성한 다음 이 제안된 박스들에 대해 분류기를 실행합니다. 분류 후에는 후처리를 사용하여 bounding box(물체를 둘러싸는 네모 상자)를 정제하고, 중복 탐지를 제거하며, 장면의 다른 물체들을 기반으로 박스들의 점수를 재계산합니다. 이런 복잡한 pipeline(탐지를 위해 단계적으로 처리되는 일련의 과정)들은 각각의 개별 구성 요소가 따로 훈련되어야 하기 때문에 느리고 최적화하기 어렵습니다.\n\n우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)을 이미지 픽셀에서 bounding box(물체를 둘러싸는 네모 상자) 좌표와 class probability(각 물체가 특정 클래스일 확률)까지의 단일 regression(연속적인 값을 예측하는 문제) 문제로 재구성합니다. 우리 시스템을 사용하면, 이미지를 단 한 번만 보는 것(YOLO)으로 어떤 물체들이 있고 어디에 있는지 예측할 수 있습니다.\n\nYOLO는 놀랍도록 간단합니다. 단일 convolutional network(이미지 처리에 특화된 신경망)가 동시에 여러 bounding box(물체를 둘러싸는 네모 상자)와 해당 박스들의 class probability(각 물체가 특정 클래스일 확률)를 예측합니다. YOLO는 전체 이미지에서 훈련하고 탐지 성능을 직접 최적화합니다. 이 통합 모델은 전통적인 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업) 방법들에 비해 여러 장점이 있습니다.\n\n첫째, YOLO는 극도로 빠릅니다. 탐지를 regression(연속적인 값을 예측하는 문제) 문제로 구성했기 때문에 복잡한 pipeline(탐지를 위해 단계적으로 처리되는 일련의 과정)이 필요하지 않습니다. 테스트 시에는 새로운 이미지에서 neural network(뇌 구조를 본뜬 인공 신경망)를 실행하여 탐지를 예측하기만 하면 됩니다. 기본 네트워크는 Titan X(NVIDIA의 고성능 그래픽 카드) GPU(그래픽 처리 장치)에서 배치 처리 없이 초당 45프레임으로 실행되고, 빠른 버전은 150 FPS(초당 처리 가능한 프레임 수) 이상으로 실행됩니다. 이는 25밀리초 미만의 지연시간으로 스트리밍 비디오를 real-time(실시간으로 물체를 탐지하는 능력)으로 처리할 수 있음을 의미합니다. 게다가 YOLO는 다른 real-time(실시간으로 물체를 탐지하는 능력) 시스템들의 두 배 이상의 평균 정밀도를 달성합니다.\n\n둘째, YOLO는 예측할 때 이미지에 대해 전역적으로 추론합니다. sliding window(창을 일정 간격으로 옮기며 물체를 탐색하는 기법)와 region proposal(이미지 속 물체 후보 영역을 생성하는 단계) 기반 기법들과 달리, YOLO는 훈련과 테스트 시에 전체 이미지를 보기 때문에 클래스들의 외관뿐만 아니라 contextual reasoning(주변 맥락을 고려해 더 정확한 예측을 하는 과정) 정보도 암묵적으로 인코딩합니다. 최고 탐지 방법 중 하나인 Fast R-CNN(빠른 영역 기반 물체 탐지 모델)은 더 큰 맥락을 볼 수 없기 때문에 이미지의 배경 패치를 물체로 잘못 인식합니다. YOLO는 Fast R-CNN(빠른 영역 기반 물체 탐지 모델)에 비해 절반 미만의 배경 오류를 만듭니다.\n\n셋째, YOLO는 물체의 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력) 가능한 표현을 학습합니다. 자연 이미지에서 훈련하고 예술 작품에서 테스트했을 때, YOLO는 DPM(변형 가능한 부품 모델)과 R-CNN(영역 기반 합성곱 신경망)과 같은 최고 탐지 방법들을 큰 차이로 능가합니다. YOLO는 높은 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력) 능력을 가지고 있어 새로운 도메인이나 예상치 못한 입력에 적용될 때 실패할 가능성이 적습니다.\n\nYOLO는 여전히 정확도 면에서 최신 탐지 시스템들에 뒤처집니다. 이미지에서 물체를 빠르게 식별할 수 있지만 일부 물체들, 특히 작은 물체들을 정확히 위치 파악하는 데 어려움을 겪습니다. 우리는 실험에서 이런 trade-off들을 더 자세히 살펴봅니다.\n\n우리의 모든 훈련 및 테스트 코드는 오픈소스입니다. 다양한 사전 훈련된 모델들도 다운로드할 수 있습니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_2_1",
          "easy_paragraph_text": "인간은 이미지를 잠깐 보기만 해도 장면 속 물체가 무엇이고 어디 있는지, 서로 어떻게 상호작용하는지 금방 파악합니다. 인간의 시각 시스템은 빠르고 정확하여 운전과 같은 복잡한 작업도 거의 의식적 노력 없이 수행할 수 있습니다. 빠르고 정확한 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업) 알고리즘이 있다면 컴퓨터가 특수 센서 없이도 자동차를 운전할 수 있고, 보조 장치가 인간 사용자에게 real-time(실시간으로 물체를 탐지하는 능력) 장면 정보를 전달할 수 있으며, 범용적이고 반응성 있는 로봇 시스템의 잠재력을 열어줄 것입니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_2_2",
          "easy_paragraph_text": "현재 탐지 시스템들은 분류기를 재활용하여 탐지를 수행합니다. 물체를 탐지하기 위해 이런 시스템들은 해당 물체에 대한 분류기를 가져와서 테스트 이미지의 다양한 위치와 크기에서 평가합니다. DPM(변형 가능한 부품 모델)과 같은 시스템들은 sliding window(창을 일정 간격으로 옮기며 물체를 탐색하는 기법) 접근법을 사용하여 분류기를 전체 이미지에서 균등한 간격으로 실행합니다.",
          "easy_paragraph_order": 2,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_2_3",
          "easy_paragraph_text": "R-CNN(영역 기반 합성곱 신경망)과 같은 최신 접근법들은 region proposal(이미지 속 물체 후보 영역을 생성하는 단계) 방법을 사용하여 먼저 이미지에서 잠재적 bounding box(물체를 둘러싸는 네모 상자)를 생성한 다음 이 제안된 박스들에 대해 분류기를 실행합니다. 분류 후에는 후처리를 사용하여 bounding box(물체를 둘러싸는 네모 상자)를 정제하고, 중복 탐지를 제거하며, 장면의 다른 물체들을 기반으로 박스들의 점수를 재계산합니다. 이런 복잡한 pipeline(탐지를 위해 단계적으로 처리되는 일련의 과정)들은 각각의 개별 구성 요소가 따로 훈련되어야 하기 때문에 느리고 최적화하기 어렵습니다.",
          "easy_paragraph_order": 3,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_2_4",
          "easy_paragraph_text": "우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)을 이미지 픽셀에서 bounding box(물체를 둘러싸는 네모 상자) 좌표와 class probability(각 물체가 특정 클래스일 확률)까지의 단일 regression(연속적인 값을 예측하는 문제) 문제로 재구성합니다. 우리 시스템을 사용하면, 이미지를 단 한 번만 보는 것(YOLO)으로 어떤 물체들이 있고 어디에 있는지 예측할 수 있습니다.",
          "easy_paragraph_order": 4,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_2_5",
          "easy_paragraph_text": "YOLO는 단일 convolutional network(이미지 처리에 특화된 신경망)가 동시에 여러 bounding box(물체를 둘러싸는 네모 상자)와 해당 박스들의 class probability(각 물체가 특정 클래스일 확률)를 예측합니다. YOLO는 전체 이미지에서 훈련하고 탐지 성능을 직접 최적화합니다. 이 통합 모델은 전통적인 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업) 방법들에 비해 여러 장점이 있습니다.",
          "easy_paragraph_order": 5,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_2_6",
          "easy_paragraph_text": "첫째, YOLO는 극도로 빠릅니다. 탐지를 regression(연속적인 값을 예측하는 문제) 문제로 구성했기 때문에 복잡한 pipeline(탐지를 위해 단계적으로 처리되는 일련의 과정)이 필요하지 않습니다. 테스트 시에는 새로운 이미지에서 neural network(뇌 구조를 본뜬 인공 신경망)를 실행하여 탐지를 예측하기만 하면 됩니다. 기본 네트워크는 Titan X(NVIDIA의 고성능 그래픽 카드) GPU(그래픽 처리 장치)에서 배치 처리 없이 초당 45프레임으로 실행되고, 빠른 버전은 150 FPS(초당 처리 가능한 프레임 수) 이상으로 실행됩니다. 이는 25밀리초 미만의 지연시간으로 스트리밍 비디오를 real-time(실시간으로 물체를 탐지하는 능력)으로 처리할 수 있음을 의미합니다. 게다가 YOLO는 다른 real-time(실시간으로 물체를 탐지하는 능력) 시스템들의 두 배 이상의 평균 정밀도를 달성합니다.",
          "easy_paragraph_order": 6,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_2_7",
          "easy_paragraph_text": "둘째, YOLO는 예측할 때 이미지에 대해 전역적으로 추론합니다. sliding window(창을 일정 간격으로 옮기며 물체를 탐색하는 기법)와 region proposal(이미지 속 물체 후보 영역을 생성하는 단계) 기반 기법들과 달리, YOLO는 훈련과 테스트 시에 전체 이미지를 보기 때문에 클래스들의 외관뿐만 아니라 contextual reasoning(주변 맥락을 고려해 더 정확한 예측을 하는 과정) 정보도 암묵적으로 인코딩합니다. 최고 탐지 방법 중 하나인 Fast R-CNN(빠른 영역 기반 물체 탐지 모델)은 더 큰 맥락을 볼 수 없기 때문에 이미지의 배경 패치를 물체로 잘못 인식합니다. YOLO는 Fast R-CNN(빠른 영역 기반 물체 탐지 모델)에 비해 절반 미만의 배경 오류를 만듭니다.",
          "easy_paragraph_order": 7,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_2_8",
          "easy_paragraph_text": "셋째, YOLO는 물체의 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력) 가능한 표현을 학습합니다. 자연 이미지에서 훈련하고 예술 작품에서 테스트했을 때, YOLO는 DPM(변형 가능한 부품 모델)과 R-CNN(영역 기반 합성곱 신경망)과 같은 최고 탐지 방법들을 큰 차이로 능가합니다. YOLO는 높은 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력) 능력을 가지고 있어 새로운 도메인이나 예상치 못한 입력에 적용될 때 실패할 가능성이 적습니다.",
          "easy_paragraph_order": 8,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_2_9",
          "easy_paragraph_text": "YOLO는 여전히 정확도 면에서 최신 탐지 시스템들에 뒤처집니다. 이미지에서 물체를 빠르게 식별할 수 있지만 일부 물체들, 특히 작은 물체들을 정확히 위치 파악하는 데 어려움을 겪습니다. 우리는 실험에서 이런 trade-off들을 더 자세히 살펴봅니다.",
          "easy_paragraph_order": 9,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_2_10",
          "easy_paragraph_text": "우리의 모든 훈련 및 테스트 코드는 오픈소스입니다. 다양한 사전 훈련된 모델들도 다운로드할 수 있습니다.",
          "easy_paragraph_order": 10,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_3",
      "easy_section_title": "Unified Detection",
      "easy_section_type": "section",
      "easy_section_order": 3,
      "easy_section_level": 1,
      "easy_content": "우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)의 개별 구성 요소들을 단일 neural network(뇌 구조를 본뜬 인공 신경망)로 통합합니다. 우리 네트워크는 전체 이미지의 feature(이미지에서 추출된 숫자 표현, 물체를 구분하는 단서)를 사용하여 각 bounding box(물체를 둘러싸는 네모 상자)를 예측합니다. 또한 이미지의 모든 클래스에 대한 모든 bounding box(물체를 둘러싸는 네모 상자)를 동시에 예측합니다. 이는 우리 네트워크가 전체 이미지와 이미지 속 모든 물체들에 대해 전역적으로 추론한다는 의미입니다. YOLO 설계는 높은 평균 정밀도를 유지하면서 end-to-end(입력부터 출력까지 한 번에 처리하는 방식) 훈련과 real-time(실시간으로 물체를 탐지하는 능력) 속도를 가능하게 합니다.\n\n우리 시스템은 입력 이미지를 S×S 그리드로 나눕니다. 물체의 중심이 grid cell(이미지를 나누는 격자의 칸)에 속하면 해당 grid cell(이미지를 나누는 격자의 칸)이 그 물체를 탐지할 책임을 집니다.\n\n각 grid cell(이미지를 나누는 격자의 칸)은 B개의 bounding box(물체를 둘러싸는 네모 상자)와 해당 박스들의 confidence score(예측한 상자가 물체를 제대로 포함할 확률)를 예측합니다. 이 confidence score(예측한 상자가 물체를 제대로 포함할 확률)는 모델이 박스에 물체가 포함되어 있다고 얼마나 확신하는지와 예측한 박스가 얼마나 정확하다고 생각하는지를 반영합니다. 공식적으로 우리는 confidence를 Pr(Object) * IOU(예측 박스와 실제 박스의 겹치는 비율)로 정의합니다. 해당 셀에 물체가 없으면 confidence score(예측한 상자가 물체를 제대로 포함할 확률)는 0이 되어야 합니다. 그렇지 않으면 confidence score(예측한 상자가 물체를 제대로 포함할 확률)가 예측된 박스와 실제 정답 박스 간의 IOU(예측 박스와 실제 박스의 겹치는 비율)와 같기를 원합니다.\n\n각 bounding box(물체를 둘러싸는 네모 상자)는 5개의 예측으로 구성됩니다: x, y, w, h, 그리고 confidence입니다. (x,y) 좌표는 grid cell(이미지를 나누는 격자의 칸)의 경계에 대한 박스 중심의 상대적 위치를 나타냅니다. 너비와 높이는 전체 이미지에 대해 상대적으로 예측됩니다. 마지막으로 confidence 예측은 예측된 박스와 임의의 실제 정답 박스 간의 IOU(예측 박스와 실제 박스의 겹치는 비율)를 나타냅니다.\n\n각 grid cell(이미지를 나누는 격자의 칸)은 또한 C개의 조건부 class probability(각 물체가 특정 클래스일 확률), Pr(Class_i | Object)를 예측합니다. 이 확률들은 grid cell(이미지를 나누는 격자의 칸)에 물체가 포함되어 있다는 조건 하에서 계산됩니다. 박스 B의 개수에 관계없이 grid cell(이미지를 나누는 격자의 칸)당 하나의 class probability(각 물체가 특정 클래스일 확률) 집합만 예측합니다.\n\n테스트 시에는 조건부 class probability(각 물체가 특정 클래스일 확률)와 개별 박스 confidence 예측을 곱합니다: Pr(Class_i | Object) * Pr(Object) * IOU = Pr(Class_i) * IOU. 이는 각 박스에 대한 클래스별 confidence score(예측한 상자가 물체를 제대로 포함할 확률)를 제공합니다. 이 점수들은 해당 클래스가 박스에 나타날 확률과 예측된 박스가 물체에 얼마나 잘 맞는지를 모두 인코딩합니다.\n\nPASCAL VOC(물체 탐지 성능을 측정하는 표준 데이터셋)에서 YOLO를 평가하기 위해 S=7, B=2를 사용합니다. PASCAL VOC(물체 탐지 성능을 측정하는 표준 데이터셋)는 20개의 라벨된 클래스를 가지므로 C=20입니다. 최종 예측은 7×7×30 텐서입니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_3_1",
          "easy_paragraph_text": "우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)의 개별 구성 요소들을 단일 neural network(뇌 구조를 본뜬 인공 신경망)로 통합합니다. 우리 네트워크는 전체 이미지의 feature(이미지에서 추출된 숫자 표현, 물체를 구분하는 단서)를 사용하여 각 bounding box(물체를 둘러싸는 네모 상자)를 예측합니다. 또한 이미지의 모든 클래스에 대한 모든 bounding box(물체를 둘러싸는 네모 상자)를 동시에 예측합니다. 이는 우리 네트워크가 전체 이미지와 이미지 속 모든 물체들에 대해 전역적으로 추론한다는 의미입니다. YOLO 설계는 높은 평균 정밀도를 유지하면서 end-to-end(입력부터 출력까지 한 번에 처리하는 방식) 훈련과 real-time(실시간으로 물체를 탐지하는 능력) 속도를 가능하게 합니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_3_2",
          "easy_paragraph_text": "우리 시스템은 입력 이미지를 S×S 그리드로 나눕니다. 물체의 중심이 grid cell(이미지를 나누는 격자의 칸)에 속하면 해당 grid cell(이미지를 나누는 격자의 칸)이 그 물체를 탐지할 책임을 집니다.",
          "easy_paragraph_order": 2,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_3_3",
          "easy_paragraph_text": "각 grid cell(이미지를 나누는 격자의 칸)은 B개의 bounding box(물체를 둘러싸는 네모 상자)와 해당 박스들의 confidence score(예측한 상자가 물체를 제대로 포함할 확률)를 예측합니다. 이 confidence score(예측한 상자가 물체를 제대로 포함할 확률)는 모델이 박스에 물체가 포함되어 있다고 얼마나 확신하는지와 예측한 박스가 얼마나 정확하다고 생각하는지를 반영합니다. 공식적으로 우리는 confidence를 Pr(Object) * IOU(예측 박스와 실제 박스의 겹치는 비율)로 정의합니다. 해당 셀에 물체가 없으면 confidence score(예측한 상자가 물체를 제대로 포함할 확률)는 0이 되어야 합니다. 그렇지 않으면 confidence score(예측한 상자가 물체를 제대로 포함할 확률)가 예측된 박스와 실제 정답 박스 간의 IOU(예측 박스와 실제 박스의 겹치는 비율)와 같기를 원합니다.",
          "easy_paragraph_order": 3,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_3_4",
          "easy_paragraph_text": "각 bounding box(물체를 둘러싸는 네모 상자)는 5개의 예측으로 구성됩니다: x, y, w, h, 그리고 confidence입니다. (x,y) 좌표는 grid cell(이미지를 나누는 격자의 칸)의 경계에 대한 박스 중심의 상대적 위치를 나타냅니다. 너비와 높이는 전체 이미지에 대해 상대적으로 예측됩니다. 마지막으로 confidence 예측은 예측된 박스와 임의의 실제 정답 박스 간의 IOU(예측 박스와 실제 박스의 겹치는 비율)를 나타냅니다.",
          "easy_paragraph_order": 4,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_3_5",
          "easy_paragraph_text": "각 grid cell(이미지를 나누는 격자의 칸)은 또한 C개의 조건부 class probability(각 물체가 특정 클래스일 확률), Pr(Class_i | Object)를 예측합니다. 이 확률들은 grid cell(이미지를 나누는 격자의 칸)에 물체가 포함되어 있다는 조건 하에서 계산됩니다. 박스 B의 개수에 관계없이 grid cell(이미지를 나누는 격자의 칸)당 하나의 class probability(각 물체가 특정 클래스일 확률) 집합만 예측합니다.",
          "easy_paragraph_order": 5,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_3_6",
          "easy_paragraph_text": "테스트 시에는 조건부 class probability(각 물체가 특정 클래스일 확률)와 개별 박스 confidence 예측을 곱합니다: Pr(Class_i | Object) * Pr(Object) * IOU = Pr(Class_i) * IOU. 이는 각 박스에 대한 클래스별 confidence score(예측한 상자가 물체를 제대로 포함할 확률)를 제공합니다. 이 점수들은 해당 클래스가 박스에 나타날 확률과 예측된 박스가 물체에 얼마나 잘 맞는지를 모두 인코딩합니다.",
          "easy_paragraph_order": 6,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_3_7",
          "easy_paragraph_text": "PASCAL VOC(물체 탐지 성능을 측정하는 표준 데이터셋)에서 YOLO를 평가하기 위해 S=7, B=2를 사용합니다. PASCAL VOC(물체 탐지 성능을 측정하는 표준 데이터셋)는 20개의 라벨된 클래스를 가지므로 C=20입니다. 최종 예측은 7×7×30 텐서입니다.",
          "easy_paragraph_order": 7,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_4",
      "easy_section_title": "Network Design",
      "easy_section_type": "subsection",
      "easy_section_order": 4,
      "easy_section_level": 2,
      "parent_section_index": 2,
      "easy_content": "우리는 이 모델을 convolutional neural network(이미지 처리에 특화된 신경망)로 구현하고 PASCAL VOC(물체 탐지 성능을 측정하는 표준 데이터셋) 탐지 데이터셋에서 평가합니다. 네트워크의 초기 convolutional layer들은 이미지에서 feature(이미지에서 추출된 숫자 표현, 물체를 구분하는 단서)를 추출하고 fully connected layer들은 출력 확률과 좌표를 예측합니다.\n\n우리 네트워크 아키텍처는 이미지 분류용 GoogLeNet(구글에서 개발한 깊은 신경망 구조) 모델에서 영감을 받았습니다. 우리 네트워크는 24개의 convolutional layer와 그 뒤에 2개의 fully connected layer를 가지고 있습니다. GoogLeNet(구글에서 개발한 깊은 신경망 구조)에서 사용하는 inception 모듈 대신, 우리는 단순히 1×1 reduction layer들과 그 뒤에 3×3 convolutional layer들을 사용합니다.\n\n우리는 또한 빠른 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)의 경계를 넓히기 위해 설계된 빠른 버전의 YOLO를 훈련시킵니다. Fast YOLO는 더 적은 convolutional layer(24개 대신 9개)와 해당 layer들에서 더 적은 filter를 사용하는 neural network(뇌 구조를 본뜬 인공 신경망)를 사용합니다. 네트워크 크기를 제외하고는 YOLO와 Fast YOLO 간의 모든 훈련 및 테스트 매개변수는 동일합니다.\n\n우리 네트워크의 최종 출력은 7×7×30 예측 텐서입니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_4_1",
          "easy_paragraph_text": "우리는 이 모델을 convolutional neural network(이미지 처리에 특화된 신경망)로 구현하고 PASCAL VOC(물체 탐지 성능을 측정하는 표준 데이터셋) 탐지 데이터셋에서 평가합니다. 네트워크의 초기 convolutional layer들은 이미지에서 feature(이미지에서 추출된 숫자 표현, 물체를 구분하는 단서)를 추출하고 fully connected layer들은 출력 확률과 좌표를 예측합니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_4_2",
          "easy_paragraph_text": "우리 네트워크 아키텍처는 이미지 분류용 GoogLeNet(구글에서 개발한 깊은 신경망 구조) 모델에서 영감을 받았습니다. 우리 네트워크는 24개의 convolutional layer와 그 뒤에 2개의 fully connected layer를 가지고 있습니다. GoogLeNet(구글에서 개발한 깊은 신경망 구조)에서 사용하는 inception 모듈 대신, 우리는 단순히 1×1 reduction layer들과 그 뒤에 3×3 convolutional layer들을 사용합니다.",
          "easy_paragraph_order": 2,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_4_3",
          "easy_paragraph_text": "우리는 또한 빠른 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)의 경계를 넓히기 위해 설계된 빠른 버전의 YOLO를 훈련시킵니다. Fast YOLO는 더 적은 convolutional layer(24개 대신 9개)와 해당 layer들에서 더 적은 filter를 사용하는 neural network(뇌 구조를 본뜬 인공 신경망)를 사용합니다. 네트워크 크기를 제외하고는 YOLO와 Fast YOLO 간의 모든 훈련 및 테스트 매개변수는 동일합니다.",
          "easy_paragraph_order": 3,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_4_4",
          "easy_paragraph_text": "우리 네트워크의 최종 출력은 7×7×30 예측 텐서입니다.",
          "easy_paragraph_order": 4,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_5",
      "easy_section_title": "Training",
      "easy_section_type": "subsection",
      "easy_section_order": 5,
      "easy_section_level": 2,
      "parent_section_index": 2,
      "easy_content": "우리는 ImageNet(대규모 이미지 분류 데이터셋) 1000-클래스 경쟁 데이터셋에서 convolutional layer들을 사전 훈련합니다. 사전 훈련을 위해 우리는 처음 20개의 convolutional layer와 그 뒤에 average-pooling layer와 fully connected layer를 사용합니다. 이 네트워크를 약 일주일 동안 훈련시키고 ImageNet(대규모 이미지 분류 데이터셋) 2012 검증 세트에서 single crop top-5 정확도 88%를 달성합니다. 이는 Caffe's Model Zoo의 GoogLeNet(구글에서 개발한 깊은 신경망 구조) 모델들과 비교할 만합니다. 우리는 모든 훈련과 추론에 Darknet(YOLO 모델을 구현한 딥러닝 프레임워크) 프레임워크를 사용합니다.\n\n그 다음 탐지를 수행하기 위해 모델을 변환합니다. 사전 훈련된 네트워크에 convolutional layer와 connected layer를 모두 추가하면 성능을 향상시킬 수 있다는 연구 결과에 따라, 우리는 무작위로 초기화된 가중치를 가진 4개의 convolutional layer와 2개의 fully connected layer를 추가합니다. 탐지는 종종 세밀한 시각적 정보를 요구하므로 네트워크의 입력 해상도를 224×224에서 448×448로 증가시킵니다.\n\n최종 layer는 class probability(각 물체가 특정 클래스일 확률)와 bounding box(물체를 둘러싸는 네모 상자) 좌표를 모두 예측합니다. 우리는 bounding box(물체를 둘러싸는 네모 상자)의 너비와 높이를 이미지 너비와 높이로 정규화하여 0과 1 사이에 오도록 합니다. bounding box(물체를 둘러싸는 네모 상자)의 x와 y 좌표는 특정 grid cell(이미지를 나누는 격자의 칸) 위치의 오프셋으로 매개화하므로 역시 0과 1 사이에 제한됩니다.\n\n최종 layer에는 선형 활성화 함수를 사용하고 다른 모든 layer들은 leaky ReLU(음수 값에서도 작은 기울기를 갖는 활성화 함수)를 사용합니다: x > 0이면 x, 그렇지 않으면 0.1x입니다.\n\n우리는 모델 출력에서 sum-squared error를 최적화합니다. sum-squared error는 최적화하기 쉽기 때문에 사용하지만, 평균 정밀도 최대화라는 우리 목표와 완벽하게 일치하지는 않습니다. 이는 localization error(물체 위치를 잘못 예측한 오류)와 classification error(물체의 종류를 잘못 분류한 오류)를 동등하게 가중치를 두는데, 이는 이상적이지 않을 수 있습니다. 또한 모든 이미지에서 많은 grid cell(이미지를 나누는 격자의 칸)들이 물체를 포함하지 않습니다. 이는 해당 셀들의 confidence score(예측한 상자가 물체를 제대로 포함할 확률)를 0으로 밀어내며, 종종 물체를 포함하는 셀들의 gradient를 압도합니다. 이는 모델 불안정성을 야기하여 훈련 초기에 발산을 일으킬 수 있습니다.\n\n이를 해결하기 위해 우리는 bounding box(물체를 둘러싸는 네모 상자) 좌표 예측에서의 loss를 증가시키고 물체를 포함하지 않는 박스들의 confidence 예측에서의 loss를 감소시킵니다. 이를 위해 두 매개변수 λ_coord = 5와 λ_noobj = 0.5를 사용합니다.\n\nsum-squared error는 또한 큰 박스와 작은 박스의 오류에 동등한 가중치를 둡니다. 우리의 오류 메트릭은 큰 박스에서의 작은 편차가 작은 박스에서보다 덜 중요하다는 점을 반영해야 합니다. 이를 부분적으로 해결하기 위해 너비와 높이를 직접 예측하는 대신 bounding box(물체를 둘러싸는 네모 상자) 너비와 높이의 제곱근을 예측합니다.\n\nYOLO는 grid cell(이미지를 나누는 격자의 칸)당 여러 bounding box(물체를 둘러싸는 네모 상자)를 예측합니다. 훈련 시에는 각 물체에 대해 하나의 bounding box(물체를 둘러싸는 네모 상자) 예측기만 책임지기를 원합니다. 현재 ground truth와 가장 높은 IOU(예측 박스와 실제 박스의 겹치는 비율)를 가진 예측에 기반하여 하나의 예측기를 물체 예측에 '책임'지도록 할당합니다. 이는 bounding box(물체를 둘러싸는 네모 상자) 예측기들 간의 전문화를 이끕니다. 각 예측기는 특정 크기, 종횡비, 또는 물체 클래스를 예측하는 데 더 나아지며, 전체 recall을 향상시킵니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_5_1",
          "easy_paragraph_text": "우리는 ImageNet(대규모 이미지 분류 데이터셋) 1000-클래스 경쟁 데이터셋에서 convolutional layer들을 사전 훈련합니다. 사전 훈련을 위해 우리는 처음 20개의 convolutional layer와 그 뒤에 average-pooling layer와 fully connected layer를 사용합니다. 이 네트워크를 약 일주일 동안 훈련시키고 ImageNet(대규모 이미지 분류 데이터셋) 2012 validation set(모델 성능을 검증하기 위한 데이터셋)에서 single crop(이미지의 일부분을 잘라내는 것) top-5 정확도 88%를 달성합니다. 이는 Caffe's Model Zoo의 GoogLeNet(구글에서 개발한 깊은 신경망 구조) 모델들과 비교할 만합니다. 우리는 모든 훈련과 추론에 Darknet(YOLO 모델을 구현한 딥러닝 프레임워크) 프레임워크를 사용합니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_5_2",
          "easy_paragraph_text": "그 다음 탐지를 수행하기 위해 모델을 변환합니다. 사전 훈련된 네트워크에 convolutional layer와 connected layer를 모두 추가하면 성능을 향상시킬 수 있다는 연구 결과에 따라, 우리는 무작위로 초기화된 가중치를 가진 4개의 convolutional layer와 2개의 fully connected layer를 추가합니다. 탐지는 종종 세밀한 시각적 정보를 요구하므로 네트워크의 입력 해상도를 224×224에서 448×448로 증가시킵니다.",
          "easy_paragraph_order": 2,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_5_3",
          "easy_paragraph_text": "최종 layer는 class probability(각 물체가 특정 클래스일 확률)와 bounding box(물체를 둘러싸는 네모 상자) 좌표를 모두 예측합니다. 우리는 bounding box(물체를 둘러싸는 네모 상자)의 너비와 높이를 이미지 너비와 높이로 정규화하여 0과 1 사이에 오도록 합니다. bounding box(물체를 둘러싸는 네모 상자)의 x와 y 좌표는 특정 grid cell(이미지를 나누는 격자의 칸) 위치의 오프셋으로 매개화하므로 역시 0과 1 사이에 제한됩니다.",
          "easy_paragraph_order": 3,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_5_4",
          "easy_paragraph_text": "최종 layer에는 선형 활성화 함수를 사용하고 다른 모든 layer들은 leaky ReLU(음수 값에서도 작은 기울기를 갖는 활성화 함수)를 사용합니다: x > 0이면 x, 그렇지 않으면 0.1x입니다.",
          "easy_paragraph_order": 4,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_5_5",
          "easy_paragraph_text": "우리는 모델 출력에서 sum-squared error를 최적화합니다. sum-squared error는 최적화하기 쉽기 때문에 사용하지만, 평균 정밀도 최대화라는 우리 목표와 완벽하게 일치하지는 않습니다. 이는 localization error(물체 위치를 잘못 예측한 오류)와 classification error(물체의 종류를 잘못 분류한 오류)를 동등하게 가중치를 두는데, 이는 이상적이지 않을 수 있습니다. 또한 모든 이미지에서 많은 grid cell(이미지를 나누는 격자의 칸)들이 물체를 포함하지 않습니다. 이는 해당 셀들의 confidence score(예측한 상자가 물체를 제대로 포함할 확률)를 0으로 밀어내며, 종종 물체를 포함하는 셀들의 gradient(함수의 기울기, 학습 방향을 결정)를 압도합니다. 이는 모델 불안정성을 야기하여 훈련 초기에 발산을 일으킬 수 있습니다.",
          "easy_paragraph_order": 5,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_5_6",
          "easy_paragraph_text": "이를 해결하기 위해 우리는 bounding box(물체를 둘러싸는 네모 상자) 좌표 예측에서의 loss function(모델이 학습 과정에서 최소화하려는 함수, 오차를 측정)를 증가시키고 물체를 포함하지 않는 박스들의 confidence 예측에서의 loss function(모델이 학습 과정에서 최소화하려는 함수, 오차를 측정)를 감소시킵니다. 이를 위해 두 매개변수 λ_coord = 5와 λ_noobj = 0.5를 사용합니다.",
          "easy_paragraph_order": 6,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_5_7",
          "easy_paragraph_text": "sum-squared error는 또한 큰 박스와 작은 박스의 오류에 동등한 가중치를 둡니다. 우리의 오류 메트릭은 큰 박스에서의 작은 편차가 작은 박스에서보다 덜 중요하다는 점을 반영해야 합니다. 이를 부분적으로 해결하기 위해 너비와 높이를 직접 예측하는 대신 bounding box(물체를 둘러싸는 네모 상자) 너비와 높이의 제곱근을 예측합니다.",
          "easy_paragraph_order": 7,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_5_8",
          "easy_paragraph_text": "YOLO는 grid cell(이미지를 나누는 격자의 칸)당 여러 bounding box(물체를 둘러싸는 네모 상자)를 예측합니다. 훈련 시에는 각 물체에 대해 하나의 bounding box(물체를 둘러싸는 네모 상자) 예측기만 책임지기를 원합니다. 현재 ground truth(실제 정답 데이터)와 가장 높은 IOU(예측 박스와 실제 박스의 겹치는 비율)를 가진 예측에 기반하여 하나의 예측기를 물체 예측에 '책임'지도록 할당합니다. 이는 bounding box(물체를 둘러싸는 네모 상자) 예측기들 간의 전문화를 이끕니다. 각 예측기는 특정 크기, aspect ratio(가로세로 비율), 또는 물체 클래스를 예측하는 데 더 나아지며, 전체 recall(실제 물체 중 올바르게 찾아낸 비율)을 향상시킵니다.",
          "easy_paragraph_order": 8,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_6",
      "easy_section_title": "Experiments",
      "easy_section_type": "section",
      "easy_section_order": 6,
      "easy_section_level": 1,
      "easy_content": "먼저 PASCAL VOC(시각 물체 클래스 데이터셋) 2007에서 YOLO를 다른 real-time detection(실시간으로 물체를 탐지하는 능력) 시스템들과 비교합니다. YOLO와 R-CNN(영역 기반 합성곱 신경망) 변형들의 차이점을 이해하기 위해 VOC(시각 물체 클래스 데이터셋) 2007에서 YOLO와 Fast R-CNN(빠른 영역 기반 물체 탐지 모델)이 만드는 오류들을 탐구합니다.\n\n또한 VOC(시각 물체 클래스 데이터셋) 2012 결과를 제시하고 현재 최신 방법들과 mAP(탐지 모델의 평균 정확도 지표)를 비교합니다. 마지막으로 YOLO가 두 개의 예술 작품 데이터셋에서 다른 탐지기들보다 새로운 도메인에 더 잘 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력)됨을 보여줍니다.\n\n우리는 PASCAL VOC(시각 물체 클래스 데이터셋) 2007과 2012의 훈련 및 검증 데이터셋에서 네트워크를 약 135 epoch 동안 훈련시킵니다. 2012에서 테스트할 때는 훈련을 위해 VOC(시각 물체 클래스 데이터셋) 2007 테스트 데이터도 포함시킵니다. 훈련 전반에 걸쳐 batch size 64, momentum 0.9, decay 0.0005를 사용합니다.\n\n학습률 스케줄은 다음과 같습니다: 처음 epoch들에서는 학습률을 10^-3에서 10^-2로 천천히 올립니다. 높은 학습률로 시작하면 불안정한 gradient(함수의 기울기, 학습 방향을 결정) 때문에 모델이 종종 발산합니다. 75 epoch 동안 10^-2로 계속 훈련한 다음, 30 epoch 동안 10^-3, 마지막으로 30 epoch 동안 10^-4로 훈련합니다.\n\nOverfitting을 피하기 위해 dropout(과적합을 막기 위해 일부 뉴런을 무작위로 꺼버리는 기법)과 광범위한 data augmentation(데이터를 변형해 다양성을 높이는 기법)을 사용합니다. 첫 번째 connected layer 후의 dropout(과적합을 막기 위해 일부 뉴런을 무작위로 꺼버리는 기법) layer(rate = 0.5)는 layer 간의 공동 적응을 방지합니다. data augmentation(데이터를 변형해 다양성을 높이는 기법)을 위해 원본 이미지 크기의 최대 20%까지 무작위 스케일링과 이동을 도입합니다. 또한 HSV 색 공간에서 이미지의 노출과 채도를 최대 1.5배까지 무작위로 조정합니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_6_1",
          "easy_paragraph_text": "먼저 PASCAL VOC(시각 물체 클래스 데이터셋) 2007에서 YOLO를 다른 real-time detection(실시간으로 물체를 탐지하는 능력) 시스템들과 비교합니다. YOLO와 R-CNN(영역 기반 합성곱 신경망) 변형들의 차이점을 이해하기 위해 VOC(시각 물체 클래스 데이터셋) 2007에서 YOLO와 Fast R-CNN(빠른 영역 기반 물체 탐지 모델)이 만드는 오류들을 탐구합니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_6_2",
          "easy_paragraph_text": "또한 VOC(시각 물체 클래스 데이터셋) 2012 결과를 제시하고 현재 최신 방법들과 mAP(탐지 모델의 평균 정확도 지표)를 비교합니다. 마지막으로 YOLO가 두 개의 예술 작품 데이터셋에서 다른 탐지기들보다 새로운 도메인에 더 잘 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력)됨을 보여줍니다.",
          "easy_paragraph_order": 2,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_6_3",
          "easy_paragraph_text": "우리는 PASCAL VOC(시각 물체 클래스 데이터셋) 2007과 2012의 training data(모델 학습에 사용되는 데이터) 및 validation set(모델 성능을 검증하기 위한 데이터셋)에서 네트워크를 약 135 epoch 동안 훈련시킵니다. 2012에서 테스트할 때는 훈련을 위해 VOC(시각 물체 클래스 데이터셋) 2007 테스트 데이터도 포함시킵니다. 훈련 전반에 걸쳐 batch size 64, momentum 0.9, decay 0.0005를 사용합니다.",
          "easy_paragraph_order": 3,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_6_4",
          "easy_paragraph_text": "학습률 스케줄은 다음과 같습니다: 처음 epoch들에서는 learning rate(학습 속도를 결정하는 하이퍼파라미터)를 10^-3에서 10^-2로 천천히 올립니다. 높은 learning rate(학습 속도를 결정하는 하이퍼파라미터)로 시작하면 불안정한 gradient(함수의 기울기, 학습 방향을 결정) 때문에 모델이 종종 발산합니다. 75 epoch 동안 10^-2로 계속 훈련한 다음, 30 epoch 동안 10^-3, 마지막으로 30 epoch 동안 10^-4로 훈련합니다.",
          "easy_paragraph_order": 4,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_6_5",
          "easy_paragraph_text": "overfitting을 피하기 위해 dropout(과적합을 막기 위해 일부 뉴런을 무작위로 꺼버리는 기법)과 광범위한 data augmentation(데이터를 변형해 다양성을 높이는 기법)을 사용합니다. 첫 번째 connected layer 후의 dropout(과적합을 막기 위해 일부 뉴런을 무작위로 꺼버리는 기법) layer(rate = 0.5)는 layer 간의 공동 적응을 방지합니다. data augmentation(데이터를 변형해 다양성을 높이는 기법)을 위해 원본 이미지 크기의 최대 20%까지 무작위 스케일링과 이동을 도입합니다. 또한 HSV 색 공간에서 이미지의 노출과 채도를 최대 1.5배까지 무작위로 조정합니다.",
          "easy_paragraph_order": 5,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_7",
      "easy_section_title": "Comparison to Other Real-Time Systems",
      "easy_section_type": "subsection",
      "easy_section_order": 7,
      "easy_section_level": 2,
      "parent_section_index": 5,
      "easy_content": "object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)의 많은 연구 노력들이 표준 탐지 pipeline(탐지를 위해 단계적으로 처리되는 일련의 과정)을 빠르게 만드는 데 초점을 맞추고 있습니다. 그러나 Sadeghi 등만이 실제로 real-time(실시간으로 물체를 탐지하는 능력)(초당 30프레임 이상)으로 실행되는 탐지 시스템을 제작했습니다. 우리는 YOLO를 30Hz 또는 100Hz로 실행되는 DPM(변형 가능한 부품 모델)의 GPU(그래픽 처리 장치) 구현과 비교합니다. 다른 노력들은 real-time(실시간으로 물체를 탐지하는 능력) 목표에 도달하지 못하지만, 우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업) 시스템에서 사용 가능한 정확도-성능 trade-off(서로 상충하는 요소들 간의 균형)를 검토하기 위해 상대적 mAP(탐지 모델의 평균 정확도 지표)와 속도도 비교합니다.\n\nFast YOLO는 PASCAL에서 가장 빠른 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업) 방법입니다. 52.7% mAP(탐지 모델의 평균 정확도 지표)로 real-time detection(실시간으로 물체를 탐지하는 능력)에 대한 이전 연구보다 두 배 이상 정확합니다. YOLO는 여전히 real-time(실시간으로 물체를 탐지하는 능력) 성능을 유지하면서 mAP(탐지 모델의 평균 정확도 지표)를 63.4%로 끌어올립니다. 이는 다른 real-time(실시간으로 물체를 탐지하는 능력) 방법들보다 10 mAP(탐지 모델의 평균 정확도 지표) 이상 높으며 Fast R-CNN(빠른 영역 기반 물체 탐지 모델)과 같은 정확도를 달성합니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_7_1",
          "easy_paragraph_text": "object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)의 많은 연구 노력들이 표준 탐지 pipeline(탐지를 위해 단계적으로 처리되는 일련의 과정)을 빠르게 만드는 데 초점을 맞추고 있습니다. 그러나 Sadeghi 등만이 실제로 real-time detection(실시간으로 물체를 탐지하는 능력)(초당 30프레임 이상)으로 실행되는 탐지 시스템을 제작했습니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_8",
      "easy_section_title": "VOC 2007 Error Analysis",
      "easy_section_type": "subsection",
      "easy_section_order": 8,
      "easy_section_level": 2,
      "parent_section_index": 5,
      "easy_content": "YOLO와 최신 탐지기들 간의 차이점을 더 자세히 검토하기 위해 VOC(시각 물체 클래스 데이터셋) 2007에서 결과의 상세한 분석을 살펴봅니다. Fast R-CNN(빠른 영역 기반 물체 탐지 모델)이 PASCAL에서 가장 높은 성능을 보이는 탐지기 중 하나이고 탐지 결과가 공개적으로 사용 가능하므로 YOLO와 Fast R-CNN(빠른 영역 기반 물체 탐지 모델)을 비교합니다.\n\n우리는 Hoiem 등의 방법론과 도구를 사용합니다. 테스트 시 각 카테고리에 대해 해당 카테고리의 상위 N개 예측을 살펴봅니다. 각 예측은 올바르거나 오류 유형에 따라 분류됩니다: Correct(올바른 클래스이고 IOU(예측 박스와 실제 박스의 겹치는 비율) > 0.5), Localization(올바른 클래스이지만 0.1 < IOU(예측 박스와 실제 박스의 겹치는 비율) < 0.5), Similar(유사한 클래스, IOU(예측 박스와 실제 박스의 겹치는 비율) > 0.1), Other(틀린 클래스, IOU(예측 박스와 실제 박스의 겹치는 비율) > 0.1), Background(임의 물체에 대해 IOU(예측 박스와 실제 박스의 겹치는 비율) < 0.1).\n\nYOLO는 물체를 올바르게 위치 파악하는 데 어려움을 겪습니다. localization error(물체 위치를 잘못 예측한 오류)가 다른 모든 원인을 합친 것보다 YOLO의 오류에서 더 많은 부분을 차지합니다. Fast R-CNN(빠른 영역 기반 물체 탐지 모델)은 localization error(물체 위치를 잘못 예측한 오류)는 훨씬 적게 만들지만 배경 오류는 훨씬 많이 만듭니다. 상위 탐지 결과의 13.6%가 물체를 포함하지 않는 false positive(잘못된 양성 예측)입니다. Fast R-CNN(빠른 영역 기반 물체 탐지 모델)은 YOLO보다 배경 탐지를 예측할 가능성이 거의 3배 높습니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_8_1",
          "easy_paragraph_text": "YOLO와 최신 탐지기들 간의 차이점을 더 자세히 검토하기 위해 VOC 2007에서 결과의 상세한 분석을 살펴봅니다. Fast R-CNN이 PASCAL에서 가장 높은 성능을 보이는 탐지기 중 하나이므로 YOLO와 Fast R-CNN을 비교합니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_9",
      "easy_section_title": "Combining Fast R-CNN and YOLO",
      "easy_section_type": "subsection",
      "easy_section_order": 9,
      "easy_section_level": 2,
      "parent_section_index": 5,
      "easy_content": "YOLO는 Fast R-CNN보다 배경 실수를 훨씬 적게 만듭니다. YOLO를 사용하여 Fast R-CNN의 배경 탐지를 제거함으로써 성능에서 상당한 향상을 얻습니다.\n\n최고의 Fast R-CNN 모델은 VOC 2007 테스트 세트에서 71.8%의 mAP를 달성합니다. YOLO와 결합했을 때 mAP가 3.2% 증가하여 75.0%가 됩니다. YOLO로부터의 향상은 단순히 모델 앙상블의 부산물이 아닙니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_9_1",
          "easy_paragraph_text": "YOLO는 Fast R-CNN보다 배경 실수를 훨씬 적게 만듭니다. YOLO를 사용하여 Fast R-CNN의 배경 탐지를 제거함으로써 성능에서 상당한 향상을 얻습니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_10",
      "easy_section_title": "VOC 2012 Results",
      "easy_section_type": "subsection",
      "easy_section_order": 10,
      "easy_section_level": 2,
      "parent_section_index": 5,
      "easy_content": "VOC 2012 테스트 세트에서 YOLO는 57.9% mAP를 기록합니다. 이는 현재 최신 기술보다 낮으며, VGG-16을 사용하는 원래 R-CNN에 더 가깝습니다.\n\n우리 시스템은 가장 가까운 경쟁자들에 비해 작은 물체들과 어려움을 겪습니다. bottle, sheep, tv/monitor와 같은 카테고리에서 YOLO는 R-CNN이나 Feature Edit보다 8-10% 낮은 점수를 받습니다. 하지만 cat이나 train과 같은 다른 카테고리에서는 YOLO가 더 높은 성능을 달성합니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_10_1",
          "easy_paragraph_text": "VOC 2012 테스트 세트에서 YOLO는 57.9% mAP를 기록합니다. 이는 현재 최신 기술보다 낮으며, VGG-16을 사용하는 원래 R-CNN에 더 가깝습니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_11",
      "easy_section_title": "Generalizability: Person Detection in Artwork",
      "easy_section_type": "subsection",
      "easy_section_order": 11,
      "easy_section_level": 2,
      "parent_section_index": 5,
      "easy_content": "YOLO의 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력) 능력을 테스트하기 위해 자연 이미지에서 훈련하고 예술 작품에서 사람 탐지를 테스트합니다. 이 실험은 YOLO가 새로운 도메인으로 얼마나 잘 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력)되는지를 보여줍니다.\n\nPicasso 데이터셋과 People-Art 데이터셋에서 YOLO는 DPM(변형 가능한 부품 모델)이나 R-CNN(영역 기반 합성곱 신경망)보다 상당히 더 나은 성능을 보입니다. YOLO는 예술 작품에서 사람을 탐지할 때 R-CNN(영역 기반 합성곱 신경망)보다 AP에서 두 배 이상 높은 성능을 달성합니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_11_1",
          "easy_paragraph_text": "YOLO의 일반화 능력을 테스트하기 위해 자연 이미지에서 훈련하고 예술 작품에서 사람 탐지를 테스트합니다. 이 실험은 YOLO가 새로운 도메인으로 얼마나 잘 일반화되는지를 보여줍니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_12",
      "easy_section_title": "Real-Time Detection In The Wild",
      "easy_section_type": "section",
      "easy_section_order": 12,
      "easy_section_level": 1,
      "easy_content": "YOLO는 빠르고 정확한 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)기로서 컴퓨터 비전 애플리케이션에 이상적입니다. YOLO를 웹캠에 연결하여 카메라에서 이미지를 가져오고 탐지를 표시하는 시간을 포함하여 real-time detection(실시간으로 물체를 탐지하는 능력) 성능을 유지함을 확인합니다.\n\n결과 시스템은 대화형이고 매력적입니다. YOLO는 이미지를 개별적으로 처리하지만 웹캠에 연결되면 추적 시스템처럼 작동하여 물체가 움직이고 외관이 변할 때 탐지합니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_12_1",
          "easy_paragraph_text": "YOLO는 빠르고 정확한 물체 탐지기로서 컴퓨터 비전 애플리케이션에 이상적입니다. YOLO를 웹캠에 연결하여 카메라에서 이미지를 가져오고 탐지를 표시하는 시간을 포함하여 실시간 성능을 유지함을 확인합니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    },
    {
      "easy_section_id": "easy_section_13",
      "easy_section_title": "Conclusion",
      "easy_section_type": "section",
      "easy_section_order": 13,
      "easy_section_level": 1,
      "easy_content": "우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)을 위한 통합 모델인 YOLO를 소개합니다. 우리 모델은 구성하기 간단하고 전체 이미지에서 직접 훈련할 수 있습니다. 분류기 기반 접근법과 달리 YOLO는 탐지 성능에 직접 대응하는 loss function(모델이 학습 과정에서 최소화하려는 함수, 오차를 측정)에서 훈련되며 전체 모델이 함께 훈련됩니다.\n\nFast YOLO는 문헌에서 가장 빠른 범용 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)기이며 YOLO는 real-time detection(실시간으로 물체를 탐지하는 능력)에서 최신 기술을 발전시킵니다. YOLO는 또한 새로운 도메인에 잘 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력)되어 빠르고 견고한 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)에 의존하는 애플리케이션에 이상적입니다.",
      "easy_paragraphs": [
        {
          "easy_paragraph_id": "easy_paragraph_13_1",
          "easy_paragraph_text": "우리는 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)을 위한 통합 모델인 YOLO를 소개합니다. 우리 모델은 구성하기 간단하고 전체 이미지에서 직접 훈련할 수 있습니다. 분류기 기반 접근법과 달리 YOLO는 탐지 성능에 직접 대응하는 loss function(모델이 학습 과정에서 최소화하려는 함수, 오차를 측정)에서 훈련되며 전체 모델이 함께 훈련됩니다.",
          "easy_paragraph_order": 1,
          "easy_visualization_trigger": true
        },
        {
          "easy_paragraph_id": "easy_paragraph_13_2",
          "easy_paragraph_text": "Fast YOLO는 문헌에서 가장 빠른 범용 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)기이며 YOLO는 real-time detection(실시간으로 물체를 탐지하는 능력)에서 최신 기술을 발전시킵니다. YOLO는 또한 새로운 도메인에 잘 generalization(새로운 데이터나 다른 도메인에도 잘 동작하는 능력)되어 빠르고 견고한 object detection(이미지 속에서 물체의 위치와 종류를 찾아내는 작업)에 의존하는 애플리케이션에 이상적입니다.",
          "easy_paragraph_order": 2,
          "easy_visualization_trigger": true
        }
      ],
      "easy_visualizations": []
    }
  ],
  "metadata": {
    "generated_at": "2025-09-24 03:15:00",
    "easy_model_version": "yolo-easy-qlora-checkpoint-200",
    "total_processing_time": 0,
    "visualization_triggers": 16,
    "total_paragraphs": 21,
    "section_types": {
      "section": 6,
      "subsection": 7,
      "subsubsection": 0
    },
    "processing_status": "completed",
    "error_count": 0,
    "warnings": [],
    "errors": [],
    "resumed_from_cache": false,
    "resumed_sections": 0
  }
}